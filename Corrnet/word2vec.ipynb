{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bWdx4tuI4vL"
      },
      "source": [
        "Reference: https://www.tensorflow.org/tutorials/text/word2vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9jbt38eV1_t"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTxMG_R9W207"
      },
      "source": [
        "import io\n",
        "import itertools\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import tqdm\n",
        "\n",
        "from tensorflow.keras import Model, Sequential\n",
        "from tensorflow.keras.layers import Activation, Dense, Dot, Embedding, Flatten, GlobalAveragePooling1D, Reshape\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_mc26_HhcPP"
      },
      "source": [
        "SEED = 42 \n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8QZ9NQWWQBS"
      },
      "source": [
        "# You may load this given csv, or you may change this line to load some other csv\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/amey-kulkarni27/Datasets/main/images.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqL5sc6QHpi9"
      },
      "source": [
        "## Text Standardisation\n",
        "Converting text to lower case, removing punctuation, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNjhLclCbmHr"
      },
      "source": [
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  return tf.strings.regex_replace(lowercase, '[%s]' % re.escape(string.punctuation), '')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ra3duWa7cPga"
      },
      "source": [
        "df_stand = df['description'].apply(custom_standardization)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50WmzH5IcNkx",
        "outputId": "c1841a03-6848-4f8b-f330-66b4bb679f15"
      },
      "source": [
        "df_stand"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        tf.Tensor(b'nayo women orange  offwhite stripe...\n",
              "1        tf.Tensor(b'jaipur kurti women navy blue yoke ...\n",
              "2        tf.Tensor(b'ahika women green  offwhite printe...\n",
              "3        tf.Tensor(b'ada women yellow  white chikankari...\n",
              "4        tf.Tensor(b'soch women navy blue  grey dyed st...\n",
              "                               ...                        \n",
              "12300    tf.Tensor(b'saree swarg blue  orange checked b...\n",
              "12301    tf.Tensor(b'tikhi imli beige  red poly silk pr...\n",
              "12302    tf.Tensor(b'mitera pink  goldtoned silk blend ...\n",
              "12303    tf.Tensor(b'saree mall pink  golden woven desi...\n",
              "12304    tf.Tensor(b'kalakari india blue  white dabu ha...\n",
              "Name: description, Length: 12305, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uunqL8FdH9p"
      },
      "source": [
        "vocab_size = 1394\n",
        "sequence_length = 5\n",
        "num_ns = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYyMyPDiH297"
      },
      "source": [
        "## Vectorisation and Inverse Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqTofsSBd28B"
      },
      "source": [
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq6Q62AefKQS"
      },
      "source": [
        "text_dataset = tf.data.Dataset.from_tensor_slices(list(df_stand))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6s8WshMbfYDj",
        "outputId": "d18a0a8a-8fab-461e-beb5-654793520bf1"
      },
      "source": [
        "text_dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TensorSliceDataset shapes: (), types: tf.string>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5r8Qeiud8iA"
      },
      "source": [
        "vectorize_layer.adapt(text_dataset.batch(1024))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdG3seFVe2wO"
      },
      "source": [
        "inverse_vocab = vectorize_layer.get_vocabulary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xOBSBQl2zoy",
        "outputId": "50cf117a-0124-4f22-c28a-9147c66900ba"
      },
      "source": [
        "print(len(inverse_vocab))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSEfVRA4g7qU"
      },
      "source": [
        "text_vector_ds = text_dataset.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjGToR5-hsdU",
        "outputId": "8eab7d4c-a238-48fd-c26f-162ac6baf14d"
      },
      "source": [
        "sequences = list(text_vector_ds.as_numpy_iterator())\n",
        "len(sequences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12305"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKQ1yoYviH1M",
        "outputId": "693a8dff-4af7-40ff-c19e-a1e262119bd4"
      },
      "source": [
        "for seq in sequences[:5]:\n",
        "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[147   8  50  46  43  15   4   0   0   0   0   0] => ['nayo', 'women', 'orange', 'offwhite', 'striped', 'straight', 'kurta', '', '', '', '', '']\n",
            "[205 198   8  19   6 164  10   4   9 108   0   0] => ['jaipur', 'kurti', 'women', 'navy', 'blue', 'yoke', 'design', 'kurta', 'with', 'trousers', '', '']\n",
            "[451   8  16  46   5  15   4   0   0   0   0   0] => ['ahika', 'women', 'green', 'offwhite', 'printed', 'straight', 'kurta', '', '', '', '', '']\n",
            "[493   8  26  13 408 213  38 428  15   4   0   0] => ['ada', 'women', 'yellow', 'white', 'chikankari', 'hand', 'embroidered', 'semisheer', 'straight', 'kurta', '', '']\n",
            "[196   8  19   6  30 214  15   4   0   0   0   0] => ['soch', 'women', 'navy', 'blue', 'grey', 'dyed', 'straight', 'kurta', '', '', '', '']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXesd_eXIVbk"
      },
      "source": [
        "##Training Data\n",
        " Generates skip-gram pairs with negative sampling for a list of sequences(int-encoded sentences) based on window size, number of negative samples and vocabulary size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xcm8wnnhi1Kg"
      },
      "source": [
        "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
        "  # Elements of each training example are appended to these lists.\n",
        "  targets, contexts, labels = [], [], []\n",
        "\n",
        "  # Build the sampling table for vocab_size tokens.\n",
        "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
        "\n",
        "  # Iterate over all sequences (sentences) in dataset.\n",
        "  for sequence in tqdm.tqdm(sequences):\n",
        "\n",
        "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
        "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "          sequence, \n",
        "          vocabulary_size=vocab_size,\n",
        "          sampling_table=sampling_table,\n",
        "          window_size=window_size,\n",
        "          negative_samples=0)\n",
        "\n",
        "    # Iterate over each positive skip-gram pair to produce training examples \n",
        "    # with positive context word and negative samples.\n",
        "    for target_word, context_word in positive_skip_grams:\n",
        "      context_class = tf.expand_dims(\n",
        "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
        "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "          true_classes=context_class,\n",
        "          num_true=1, \n",
        "          num_sampled=num_ns, \n",
        "          unique=True, \n",
        "          range_max=vocab_size, \n",
        "          seed=SEED, \n",
        "          name=\"negative_sampling\")\n",
        "\n",
        "      # Build context and label vectors (for one target word)\n",
        "      negative_sampling_candidates = tf.expand_dims(\n",
        "          negative_sampling_candidates, 1)\n",
        "\n",
        "      context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
        "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
        "\n",
        "      # Append each element from the training example to global lists.\n",
        "      targets.append(target_word)\n",
        "      contexts.append(context)\n",
        "      labels.append(label)\n",
        "\n",
        "  return targets, contexts, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTqznQfrirRD",
        "outputId": "fd964194-e5c8-4846-9e91-6763fa566b63"
      },
      "source": [
        "targets, contexts, labels = generate_training_data(\n",
        "    sequences=sequences, \n",
        "    window_size=4, \n",
        "    num_ns=4, \n",
        "    vocab_size=vocab_size, \n",
        "    seed=SEED)\n",
        "print(len(targets), len(contexts), len(labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 12305/12305 [00:02<00:00, 4410.94it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "26249 26249 26249\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FajzSVZQjTz2",
        "outputId": "9a354f2c-b8e9-430e-981a-6728328cbd11"
      },
      "source": [
        "BATCH_SIZE = 512\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "print(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BatchDataset shapes: (((512,), (512, 5, 1)), (512, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iV5RmuBjYuA",
        "outputId": "8c1d7183-a3fd-490e-a08f-462088e5ffb8"
      },
      "source": [
        "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "print(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<PrefetchDataset shapes: (((512,), (512, 5, 1)), (512, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKMcz95iJBA9"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA8K0xyLnb8m"
      },
      "source": [
        "class Word2Vec(Model):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    super(Word2Vec, self).__init__()\n",
        "    self.target_embedding = Embedding(vocab_size, \n",
        "                                      embedding_dim,\n",
        "                                      input_length=1,\n",
        "                                      name=\"w2v_embedding\", )\n",
        "    self.context_embedding = Embedding(vocab_size, \n",
        "                                       embedding_dim, \n",
        "                                       input_length=num_ns+1,\n",
        "                                       name = \"ctxt_embedding\")\n",
        "    self.dots = Dot(axes=(3,2))\n",
        "    self.flatten = Flatten()\n",
        "\n",
        "  def call(self, pair):\n",
        "    target, context = pair\n",
        "    we = self.target_embedding(target)\n",
        "    ce = self.context_embedding(context)\n",
        "    dots = self.dots([ce, we])\n",
        "    return self.flatten(dots)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aA-dqchwndxi"
      },
      "source": [
        "embedding_dim = 512\n",
        "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
        "word2vec.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kh9QbU3Qn2On"
      },
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kb4RNFhMn3BG",
        "outputId": "fcc59e00-35b3-4174-8505-d1b0bf604336"
      },
      "source": [
        "word2vec.fit(dataset, epochs=1000, callbacks=[tensorboard_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.7284 - accuracy: 0.7549\n",
            "Epoch 2/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.6535 - accuracy: 0.7804\n",
            "Epoch 3/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.5963 - accuracy: 0.8018\n",
            "Epoch 4/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.5490 - accuracy: 0.8163\n",
            "Epoch 5/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.5098 - accuracy: 0.8296\n",
            "Epoch 6/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.4769 - accuracy: 0.8416\n",
            "Epoch 7/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.4493 - accuracy: 0.8514\n",
            "Epoch 8/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.4260 - accuracy: 0.8580\n",
            "Epoch 9/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.4061 - accuracy: 0.8640\n",
            "Epoch 10/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.3891 - accuracy: 0.8689\n",
            "Epoch 11/1000\n",
            "51/51 [==============================] - 1s 20ms/step - loss: 0.3744 - accuracy: 0.8734\n",
            "Epoch 12/1000\n",
            "51/51 [==============================] - 1s 20ms/step - loss: 0.3616 - accuracy: 0.8765\n",
            "Epoch 13/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.3505 - accuracy: 0.8794\n",
            "Epoch 14/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.3408 - accuracy: 0.8816\n",
            "Epoch 15/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.3322 - accuracy: 0.8841\n",
            "Epoch 16/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.3247 - accuracy: 0.8860\n",
            "Epoch 17/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.3179 - accuracy: 0.8874\n",
            "Epoch 18/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.3119 - accuracy: 0.8887\n",
            "Epoch 19/1000\n",
            "51/51 [==============================] - 1s 20ms/step - loss: 0.3066 - accuracy: 0.8897\n",
            "Epoch 20/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.3018 - accuracy: 0.8909\n",
            "Epoch 21/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2974 - accuracy: 0.8915\n",
            "Epoch 22/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2935 - accuracy: 0.8918\n",
            "Epoch 23/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2899 - accuracy: 0.8921\n",
            "Epoch 24/1000\n",
            "51/51 [==============================] - 1s 20ms/step - loss: 0.2867 - accuracy: 0.8928\n",
            "Epoch 25/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2838 - accuracy: 0.8931\n",
            "Epoch 26/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2811 - accuracy: 0.8935\n",
            "Epoch 27/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2786 - accuracy: 0.8938\n",
            "Epoch 28/1000\n",
            "51/51 [==============================] - 1s 20ms/step - loss: 0.2763 - accuracy: 0.8942\n",
            "Epoch 29/1000\n",
            "51/51 [==============================] - 1s 20ms/step - loss: 0.2743 - accuracy: 0.8944\n",
            "Epoch 30/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2723 - accuracy: 0.8942\n",
            "Epoch 31/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2706 - accuracy: 0.8945\n",
            "Epoch 32/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2689 - accuracy: 0.8943\n",
            "Epoch 33/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2674 - accuracy: 0.8943\n",
            "Epoch 34/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2660 - accuracy: 0.8941\n",
            "Epoch 35/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2646 - accuracy: 0.8943\n",
            "Epoch 36/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2634 - accuracy: 0.8943\n",
            "Epoch 37/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2623 - accuracy: 0.8943\n",
            "Epoch 38/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2612 - accuracy: 0.8943\n",
            "Epoch 39/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2602 - accuracy: 0.8944\n",
            "Epoch 40/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2592 - accuracy: 0.8946\n",
            "Epoch 41/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2583 - accuracy: 0.8946\n",
            "Epoch 42/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2575 - accuracy: 0.8946\n",
            "Epoch 43/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2567 - accuracy: 0.8949\n",
            "Epoch 44/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2559 - accuracy: 0.8950\n",
            "Epoch 45/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2552 - accuracy: 0.8951\n",
            "Epoch 46/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2546 - accuracy: 0.8950\n",
            "Epoch 47/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2539 - accuracy: 0.8951\n",
            "Epoch 48/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2533 - accuracy: 0.8951\n",
            "Epoch 49/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2528 - accuracy: 0.8951\n",
            "Epoch 50/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2522 - accuracy: 0.8951\n",
            "Epoch 51/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2517 - accuracy: 0.8951\n",
            "Epoch 52/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2512 - accuracy: 0.8951\n",
            "Epoch 53/1000\n",
            "51/51 [==============================] - 1s 20ms/step - loss: 0.2507 - accuracy: 0.8950\n",
            "Epoch 54/1000\n",
            "51/51 [==============================] - 1s 20ms/step - loss: 0.2503 - accuracy: 0.8950\n",
            "Epoch 55/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2499 - accuracy: 0.8951\n",
            "Epoch 56/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2495 - accuracy: 0.8952\n",
            "Epoch 57/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2491 - accuracy: 0.8953\n",
            "Epoch 58/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2487 - accuracy: 0.8951\n",
            "Epoch 59/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2484 - accuracy: 0.8951\n",
            "Epoch 60/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2480 - accuracy: 0.8951\n",
            "Epoch 61/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2477 - accuracy: 0.8950\n",
            "Epoch 62/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2474 - accuracy: 0.8949\n",
            "Epoch 63/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2471 - accuracy: 0.8949\n",
            "Epoch 64/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2468 - accuracy: 0.8950\n",
            "Epoch 65/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2465 - accuracy: 0.8950\n",
            "Epoch 66/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2462 - accuracy: 0.8950\n",
            "Epoch 67/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2460 - accuracy: 0.8948\n",
            "Epoch 68/1000\n",
            "51/51 [==============================] - 1s 20ms/step - loss: 0.2457 - accuracy: 0.8948\n",
            "Epoch 69/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2455 - accuracy: 0.8948\n",
            "Epoch 70/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2453 - accuracy: 0.8948\n",
            "Epoch 71/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2451 - accuracy: 0.8948\n",
            "Epoch 72/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2448 - accuracy: 0.8948\n",
            "Epoch 73/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2446 - accuracy: 0.8949\n",
            "Epoch 74/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2444 - accuracy: 0.8948\n",
            "Epoch 75/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2442 - accuracy: 0.8948\n",
            "Epoch 76/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2441 - accuracy: 0.8948\n",
            "Epoch 77/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2439 - accuracy: 0.8948\n",
            "Epoch 78/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2437 - accuracy: 0.8948\n",
            "Epoch 79/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2435 - accuracy: 0.8949\n",
            "Epoch 80/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2434 - accuracy: 0.8949\n",
            "Epoch 81/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2432 - accuracy: 0.8948\n",
            "Epoch 82/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2431 - accuracy: 0.8948\n",
            "Epoch 83/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2429 - accuracy: 0.8947\n",
            "Epoch 84/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2428 - accuracy: 0.8948\n",
            "Epoch 85/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2426 - accuracy: 0.8948\n",
            "Epoch 86/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2425 - accuracy: 0.8948\n",
            "Epoch 87/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2424 - accuracy: 0.8948\n",
            "Epoch 88/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2422 - accuracy: 0.8948\n",
            "Epoch 89/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2421 - accuracy: 0.8948\n",
            "Epoch 90/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2420 - accuracy: 0.8947\n",
            "Epoch 91/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2419 - accuracy: 0.8947\n",
            "Epoch 92/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2418 - accuracy: 0.8948\n",
            "Epoch 93/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2417 - accuracy: 0.8948\n",
            "Epoch 94/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2416 - accuracy: 0.8948\n",
            "Epoch 95/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2414 - accuracy: 0.8948\n",
            "Epoch 96/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2413 - accuracy: 0.8948\n",
            "Epoch 97/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2413 - accuracy: 0.8948\n",
            "Epoch 98/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2412 - accuracy: 0.8948\n",
            "Epoch 99/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2411 - accuracy: 0.8948\n",
            "Epoch 100/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2410 - accuracy: 0.8948\n",
            "Epoch 101/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2409 - accuracy: 0.8947\n",
            "Epoch 102/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2408 - accuracy: 0.8948\n",
            "Epoch 103/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2407 - accuracy: 0.8947\n",
            "Epoch 104/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2406 - accuracy: 0.8947\n",
            "Epoch 105/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2405 - accuracy: 0.8947\n",
            "Epoch 106/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2405 - accuracy: 0.8947\n",
            "Epoch 107/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2404 - accuracy: 0.8946\n",
            "Epoch 108/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2403 - accuracy: 0.8946\n",
            "Epoch 109/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2402 - accuracy: 0.8947\n",
            "Epoch 110/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2402 - accuracy: 0.8947\n",
            "Epoch 111/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2401 - accuracy: 0.8947\n",
            "Epoch 112/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2400 - accuracy: 0.8947\n",
            "Epoch 113/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2400 - accuracy: 0.8947\n",
            "Epoch 114/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2399 - accuracy: 0.8948\n",
            "Epoch 115/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2398 - accuracy: 0.8947\n",
            "Epoch 116/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2398 - accuracy: 0.8948\n",
            "Epoch 117/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2397 - accuracy: 0.8947\n",
            "Epoch 118/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2396 - accuracy: 0.8948\n",
            "Epoch 119/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2396 - accuracy: 0.8948\n",
            "Epoch 120/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2395 - accuracy: 0.8948\n",
            "Epoch 121/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2395 - accuracy: 0.8948\n",
            "Epoch 122/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2394 - accuracy: 0.8949\n",
            "Epoch 123/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2394 - accuracy: 0.8948\n",
            "Epoch 124/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2393 - accuracy: 0.8949\n",
            "Epoch 125/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2393 - accuracy: 0.8948\n",
            "Epoch 126/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2392 - accuracy: 0.8949\n",
            "Epoch 127/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2392 - accuracy: 0.8948\n",
            "Epoch 128/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2391 - accuracy: 0.8949\n",
            "Epoch 129/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2391 - accuracy: 0.8948\n",
            "Epoch 130/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2390 - accuracy: 0.8948\n",
            "Epoch 131/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2390 - accuracy: 0.8948\n",
            "Epoch 132/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2389 - accuracy: 0.8948\n",
            "Epoch 133/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2389 - accuracy: 0.8948\n",
            "Epoch 134/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2388 - accuracy: 0.8948\n",
            "Epoch 135/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2388 - accuracy: 0.8948\n",
            "Epoch 136/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2388 - accuracy: 0.8948\n",
            "Epoch 137/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2387 - accuracy: 0.8948\n",
            "Epoch 138/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2387 - accuracy: 0.8948\n",
            "Epoch 139/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2386 - accuracy: 0.8948\n",
            "Epoch 140/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2386 - accuracy: 0.8948\n",
            "Epoch 141/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2386 - accuracy: 0.8948\n",
            "Epoch 142/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2385 - accuracy: 0.8949\n",
            "Epoch 143/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2385 - accuracy: 0.8947\n",
            "Epoch 144/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2385 - accuracy: 0.8949\n",
            "Epoch 145/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2384 - accuracy: 0.8947\n",
            "Epoch 146/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2384 - accuracy: 0.8949\n",
            "Epoch 147/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2384 - accuracy: 0.8947\n",
            "Epoch 148/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2383 - accuracy: 0.8948\n",
            "Epoch 149/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2383 - accuracy: 0.8947\n",
            "Epoch 150/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2383 - accuracy: 0.8948\n",
            "Epoch 151/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2382 - accuracy: 0.8947\n",
            "Epoch 152/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2382 - accuracy: 0.8948\n",
            "Epoch 153/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2382 - accuracy: 0.8947\n",
            "Epoch 154/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2381 - accuracy: 0.8948\n",
            "Epoch 155/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2381 - accuracy: 0.8947\n",
            "Epoch 156/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2381 - accuracy: 0.8948\n",
            "Epoch 157/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2380 - accuracy: 0.8947\n",
            "Epoch 158/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2380 - accuracy: 0.8948\n",
            "Epoch 159/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2380 - accuracy: 0.8947\n",
            "Epoch 160/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2380 - accuracy: 0.8948\n",
            "Epoch 161/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2379 - accuracy: 0.8947\n",
            "Epoch 162/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2379 - accuracy: 0.8948\n",
            "Epoch 163/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2379 - accuracy: 0.8947\n",
            "Epoch 164/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2379 - accuracy: 0.8948\n",
            "Epoch 165/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2378 - accuracy: 0.8947\n",
            "Epoch 166/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2378 - accuracy: 0.8948\n",
            "Epoch 167/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2378 - accuracy: 0.8947\n",
            "Epoch 168/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2378 - accuracy: 0.8948\n",
            "Epoch 169/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2377 - accuracy: 0.8948\n",
            "Epoch 170/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2377 - accuracy: 0.8948\n",
            "Epoch 171/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2377 - accuracy: 0.8948\n",
            "Epoch 172/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2377 - accuracy: 0.8948\n",
            "Epoch 173/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2376 - accuracy: 0.8948\n",
            "Epoch 174/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2376 - accuracy: 0.8949\n",
            "Epoch 175/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2376 - accuracy: 0.8948\n",
            "Epoch 176/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2376 - accuracy: 0.8949\n",
            "Epoch 177/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2376 - accuracy: 0.8948\n",
            "Epoch 178/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2375 - accuracy: 0.8949\n",
            "Epoch 179/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2375 - accuracy: 0.8948\n",
            "Epoch 180/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2375 - accuracy: 0.8949\n",
            "Epoch 181/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2375 - accuracy: 0.8948\n",
            "Epoch 182/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2375 - accuracy: 0.8949\n",
            "Epoch 183/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2374 - accuracy: 0.8948\n",
            "Epoch 184/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2374 - accuracy: 0.8949\n",
            "Epoch 185/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2374 - accuracy: 0.8948\n",
            "Epoch 186/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2374 - accuracy: 0.8949\n",
            "Epoch 187/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2374 - accuracy: 0.8948\n",
            "Epoch 188/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2373 - accuracy: 0.8948\n",
            "Epoch 189/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2373 - accuracy: 0.8948\n",
            "Epoch 190/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2373 - accuracy: 0.8948\n",
            "Epoch 191/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2373 - accuracy: 0.8948\n",
            "Epoch 192/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2373 - accuracy: 0.8948\n",
            "Epoch 193/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2373 - accuracy: 0.8948\n",
            "Epoch 194/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2372 - accuracy: 0.8948\n",
            "Epoch 195/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2372 - accuracy: 0.8948\n",
            "Epoch 196/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2372 - accuracy: 0.8948\n",
            "Epoch 197/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2372 - accuracy: 0.8948\n",
            "Epoch 198/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2372 - accuracy: 0.8948\n",
            "Epoch 199/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2372 - accuracy: 0.8948\n",
            "Epoch 200/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2371 - accuracy: 0.8949\n",
            "Epoch 201/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2371 - accuracy: 0.8948\n",
            "Epoch 202/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2371 - accuracy: 0.8949\n",
            "Epoch 203/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2371 - accuracy: 0.8948\n",
            "Epoch 204/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2371 - accuracy: 0.8949\n",
            "Epoch 205/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2371 - accuracy: 0.8948\n",
            "Epoch 206/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2370 - accuracy: 0.8948\n",
            "Epoch 207/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2370 - accuracy: 0.8948\n",
            "Epoch 208/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2370 - accuracy: 0.8948\n",
            "Epoch 209/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2370 - accuracy: 0.8947\n",
            "Epoch 210/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2370 - accuracy: 0.8948\n",
            "Epoch 211/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2370 - accuracy: 0.8947\n",
            "Epoch 212/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2370 - accuracy: 0.8948\n",
            "Epoch 213/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2370 - accuracy: 0.8947\n",
            "Epoch 214/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2369 - accuracy: 0.8948\n",
            "Epoch 215/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2369 - accuracy: 0.8947\n",
            "Epoch 216/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2369 - accuracy: 0.8948\n",
            "Epoch 217/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2369 - accuracy: 0.8947\n",
            "Epoch 218/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2369 - accuracy: 0.8948\n",
            "Epoch 219/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2369 - accuracy: 0.8947\n",
            "Epoch 220/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2369 - accuracy: 0.8948\n",
            "Epoch 221/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2368 - accuracy: 0.8947\n",
            "Epoch 222/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2368 - accuracy: 0.8948\n",
            "Epoch 223/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2368 - accuracy: 0.8947\n",
            "Epoch 224/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2368 - accuracy: 0.8948\n",
            "Epoch 225/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2368 - accuracy: 0.8948\n",
            "Epoch 226/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2368 - accuracy: 0.8948\n",
            "Epoch 227/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2368 - accuracy: 0.8948\n",
            "Epoch 228/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2368 - accuracy: 0.8949\n",
            "Epoch 229/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2368 - accuracy: 0.8948\n",
            "Epoch 230/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2367 - accuracy: 0.8949\n",
            "Epoch 231/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2367 - accuracy: 0.8948\n",
            "Epoch 232/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2367 - accuracy: 0.8949\n",
            "Epoch 233/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2367 - accuracy: 0.8948\n",
            "Epoch 234/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2367 - accuracy: 0.8949\n",
            "Epoch 235/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2367 - accuracy: 0.8948\n",
            "Epoch 236/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2367 - accuracy: 0.8949\n",
            "Epoch 237/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2367 - accuracy: 0.8948\n",
            "Epoch 238/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2367 - accuracy: 0.8949\n",
            "Epoch 239/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2366 - accuracy: 0.8948\n",
            "Epoch 240/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2366 - accuracy: 0.8949\n",
            "Epoch 241/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2366 - accuracy: 0.8948\n",
            "Epoch 242/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2366 - accuracy: 0.8948\n",
            "Epoch 243/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2366 - accuracy: 0.8948\n",
            "Epoch 244/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2366 - accuracy: 0.8948\n",
            "Epoch 245/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2366 - accuracy: 0.8948\n",
            "Epoch 246/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2366 - accuracy: 0.8948\n",
            "Epoch 247/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2366 - accuracy: 0.8948\n",
            "Epoch 248/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2366 - accuracy: 0.8948\n",
            "Epoch 249/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2365 - accuracy: 0.8948\n",
            "Epoch 250/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2365 - accuracy: 0.8948\n",
            "Epoch 251/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2365 - accuracy: 0.8948\n",
            "Epoch 252/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2365 - accuracy: 0.8948\n",
            "Epoch 253/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2365 - accuracy: 0.8948\n",
            "Epoch 254/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2365 - accuracy: 0.8948\n",
            "Epoch 255/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2365 - accuracy: 0.8948\n",
            "Epoch 256/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2365 - accuracy: 0.8948\n",
            "Epoch 257/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2365 - accuracy: 0.8947\n",
            "Epoch 258/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2365 - accuracy: 0.8948\n",
            "Epoch 259/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2365 - accuracy: 0.8947\n",
            "Epoch 260/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2364 - accuracy: 0.8948\n",
            "Epoch 261/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2364 - accuracy: 0.8947\n",
            "Epoch 262/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2364 - accuracy: 0.8948\n",
            "Epoch 263/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2364 - accuracy: 0.8948\n",
            "Epoch 264/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2364 - accuracy: 0.8948\n",
            "Epoch 265/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2364 - accuracy: 0.8948\n",
            "Epoch 266/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2364 - accuracy: 0.8948\n",
            "Epoch 267/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2364 - accuracy: 0.8948\n",
            "Epoch 268/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2364 - accuracy: 0.8948\n",
            "Epoch 269/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2364 - accuracy: 0.8948\n",
            "Epoch 270/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2364 - accuracy: 0.8948\n",
            "Epoch 271/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2364 - accuracy: 0.8948\n",
            "Epoch 272/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2363 - accuracy: 0.8948\n",
            "Epoch 273/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2363 - accuracy: 0.8948\n",
            "Epoch 274/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2363 - accuracy: 0.8949\n",
            "Epoch 275/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2363 - accuracy: 0.8948\n",
            "Epoch 276/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2363 - accuracy: 0.8949\n",
            "Epoch 277/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2363 - accuracy: 0.8948\n",
            "Epoch 278/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2363 - accuracy: 0.8949\n",
            "Epoch 279/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2363 - accuracy: 0.8948\n",
            "Epoch 280/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2363 - accuracy: 0.8949\n",
            "Epoch 281/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2363 - accuracy: 0.8948\n",
            "Epoch 282/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2363 - accuracy: 0.8949\n",
            "Epoch 283/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2363 - accuracy: 0.8948\n",
            "Epoch 284/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2363 - accuracy: 0.8949\n",
            "Epoch 285/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2362 - accuracy: 0.8948\n",
            "Epoch 286/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2362 - accuracy: 0.8949\n",
            "Epoch 287/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2362 - accuracy: 0.8948\n",
            "Epoch 288/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2362 - accuracy: 0.8949\n",
            "Epoch 289/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2362 - accuracy: 0.8948\n",
            "Epoch 290/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2362 - accuracy: 0.8949\n",
            "Epoch 291/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2362 - accuracy: 0.8948\n",
            "Epoch 292/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2362 - accuracy: 0.8949\n",
            "Epoch 293/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2362 - accuracy: 0.8948\n",
            "Epoch 294/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2362 - accuracy: 0.8949\n",
            "Epoch 295/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2362 - accuracy: 0.8948\n",
            "Epoch 296/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2362 - accuracy: 0.8949\n",
            "Epoch 297/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2362 - accuracy: 0.8948\n",
            "Epoch 298/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2362 - accuracy: 0.8949\n",
            "Epoch 299/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2362 - accuracy: 0.8948\n",
            "Epoch 300/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2361 - accuracy: 0.8949\n",
            "Epoch 301/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2361 - accuracy: 0.8948\n",
            "Epoch 302/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2361 - accuracy: 0.8949\n",
            "Epoch 303/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2361 - accuracy: 0.8948\n",
            "Epoch 304/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2361 - accuracy: 0.8949\n",
            "Epoch 305/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2361 - accuracy: 0.8948\n",
            "Epoch 306/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2361 - accuracy: 0.8949\n",
            "Epoch 307/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2361 - accuracy: 0.8948\n",
            "Epoch 308/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2361 - accuracy: 0.8949\n",
            "Epoch 309/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2361 - accuracy: 0.8948\n",
            "Epoch 310/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2361 - accuracy: 0.8949\n",
            "Epoch 311/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2361 - accuracy: 0.8948\n",
            "Epoch 312/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2361 - accuracy: 0.8949\n",
            "Epoch 313/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2361 - accuracy: 0.8948\n",
            "Epoch 314/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2361 - accuracy: 0.8949\n",
            "Epoch 315/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2361 - accuracy: 0.8948\n",
            "Epoch 316/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2360 - accuracy: 0.8949\n",
            "Epoch 317/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2360 - accuracy: 0.8948\n",
            "Epoch 318/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2360 - accuracy: 0.8949\n",
            "Epoch 319/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2360 - accuracy: 0.8948\n",
            "Epoch 320/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2360 - accuracy: 0.8949\n",
            "Epoch 321/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2360 - accuracy: 0.8949\n",
            "Epoch 322/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2360 - accuracy: 0.8950\n",
            "Epoch 323/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2360 - accuracy: 0.8949\n",
            "Epoch 324/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2360 - accuracy: 0.8950\n",
            "Epoch 325/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2360 - accuracy: 0.8949\n",
            "Epoch 326/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2360 - accuracy: 0.8950\n",
            "Epoch 327/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2360 - accuracy: 0.8949\n",
            "Epoch 328/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2360 - accuracy: 0.8950\n",
            "Epoch 329/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2360 - accuracy: 0.8949\n",
            "Epoch 330/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2360 - accuracy: 0.8950\n",
            "Epoch 331/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2360 - accuracy: 0.8949\n",
            "Epoch 332/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2360 - accuracy: 0.8950\n",
            "Epoch 333/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2360 - accuracy: 0.8949\n",
            "Epoch 334/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2359 - accuracy: 0.8950\n",
            "Epoch 335/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2359 - accuracy: 0.8949\n",
            "Epoch 336/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2359 - accuracy: 0.8950\n",
            "Epoch 337/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2359 - accuracy: 0.8949\n",
            "Epoch 338/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2359 - accuracy: 0.8950\n",
            "Epoch 339/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2359 - accuracy: 0.8949\n",
            "Epoch 340/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2359 - accuracy: 0.8950\n",
            "Epoch 341/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2359 - accuracy: 0.8949\n",
            "Epoch 342/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2359 - accuracy: 0.8950\n",
            "Epoch 343/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2359 - accuracy: 0.8949\n",
            "Epoch 344/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2359 - accuracy: 0.8950\n",
            "Epoch 345/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2359 - accuracy: 0.8949\n",
            "Epoch 346/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2359 - accuracy: 0.8950\n",
            "Epoch 347/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2359 - accuracy: 0.8949\n",
            "Epoch 348/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2359 - accuracy: 0.8950\n",
            "Epoch 349/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2359 - accuracy: 0.8949\n",
            "Epoch 350/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2359 - accuracy: 0.8950\n",
            "Epoch 351/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2359 - accuracy: 0.8949\n",
            "Epoch 352/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2359 - accuracy: 0.8950\n",
            "Epoch 353/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2358 - accuracy: 0.8949\n",
            "Epoch 354/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2358 - accuracy: 0.8950\n",
            "Epoch 355/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2358 - accuracy: 0.8950\n",
            "Epoch 356/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2358 - accuracy: 0.8950\n",
            "Epoch 357/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2358 - accuracy: 0.8950\n",
            "Epoch 358/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2358 - accuracy: 0.8950\n",
            "Epoch 359/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2358 - accuracy: 0.8950\n",
            "Epoch 360/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2358 - accuracy: 0.8950\n",
            "Epoch 361/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2358 - accuracy: 0.8950\n",
            "Epoch 362/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2358 - accuracy: 0.8950\n",
            "Epoch 363/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2358 - accuracy: 0.8950\n",
            "Epoch 364/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2358 - accuracy: 0.8950\n",
            "Epoch 365/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2358 - accuracy: 0.8950\n",
            "Epoch 366/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2358 - accuracy: 0.8950\n",
            "Epoch 367/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2358 - accuracy: 0.8950\n",
            "Epoch 368/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2358 - accuracy: 0.8950\n",
            "Epoch 369/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2358 - accuracy: 0.8950\n",
            "Epoch 370/1000\n",
            "51/51 [==============================] - 1s 21ms/step - loss: 0.2358 - accuracy: 0.8950\n",
            "Epoch 371/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2358 - accuracy: 0.8949\n",
            "Epoch 372/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2358 - accuracy: 0.8950\n",
            "Epoch 373/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2358 - accuracy: 0.8949\n",
            "Epoch 374/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2358 - accuracy: 0.8950\n",
            "Epoch 375/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2357 - accuracy: 0.8949\n",
            "Epoch 376/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2357 - accuracy: 0.8950\n",
            "Epoch 377/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2357 - accuracy: 0.8949\n",
            "Epoch 378/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2357 - accuracy: 0.8950\n",
            "Epoch 379/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2357 - accuracy: 0.8949\n",
            "Epoch 380/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2357 - accuracy: 0.8950\n",
            "Epoch 381/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2357 - accuracy: 0.8950\n",
            "Epoch 382/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2357 - accuracy: 0.8950\n",
            "Epoch 383/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2357 - accuracy: 0.8950\n",
            "Epoch 384/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2357 - accuracy: 0.8950\n",
            "Epoch 385/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2357 - accuracy: 0.8950\n",
            "Epoch 386/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2357 - accuracy: 0.8950\n",
            "Epoch 387/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2357 - accuracy: 0.8950\n",
            "Epoch 388/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2357 - accuracy: 0.8950\n",
            "Epoch 389/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2357 - accuracy: 0.8950\n",
            "Epoch 390/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2357 - accuracy: 0.8950\n",
            "Epoch 391/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2357 - accuracy: 0.8950\n",
            "Epoch 392/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2357 - accuracy: 0.8950\n",
            "Epoch 393/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2357 - accuracy: 0.8950\n",
            "Epoch 394/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2357 - accuracy: 0.8950\n",
            "Epoch 395/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2357 - accuracy: 0.8950\n",
            "Epoch 396/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2357 - accuracy: 0.8950\n",
            "Epoch 397/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2357 - accuracy: 0.8950\n",
            "Epoch 398/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2356 - accuracy: 0.8950\n",
            "Epoch 399/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2356 - accuracy: 0.8950\n",
            "Epoch 400/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2356 - accuracy: 0.8950\n",
            "Epoch 401/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2356 - accuracy: 0.8950\n",
            "Epoch 402/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2356 - accuracy: 0.8950\n",
            "Epoch 403/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2356 - accuracy: 0.8950\n",
            "Epoch 404/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2356 - accuracy: 0.8950\n",
            "Epoch 405/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2356 - accuracy: 0.8950\n",
            "Epoch 406/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2356 - accuracy: 0.8950\n",
            "Epoch 407/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2356 - accuracy: 0.8950\n",
            "Epoch 408/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2356 - accuracy: 0.8950\n",
            "Epoch 409/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2356 - accuracy: 0.8950\n",
            "Epoch 410/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2356 - accuracy: 0.8950\n",
            "Epoch 411/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2356 - accuracy: 0.8950\n",
            "Epoch 412/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2356 - accuracy: 0.8950\n",
            "Epoch 413/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2356 - accuracy: 0.8950\n",
            "Epoch 414/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2356 - accuracy: 0.8950\n",
            "Epoch 415/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2356 - accuracy: 0.8950\n",
            "Epoch 416/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2356 - accuracy: 0.8950\n",
            "Epoch 417/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2356 - accuracy: 0.8950\n",
            "Epoch 418/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2356 - accuracy: 0.8950\n",
            "Epoch 419/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2356 - accuracy: 0.8950\n",
            "Epoch 420/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2356 - accuracy: 0.8950\n",
            "Epoch 421/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2356 - accuracy: 0.8950\n",
            "Epoch 422/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2356 - accuracy: 0.8950\n",
            "Epoch 423/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2356 - accuracy: 0.8950\n",
            "Epoch 424/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2355 - accuracy: 0.8951\n",
            "Epoch 425/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2355 - accuracy: 0.8950\n",
            "Epoch 426/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2355 - accuracy: 0.8951\n",
            "Epoch 427/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2355 - accuracy: 0.8950\n",
            "Epoch 428/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2355 - accuracy: 0.8951\n",
            "Epoch 429/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2355 - accuracy: 0.8950\n",
            "Epoch 430/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2355 - accuracy: 0.8951\n",
            "Epoch 431/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2355 - accuracy: 0.8951\n",
            "Epoch 432/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2355 - accuracy: 0.8951\n",
            "Epoch 433/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2355 - accuracy: 0.8951\n",
            "Epoch 434/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2355 - accuracy: 0.8951\n",
            "Epoch 435/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2355 - accuracy: 0.8951\n",
            "Epoch 436/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2355 - accuracy: 0.8951\n",
            "Epoch 437/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2355 - accuracy: 0.8951\n",
            "Epoch 438/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2355 - accuracy: 0.8951\n",
            "Epoch 439/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2355 - accuracy: 0.8951\n",
            "Epoch 440/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2355 - accuracy: 0.8951\n",
            "Epoch 441/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2355 - accuracy: 0.8951\n",
            "Epoch 442/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2355 - accuracy: 0.8951\n",
            "Epoch 443/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2355 - accuracy: 0.8951\n",
            "Epoch 444/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2355 - accuracy: 0.8951\n",
            "Epoch 445/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2355 - accuracy: 0.8951\n",
            "Epoch 446/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2355 - accuracy: 0.8951\n",
            "Epoch 447/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2355 - accuracy: 0.8951\n",
            "Epoch 448/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2355 - accuracy: 0.8951\n",
            "Epoch 449/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2355 - accuracy: 0.8951\n",
            "Epoch 450/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2355 - accuracy: 0.8951\n",
            "Epoch 451/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2355 - accuracy: 0.8951\n",
            "Epoch 452/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2354 - accuracy: 0.8951\n",
            "Epoch 453/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2354 - accuracy: 0.8951\n",
            "Epoch 454/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2354 - accuracy: 0.8951\n",
            "Epoch 455/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2354 - accuracy: 0.8951\n",
            "Epoch 456/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2354 - accuracy: 0.8951\n",
            "Epoch 457/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2354 - accuracy: 0.8951\n",
            "Epoch 458/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2354 - accuracy: 0.8951\n",
            "Epoch 459/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2354 - accuracy: 0.8951\n",
            "Epoch 460/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2354 - accuracy: 0.8951\n",
            "Epoch 461/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2354 - accuracy: 0.8951\n",
            "Epoch 462/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2354 - accuracy: 0.8951\n",
            "Epoch 463/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2354 - accuracy: 0.8951\n",
            "Epoch 464/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2354 - accuracy: 0.8952\n",
            "Epoch 465/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2354 - accuracy: 0.8951\n",
            "Epoch 466/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2354 - accuracy: 0.8952\n",
            "Epoch 467/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2354 - accuracy: 0.8951\n",
            "Epoch 468/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2354 - accuracy: 0.8952\n",
            "Epoch 469/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2354 - accuracy: 0.8951\n",
            "Epoch 470/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2354 - accuracy: 0.8952\n",
            "Epoch 471/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2354 - accuracy: 0.8951\n",
            "Epoch 472/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2354 - accuracy: 0.8952\n",
            "Epoch 473/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2354 - accuracy: 0.8951\n",
            "Epoch 474/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2354 - accuracy: 0.8951\n",
            "Epoch 475/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2354 - accuracy: 0.8951\n",
            "Epoch 476/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2354 - accuracy: 0.8951\n",
            "Epoch 477/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2354 - accuracy: 0.8951\n",
            "Epoch 478/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2354 - accuracy: 0.8951\n",
            "Epoch 479/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2354 - accuracy: 0.8951\n",
            "Epoch 480/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2354 - accuracy: 0.8951\n",
            "Epoch 481/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2354 - accuracy: 0.8951\n",
            "Epoch 482/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2354 - accuracy: 0.8951\n",
            "Epoch 483/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2353 - accuracy: 0.8951\n",
            "Epoch 484/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2353 - accuracy: 0.8951\n",
            "Epoch 485/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2353 - accuracy: 0.8951\n",
            "Epoch 486/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2353 - accuracy: 0.8951\n",
            "Epoch 487/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2353 - accuracy: 0.8951\n",
            "Epoch 488/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2353 - accuracy: 0.8951\n",
            "Epoch 489/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2353 - accuracy: 0.8951\n",
            "Epoch 490/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2353 - accuracy: 0.8951\n",
            "Epoch 491/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2353 - accuracy: 0.8951\n",
            "Epoch 492/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2353 - accuracy: 0.8951\n",
            "Epoch 493/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2353 - accuracy: 0.8951\n",
            "Epoch 494/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2353 - accuracy: 0.8951\n",
            "Epoch 495/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2353 - accuracy: 0.8951\n",
            "Epoch 496/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2353 - accuracy: 0.8951\n",
            "Epoch 497/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2353 - accuracy: 0.8951\n",
            "Epoch 498/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2353 - accuracy: 0.8951\n",
            "Epoch 499/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2353 - accuracy: 0.8951\n",
            "Epoch 500/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2353 - accuracy: 0.8951\n",
            "Epoch 501/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2353 - accuracy: 0.8951\n",
            "Epoch 502/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2353 - accuracy: 0.8951\n",
            "Epoch 503/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2353 - accuracy: 0.8951\n",
            "Epoch 504/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2353 - accuracy: 0.8951\n",
            "Epoch 505/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2353 - accuracy: 0.8951\n",
            "Epoch 506/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2353 - accuracy: 0.8951\n",
            "Epoch 507/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2353 - accuracy: 0.8951\n",
            "Epoch 508/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2353 - accuracy: 0.8951\n",
            "Epoch 509/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2353 - accuracy: 0.8951\n",
            "Epoch 510/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2353 - accuracy: 0.8951\n",
            "Epoch 511/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2353 - accuracy: 0.8951\n",
            "Epoch 512/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2353 - accuracy: 0.8951\n",
            "Epoch 513/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2353 - accuracy: 0.8951\n",
            "Epoch 514/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2353 - accuracy: 0.8952\n",
            "Epoch 515/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2353 - accuracy: 0.8951\n",
            "Epoch 516/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2352 - accuracy: 0.8952\n",
            "Epoch 517/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2352 - accuracy: 0.8951\n",
            "Epoch 518/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2352 - accuracy: 0.8952\n",
            "Epoch 519/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2352 - accuracy: 0.8951\n",
            "Epoch 520/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2352 - accuracy: 0.8952\n",
            "Epoch 521/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2352 - accuracy: 0.8951\n",
            "Epoch 522/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2352 - accuracy: 0.8952\n",
            "Epoch 523/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2352 - accuracy: 0.8951\n",
            "Epoch 524/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2352 - accuracy: 0.8952\n",
            "Epoch 525/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2352 - accuracy: 0.8952\n",
            "Epoch 526/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2352 - accuracy: 0.8952\n",
            "Epoch 527/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2352 - accuracy: 0.8952\n",
            "Epoch 528/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2352 - accuracy: 0.8953\n",
            "Epoch 529/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2352 - accuracy: 0.8952\n",
            "Epoch 530/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2352 - accuracy: 0.8953\n",
            "Epoch 531/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2352 - accuracy: 0.8952\n",
            "Epoch 532/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2352 - accuracy: 0.8953\n",
            "Epoch 533/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2352 - accuracy: 0.8952\n",
            "Epoch 534/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2352 - accuracy: 0.8953\n",
            "Epoch 535/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2352 - accuracy: 0.8952\n",
            "Epoch 536/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2352 - accuracy: 0.8953\n",
            "Epoch 537/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2352 - accuracy: 0.8952\n",
            "Epoch 538/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2352 - accuracy: 0.8953\n",
            "Epoch 539/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2352 - accuracy: 0.8952\n",
            "Epoch 540/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2352 - accuracy: 0.8953\n",
            "Epoch 541/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2352 - accuracy: 0.8952\n",
            "Epoch 542/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2352 - accuracy: 0.8953\n",
            "Epoch 543/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2352 - accuracy: 0.8952\n",
            "Epoch 544/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2352 - accuracy: 0.8953\n",
            "Epoch 545/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2352 - accuracy: 0.8952\n",
            "Epoch 546/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2352 - accuracy: 0.8953\n",
            "Epoch 547/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2352 - accuracy: 0.8952\n",
            "Epoch 548/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2352 - accuracy: 0.8953\n",
            "Epoch 549/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2352 - accuracy: 0.8952\n",
            "Epoch 550/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2352 - accuracy: 0.8953\n",
            "Epoch 551/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2352 - accuracy: 0.8952\n",
            "Epoch 552/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2352 - accuracy: 0.8953\n",
            "Epoch 553/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2351 - accuracy: 0.8952\n",
            "Epoch 554/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2351 - accuracy: 0.8953\n",
            "Epoch 555/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2351 - accuracy: 0.8952\n",
            "Epoch 556/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2351 - accuracy: 0.8953\n",
            "Epoch 557/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2351 - accuracy: 0.8952\n",
            "Epoch 558/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2351 - accuracy: 0.8953\n",
            "Epoch 559/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2351 - accuracy: 0.8952\n",
            "Epoch 560/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2351 - accuracy: 0.8953\n",
            "Epoch 561/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2351 - accuracy: 0.8952\n",
            "Epoch 562/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2351 - accuracy: 0.8953\n",
            "Epoch 563/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2351 - accuracy: 0.8952\n",
            "Epoch 564/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2351 - accuracy: 0.8953\n",
            "Epoch 565/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2351 - accuracy: 0.8952\n",
            "Epoch 566/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2351 - accuracy: 0.8953\n",
            "Epoch 567/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2351 - accuracy: 0.8951\n",
            "Epoch 568/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2351 - accuracy: 0.8953\n",
            "Epoch 569/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2351 - accuracy: 0.8951\n",
            "Epoch 570/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2351 - accuracy: 0.8953\n",
            "Epoch 571/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2351 - accuracy: 0.8951\n",
            "Epoch 572/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2351 - accuracy: 0.8953\n",
            "Epoch 573/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2351 - accuracy: 0.8951\n",
            "Epoch 574/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2351 - accuracy: 0.8953\n",
            "Epoch 575/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2351 - accuracy: 0.8951\n",
            "Epoch 576/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2351 - accuracy: 0.8953\n",
            "Epoch 577/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2351 - accuracy: 0.8951\n",
            "Epoch 578/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2351 - accuracy: 0.8953\n",
            "Epoch 579/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2351 - accuracy: 0.8951\n",
            "Epoch 580/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2351 - accuracy: 0.8952\n",
            "Epoch 581/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2351 - accuracy: 0.8951\n",
            "Epoch 582/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2351 - accuracy: 0.8952\n",
            "Epoch 583/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2351 - accuracy: 0.8951\n",
            "Epoch 584/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2351 - accuracy: 0.8952\n",
            "Epoch 585/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2351 - accuracy: 0.8951\n",
            "Epoch 586/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2351 - accuracy: 0.8952\n",
            "Epoch 587/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2351 - accuracy: 0.8951\n",
            "Epoch 588/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2351 - accuracy: 0.8952\n",
            "Epoch 589/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2351 - accuracy: 0.8951\n",
            "Epoch 590/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2351 - accuracy: 0.8953\n",
            "Epoch 591/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2351 - accuracy: 0.8951\n",
            "Epoch 592/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2350 - accuracy: 0.8953\n",
            "Epoch 593/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2350 - accuracy: 0.8951\n",
            "Epoch 594/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2350 - accuracy: 0.8953\n",
            "Epoch 595/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2350 - accuracy: 0.8951\n",
            "Epoch 596/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2350 - accuracy: 0.8953\n",
            "Epoch 597/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2350 - accuracy: 0.8951\n",
            "Epoch 598/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2350 - accuracy: 0.8953\n",
            "Epoch 599/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2350 - accuracy: 0.8951\n",
            "Epoch 600/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2350 - accuracy: 0.8953\n",
            "Epoch 601/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2350 - accuracy: 0.8951\n",
            "Epoch 602/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2350 - accuracy: 0.8953\n",
            "Epoch 603/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2350 - accuracy: 0.8951\n",
            "Epoch 604/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2350 - accuracy: 0.8953\n",
            "Epoch 605/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2350 - accuracy: 0.8951\n",
            "Epoch 606/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2350 - accuracy: 0.8953\n",
            "Epoch 607/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2350 - accuracy: 0.8951\n",
            "Epoch 608/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2350 - accuracy: 0.8953\n",
            "Epoch 609/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2350 - accuracy: 0.8951\n",
            "Epoch 610/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2350 - accuracy: 0.8952\n",
            "Epoch 611/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2350 - accuracy: 0.8951\n",
            "Epoch 612/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2350 - accuracy: 0.8952\n",
            "Epoch 613/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2350 - accuracy: 0.8951\n",
            "Epoch 614/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2350 - accuracy: 0.8952\n",
            "Epoch 615/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2350 - accuracy: 0.8951\n",
            "Epoch 616/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2350 - accuracy: 0.8952\n",
            "Epoch 617/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2350 - accuracy: 0.8951\n",
            "Epoch 618/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2350 - accuracy: 0.8952\n",
            "Epoch 619/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2350 - accuracy: 0.8951\n",
            "Epoch 620/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2350 - accuracy: 0.8952\n",
            "Epoch 621/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2350 - accuracy: 0.8951\n",
            "Epoch 622/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2350 - accuracy: 0.8952\n",
            "Epoch 623/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2350 - accuracy: 0.8951\n",
            "Epoch 624/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2350 - accuracy: 0.8952\n",
            "Epoch 625/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2350 - accuracy: 0.8951\n",
            "Epoch 626/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2350 - accuracy: 0.8952\n",
            "Epoch 627/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2350 - accuracy: 0.8951\n",
            "Epoch 628/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2350 - accuracy: 0.8952\n",
            "Epoch 629/1000\n",
            "51/51 [==============================] - 1s 22ms/step - loss: 0.2350 - accuracy: 0.8951\n",
            "Epoch 630/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2350 - accuracy: 0.8952\n",
            "Epoch 631/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2350 - accuracy: 0.8951\n",
            "Epoch 632/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2350 - accuracy: 0.8952\n",
            "Epoch 633/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2350 - accuracy: 0.8951\n",
            "Epoch 634/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2350 - accuracy: 0.8952\n",
            "Epoch 635/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2349 - accuracy: 0.8951\n",
            "Epoch 636/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2349 - accuracy: 0.8952\n",
            "Epoch 637/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2349 - accuracy: 0.8951\n",
            "Epoch 638/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2349 - accuracy: 0.8952\n",
            "Epoch 639/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2349 - accuracy: 0.8951\n",
            "Epoch 640/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2349 - accuracy: 0.8952\n",
            "Epoch 641/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2349 - accuracy: 0.8951\n",
            "Epoch 642/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2349 - accuracy: 0.8952\n",
            "Epoch 643/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2349 - accuracy: 0.8951\n",
            "Epoch 644/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2349 - accuracy: 0.8952\n",
            "Epoch 645/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2349 - accuracy: 0.8951\n",
            "Epoch 646/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2349 - accuracy: 0.8952\n",
            "Epoch 647/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2349 - accuracy: 0.8951\n",
            "Epoch 648/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2349 - accuracy: 0.8952\n",
            "Epoch 649/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2349 - accuracy: 0.8951\n",
            "Epoch 650/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2349 - accuracy: 0.8952\n",
            "Epoch 651/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2349 - accuracy: 0.8951\n",
            "Epoch 652/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2349 - accuracy: 0.8952\n",
            "Epoch 653/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2349 - accuracy: 0.8951\n",
            "Epoch 654/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2349 - accuracy: 0.8952\n",
            "Epoch 655/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2349 - accuracy: 0.8951\n",
            "Epoch 656/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2349 - accuracy: 0.8952\n",
            "Epoch 657/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2349 - accuracy: 0.8951\n",
            "Epoch 658/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2349 - accuracy: 0.8952\n",
            "Epoch 659/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2349 - accuracy: 0.8951\n",
            "Epoch 660/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2349 - accuracy: 0.8952\n",
            "Epoch 661/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2349 - accuracy: 0.8951\n",
            "Epoch 662/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2349 - accuracy: 0.8952\n",
            "Epoch 663/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2349 - accuracy: 0.8951\n",
            "Epoch 664/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2349 - accuracy: 0.8952\n",
            "Epoch 665/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2349 - accuracy: 0.8951\n",
            "Epoch 666/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2349 - accuracy: 0.8952\n",
            "Epoch 667/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2349 - accuracy: 0.8951\n",
            "Epoch 668/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2349 - accuracy: 0.8952\n",
            "Epoch 669/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2349 - accuracy: 0.8951\n",
            "Epoch 670/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2349 - accuracy: 0.8952\n",
            "Epoch 671/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2349 - accuracy: 0.8951\n",
            "Epoch 672/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2349 - accuracy: 0.8952\n",
            "Epoch 673/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2349 - accuracy: 0.8951\n",
            "Epoch 674/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2349 - accuracy: 0.8952\n",
            "Epoch 675/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2349 - accuracy: 0.8951\n",
            "Epoch 676/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2349 - accuracy: 0.8952\n",
            "Epoch 677/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2349 - accuracy: 0.8951\n",
            "Epoch 678/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2349 - accuracy: 0.8952\n",
            "Epoch 679/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2349 - accuracy: 0.8951\n",
            "Epoch 680/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2349 - accuracy: 0.8952\n",
            "Epoch 681/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2349 - accuracy: 0.8951\n",
            "Epoch 682/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2348 - accuracy: 0.8952\n",
            "Epoch 683/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2348 - accuracy: 0.8951\n",
            "Epoch 684/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2348 - accuracy: 0.8952\n",
            "Epoch 685/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2348 - accuracy: 0.8951\n",
            "Epoch 686/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2348 - accuracy: 0.8952\n",
            "Epoch 687/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2348 - accuracy: 0.8951\n",
            "Epoch 688/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2348 - accuracy: 0.8952\n",
            "Epoch 689/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2348 - accuracy: 0.8951\n",
            "Epoch 690/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2348 - accuracy: 0.8952\n",
            "Epoch 691/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2348 - accuracy: 0.8951\n",
            "Epoch 692/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2348 - accuracy: 0.8953\n",
            "Epoch 693/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2348 - accuracy: 0.8951\n",
            "Epoch 694/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2348 - accuracy: 0.8953\n",
            "Epoch 695/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2348 - accuracy: 0.8951\n",
            "Epoch 696/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2348 - accuracy: 0.8953\n",
            "Epoch 697/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2348 - accuracy: 0.8951\n",
            "Epoch 698/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2348 - accuracy: 0.8953\n",
            "Epoch 699/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2348 - accuracy: 0.8952\n",
            "Epoch 700/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2348 - accuracy: 0.8953\n",
            "Epoch 701/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2348 - accuracy: 0.8952\n",
            "Epoch 702/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2348 - accuracy: 0.8953\n",
            "Epoch 703/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2348 - accuracy: 0.8952\n",
            "Epoch 704/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2348 - accuracy: 0.8953\n",
            "Epoch 705/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2348 - accuracy: 0.8952\n",
            "Epoch 706/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2348 - accuracy: 0.8953\n",
            "Epoch 707/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2348 - accuracy: 0.8952\n",
            "Epoch 708/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2348 - accuracy: 0.8953\n",
            "Epoch 709/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2348 - accuracy: 0.8952\n",
            "Epoch 710/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2348 - accuracy: 0.8953\n",
            "Epoch 711/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2348 - accuracy: 0.8952\n",
            "Epoch 712/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2348 - accuracy: 0.8953\n",
            "Epoch 713/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2348 - accuracy: 0.8952\n",
            "Epoch 714/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2348 - accuracy: 0.8953\n",
            "Epoch 715/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2348 - accuracy: 0.8952\n",
            "Epoch 716/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2348 - accuracy: 0.8953\n",
            "Epoch 717/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2348 - accuracy: 0.8952\n",
            "Epoch 718/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2348 - accuracy: 0.8953\n",
            "Epoch 719/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2348 - accuracy: 0.8952\n",
            "Epoch 720/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2348 - accuracy: 0.8953\n",
            "Epoch 721/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2348 - accuracy: 0.8952\n",
            "Epoch 722/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2348 - accuracy: 0.8953\n",
            "Epoch 723/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2348 - accuracy: 0.8952\n",
            "Epoch 724/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2348 - accuracy: 0.8953\n",
            "Epoch 725/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2348 - accuracy: 0.8953\n",
            "Epoch 726/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2348 - accuracy: 0.8954\n",
            "Epoch 727/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2348 - accuracy: 0.8953\n",
            "Epoch 728/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2348 - accuracy: 0.8954\n",
            "Epoch 729/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2348 - accuracy: 0.8953\n",
            "Epoch 730/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2348 - accuracy: 0.8954\n",
            "Epoch 731/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2348 - accuracy: 0.8953\n",
            "Epoch 732/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2348 - accuracy: 0.8954\n",
            "Epoch 733/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2347 - accuracy: 0.8953\n",
            "Epoch 734/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2347 - accuracy: 0.8954\n",
            "Epoch 735/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.2347 - accuracy: 0.8953\n",
            "Epoch 736/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2347 - accuracy: 0.8954\n",
            "Epoch 737/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2347 - accuracy: 0.8953\n",
            "Epoch 738/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2347 - accuracy: 0.8954\n",
            "Epoch 739/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2347 - accuracy: 0.8953\n",
            "Epoch 740/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2347 - accuracy: 0.8954\n",
            "Epoch 741/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2347 - accuracy: 0.8953\n",
            "Epoch 742/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.2347 - accuracy: 0.8954\n",
            "Epoch 743/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2347 - accuracy: 0.8953\n",
            "Epoch 744/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2347 - accuracy: 0.8954\n",
            "Epoch 745/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2347 - accuracy: 0.8953\n",
            "Epoch 746/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2347 - accuracy: 0.8954\n",
            "Epoch 747/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2347 - accuracy: 0.8953\n",
            "Epoch 748/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2347 - accuracy: 0.8954\n",
            "Epoch 749/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2347 - accuracy: 0.8953\n",
            "Epoch 750/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.2347 - accuracy: 0.8954\n",
            "Epoch 751/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2347 - accuracy: 0.8953\n",
            "Epoch 752/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2347 - accuracy: 0.8954\n",
            "Epoch 753/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2347 - accuracy: 0.8953\n",
            "Epoch 754/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2347 - accuracy: 0.8954\n",
            "Epoch 755/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2347 - accuracy: 0.8953\n",
            "Epoch 756/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2347 - accuracy: 0.8954\n",
            "Epoch 757/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2347 - accuracy: 0.8953\n",
            "Epoch 758/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2347 - accuracy: 0.8954\n",
            "Epoch 759/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2347 - accuracy: 0.8953\n",
            "Epoch 760/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2347 - accuracy: 0.8955\n",
            "Epoch 761/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2347 - accuracy: 0.8953\n",
            "Epoch 762/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2347 - accuracy: 0.8955\n",
            "Epoch 763/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2347 - accuracy: 0.8954\n",
            "Epoch 764/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2347 - accuracy: 0.8955\n",
            "Epoch 765/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2347 - accuracy: 0.8954\n",
            "Epoch 766/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2347 - accuracy: 0.8955\n",
            "Epoch 767/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2347 - accuracy: 0.8954\n",
            "Epoch 768/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2347 - accuracy: 0.8955\n",
            "Epoch 769/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2347 - accuracy: 0.8954\n",
            "Epoch 770/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2347 - accuracy: 0.8955\n",
            "Epoch 771/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2347 - accuracy: 0.8954\n",
            "Epoch 772/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2347 - accuracy: 0.8955\n",
            "Epoch 773/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2347 - accuracy: 0.8954\n",
            "Epoch 774/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2347 - accuracy: 0.8955\n",
            "Epoch 775/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2347 - accuracy: 0.8954\n",
            "Epoch 776/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2347 - accuracy: 0.8955\n",
            "Epoch 777/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2347 - accuracy: 0.8954\n",
            "Epoch 778/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2347 - accuracy: 0.8955\n",
            "Epoch 779/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2347 - accuracy: 0.8954\n",
            "Epoch 780/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2347 - accuracy: 0.8955\n",
            "Epoch 781/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2347 - accuracy: 0.8954\n",
            "Epoch 782/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2347 - accuracy: 0.8955\n",
            "Epoch 783/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2347 - accuracy: 0.8954\n",
            "Epoch 784/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2347 - accuracy: 0.8955\n",
            "Epoch 785/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2347 - accuracy: 0.8954\n",
            "Epoch 786/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2347 - accuracy: 0.8955\n",
            "Epoch 787/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2347 - accuracy: 0.8954\n",
            "Epoch 788/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8955\n",
            "Epoch 789/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8954\n",
            "Epoch 790/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2346 - accuracy: 0.8955\n",
            "Epoch 791/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8954\n",
            "Epoch 792/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8955\n",
            "Epoch 793/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2346 - accuracy: 0.8954\n",
            "Epoch 794/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8955\n",
            "Epoch 795/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2346 - accuracy: 0.8954\n",
            "Epoch 796/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2346 - accuracy: 0.8955\n",
            "Epoch 797/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8954\n",
            "Epoch 798/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8955\n",
            "Epoch 799/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8954\n",
            "Epoch 800/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8955\n",
            "Epoch 801/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8954\n",
            "Epoch 802/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8955\n",
            "Epoch 803/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2346 - accuracy: 0.8954\n",
            "Epoch 804/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8955\n",
            "Epoch 805/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2346 - accuracy: 0.8954\n",
            "Epoch 806/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2346 - accuracy: 0.8955\n",
            "Epoch 807/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2346 - accuracy: 0.8954\n",
            "Epoch 808/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8954\n",
            "Epoch 809/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8953\n",
            "Epoch 810/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8954\n",
            "Epoch 811/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2346 - accuracy: 0.8953\n",
            "Epoch 812/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8954\n",
            "Epoch 813/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2346 - accuracy: 0.8953\n",
            "Epoch 814/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8954\n",
            "Epoch 815/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8953\n",
            "Epoch 816/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8954\n",
            "Epoch 817/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8953\n",
            "Epoch 818/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8954\n",
            "Epoch 819/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2346 - accuracy: 0.8953\n",
            "Epoch 820/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8954\n",
            "Epoch 821/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8953\n",
            "Epoch 822/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2346 - accuracy: 0.8954\n",
            "Epoch 823/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2346 - accuracy: 0.8953\n",
            "Epoch 824/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8954\n",
            "Epoch 825/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8953\n",
            "Epoch 826/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8954\n",
            "Epoch 827/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8953\n",
            "Epoch 828/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2346 - accuracy: 0.8955\n",
            "Epoch 829/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2346 - accuracy: 0.8954\n",
            "Epoch 830/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8955\n",
            "Epoch 831/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8954\n",
            "Epoch 832/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8955\n",
            "Epoch 833/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8954\n",
            "Epoch 834/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8955\n",
            "Epoch 835/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8954\n",
            "Epoch 836/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8955\n",
            "Epoch 837/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8954\n",
            "Epoch 838/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8955\n",
            "Epoch 839/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2346 - accuracy: 0.8954\n",
            "Epoch 840/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8955\n",
            "Epoch 841/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8954\n",
            "Epoch 842/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8955\n",
            "Epoch 843/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8954\n",
            "Epoch 844/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8955\n",
            "Epoch 845/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8954\n",
            "Epoch 846/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2346 - accuracy: 0.8955\n",
            "Epoch 847/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2346 - accuracy: 0.8954\n",
            "Epoch 848/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 849/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 850/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 851/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 852/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 853/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 854/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 855/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 856/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 857/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 858/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 859/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 860/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 861/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 862/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 863/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 864/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 865/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 866/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 867/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 868/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 869/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 870/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8956\n",
            "Epoch 871/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 872/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8956\n",
            "Epoch 873/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 874/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8956\n",
            "Epoch 875/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 876/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8956\n",
            "Epoch 877/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 878/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2345 - accuracy: 0.8956\n",
            "Epoch 879/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 880/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8956\n",
            "Epoch 881/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 882/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8956\n",
            "Epoch 883/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 884/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2345 - accuracy: 0.8956\n",
            "Epoch 885/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 886/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8956\n",
            "Epoch 887/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 888/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8956\n",
            "Epoch 889/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 890/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2345 - accuracy: 0.8956\n",
            "Epoch 891/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 892/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2345 - accuracy: 0.8956\n",
            "Epoch 893/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 894/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8956\n",
            "Epoch 895/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 896/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2345 - accuracy: 0.8956\n",
            "Epoch 897/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 898/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2345 - accuracy: 0.8956\n",
            "Epoch 899/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 900/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2345 - accuracy: 0.8956\n",
            "Epoch 901/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8954\n",
            "Epoch 902/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 903/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8954\n",
            "Epoch 904/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 905/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8954\n",
            "Epoch 906/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 907/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2345 - accuracy: 0.8954\n",
            "Epoch 908/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 909/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2345 - accuracy: 0.8954\n",
            "Epoch 910/1000\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 911/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 912/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 913/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2345 - accuracy: 0.8955\n",
            "Epoch 914/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 915/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 916/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 917/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 918/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 919/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 920/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 921/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 922/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 923/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 924/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 925/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 926/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 927/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 928/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 929/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 930/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 931/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 932/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 933/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 934/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 935/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 936/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 937/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 938/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 939/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 940/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 941/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 942/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 943/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 944/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 945/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 946/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 947/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 948/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 949/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 950/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 951/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 952/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 953/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 954/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 955/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 956/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 957/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 958/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 959/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 960/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2344 - accuracy: 0.8956\n",
            "Epoch 961/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 962/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.2344 - accuracy: 0.8956\n",
            "Epoch 963/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 964/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2344 - accuracy: 0.8956\n",
            "Epoch 965/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 966/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2344 - accuracy: 0.8956\n",
            "Epoch 967/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 968/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2344 - accuracy: 0.8956\n",
            "Epoch 969/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 970/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.2344 - accuracy: 0.8956\n",
            "Epoch 971/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 972/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.2344 - accuracy: 0.8956\n",
            "Epoch 973/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 974/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2344 - accuracy: 0.8956\n",
            "Epoch 975/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 976/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2344 - accuracy: 0.8956\n",
            "Epoch 977/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 978/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2344 - accuracy: 0.8956\n",
            "Epoch 979/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 980/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.2344 - accuracy: 0.8956\n",
            "Epoch 981/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 982/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2344 - accuracy: 0.8956\n",
            "Epoch 983/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2344 - accuracy: 0.8955\n",
            "Epoch 984/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.2344 - accuracy: 0.8956\n",
            "Epoch 985/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2343 - accuracy: 0.8955\n",
            "Epoch 986/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2343 - accuracy: 0.8956\n",
            "Epoch 987/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2343 - accuracy: 0.8955\n",
            "Epoch 988/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2343 - accuracy: 0.8956\n",
            "Epoch 989/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2343 - accuracy: 0.8955\n",
            "Epoch 990/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2343 - accuracy: 0.8956\n",
            "Epoch 991/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2343 - accuracy: 0.8955\n",
            "Epoch 992/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2343 - accuracy: 0.8956\n",
            "Epoch 993/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2343 - accuracy: 0.8955\n",
            "Epoch 994/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2343 - accuracy: 0.8956\n",
            "Epoch 995/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2343 - accuracy: 0.8955\n",
            "Epoch 996/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2343 - accuracy: 0.8956\n",
            "Epoch 997/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.2343 - accuracy: 0.8955\n",
            "Epoch 998/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2343 - accuracy: 0.8956\n",
            "Epoch 999/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.2343 - accuracy: 0.8955\n",
            "Epoch 1000/1000\n",
            "51/51 [==============================] - 1s 24ms/step - loss: 0.2343 - accuracy: 0.8956\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd0031f1790>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6uz1d3FtEBK"
      },
      "source": [
        "weights1 = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
        "weights2 = word2vec.get_layer('ctxt_embedding').get_weights()[0]\n",
        "vocab = vectorize_layer.get_vocabulary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JkX0yDjJFy-"
      },
      "source": [
        "### Having a look at the weight vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "0teXKRK3tFqn",
        "outputId": "4c6b7245-0fa7-4767-e58d-6c8be7fd12ef"
      },
      "source": [
        "pd.DataFrame(weights1).head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>472</th>\n",
              "      <th>473</th>\n",
              "      <th>474</th>\n",
              "      <th>475</th>\n",
              "      <th>476</th>\n",
              "      <th>477</th>\n",
              "      <th>478</th>\n",
              "      <th>479</th>\n",
              "      <th>480</th>\n",
              "      <th>481</th>\n",
              "      <th>482</th>\n",
              "      <th>483</th>\n",
              "      <th>484</th>\n",
              "      <th>485</th>\n",
              "      <th>486</th>\n",
              "      <th>487</th>\n",
              "      <th>488</th>\n",
              "      <th>489</th>\n",
              "      <th>490</th>\n",
              "      <th>491</th>\n",
              "      <th>492</th>\n",
              "      <th>493</th>\n",
              "      <th>494</th>\n",
              "      <th>495</th>\n",
              "      <th>496</th>\n",
              "      <th>497</th>\n",
              "      <th>498</th>\n",
              "      <th>499</th>\n",
              "      <th>500</th>\n",
              "      <th>501</th>\n",
              "      <th>502</th>\n",
              "      <th>503</th>\n",
              "      <th>504</th>\n",
              "      <th>505</th>\n",
              "      <th>506</th>\n",
              "      <th>507</th>\n",
              "      <th>508</th>\n",
              "      <th>509</th>\n",
              "      <th>510</th>\n",
              "      <th>511</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.025069</td>\n",
              "      <td>-0.000066</td>\n",
              "      <td>-0.031654</td>\n",
              "      <td>0.014237</td>\n",
              "      <td>-0.023404</td>\n",
              "      <td>0.016262</td>\n",
              "      <td>0.000415</td>\n",
              "      <td>0.037340</td>\n",
              "      <td>0.025396</td>\n",
              "      <td>0.031867</td>\n",
              "      <td>-0.047326</td>\n",
              "      <td>0.047726</td>\n",
              "      <td>0.036843</td>\n",
              "      <td>-0.045014</td>\n",
              "      <td>0.013775</td>\n",
              "      <td>0.005584</td>\n",
              "      <td>-0.007188</td>\n",
              "      <td>0.040271</td>\n",
              "      <td>-0.040543</td>\n",
              "      <td>-0.037266</td>\n",
              "      <td>0.039560</td>\n",
              "      <td>0.046198</td>\n",
              "      <td>-0.030002</td>\n",
              "      <td>-0.002558</td>\n",
              "      <td>-0.032246</td>\n",
              "      <td>-0.027968</td>\n",
              "      <td>-0.021535</td>\n",
              "      <td>-0.021913</td>\n",
              "      <td>0.003012</td>\n",
              "      <td>-0.005837</td>\n",
              "      <td>-0.037709</td>\n",
              "      <td>-0.001804</td>\n",
              "      <td>0.034664</td>\n",
              "      <td>0.041707</td>\n",
              "      <td>0.026736</td>\n",
              "      <td>0.005130</td>\n",
              "      <td>0.014264</td>\n",
              "      <td>-0.040133</td>\n",
              "      <td>0.039527</td>\n",
              "      <td>0.003013</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.004466</td>\n",
              "      <td>0.010940</td>\n",
              "      <td>0.008247</td>\n",
              "      <td>-0.018976</td>\n",
              "      <td>0.049164</td>\n",
              "      <td>-0.013599</td>\n",
              "      <td>-0.019706</td>\n",
              "      <td>-0.041574</td>\n",
              "      <td>-0.045537</td>\n",
              "      <td>0.024910</td>\n",
              "      <td>0.034660</td>\n",
              "      <td>0.016334</td>\n",
              "      <td>0.018006</td>\n",
              "      <td>0.012014</td>\n",
              "      <td>-0.039957</td>\n",
              "      <td>-0.021537</td>\n",
              "      <td>-0.025172</td>\n",
              "      <td>-0.008531</td>\n",
              "      <td>0.029554</td>\n",
              "      <td>-0.046537</td>\n",
              "      <td>0.033630</td>\n",
              "      <td>0.029231</td>\n",
              "      <td>-0.029821</td>\n",
              "      <td>0.038628</td>\n",
              "      <td>-0.034763</td>\n",
              "      <td>0.040339</td>\n",
              "      <td>0.006135</td>\n",
              "      <td>-0.037127</td>\n",
              "      <td>-0.034391</td>\n",
              "      <td>0.019031</td>\n",
              "      <td>0.043469</td>\n",
              "      <td>0.021912</td>\n",
              "      <td>0.033245</td>\n",
              "      <td>-0.041135</td>\n",
              "      <td>0.033065</td>\n",
              "      <td>0.026778</td>\n",
              "      <td>-0.000642</td>\n",
              "      <td>-0.041829</td>\n",
              "      <td>0.043674</td>\n",
              "      <td>0.017925</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.033276</td>\n",
              "      <td>-0.040155</td>\n",
              "      <td>0.039880</td>\n",
              "      <td>-0.037529</td>\n",
              "      <td>-0.021232</td>\n",
              "      <td>-0.046790</td>\n",
              "      <td>-0.014804</td>\n",
              "      <td>-0.000618</td>\n",
              "      <td>0.033305</td>\n",
              "      <td>0.037357</td>\n",
              "      <td>0.023398</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>0.023055</td>\n",
              "      <td>-0.021867</td>\n",
              "      <td>0.030498</td>\n",
              "      <td>-0.007698</td>\n",
              "      <td>-0.045484</td>\n",
              "      <td>0.003096</td>\n",
              "      <td>-0.007141</td>\n",
              "      <td>0.030076</td>\n",
              "      <td>-0.025715</td>\n",
              "      <td>-0.019170</td>\n",
              "      <td>0.032367</td>\n",
              "      <td>0.048124</td>\n",
              "      <td>0.010594</td>\n",
              "      <td>0.014132</td>\n",
              "      <td>-0.011346</td>\n",
              "      <td>-0.032680</td>\n",
              "      <td>-0.033209</td>\n",
              "      <td>0.009414</td>\n",
              "      <td>0.049307</td>\n",
              "      <td>-0.043517</td>\n",
              "      <td>0.037503</td>\n",
              "      <td>-0.009895</td>\n",
              "      <td>-0.036917</td>\n",
              "      <td>0.029667</td>\n",
              "      <td>-0.030794</td>\n",
              "      <td>-0.030562</td>\n",
              "      <td>0.006142</td>\n",
              "      <td>-0.036629</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.018165</td>\n",
              "      <td>-0.010695</td>\n",
              "      <td>-0.043080</td>\n",
              "      <td>0.003355</td>\n",
              "      <td>-0.034861</td>\n",
              "      <td>0.025250</td>\n",
              "      <td>-0.020540</td>\n",
              "      <td>-0.005703</td>\n",
              "      <td>0.005456</td>\n",
              "      <td>-0.013887</td>\n",
              "      <td>-0.001461</td>\n",
              "      <td>0.020453</td>\n",
              "      <td>0.038282</td>\n",
              "      <td>0.049951</td>\n",
              "      <td>0.046041</td>\n",
              "      <td>-0.001067</td>\n",
              "      <td>0.041082</td>\n",
              "      <td>0.008728</td>\n",
              "      <td>0.032247</td>\n",
              "      <td>-0.006020</td>\n",
              "      <td>0.026796</td>\n",
              "      <td>-0.017289</td>\n",
              "      <td>-0.043549</td>\n",
              "      <td>0.033042</td>\n",
              "      <td>-0.042079</td>\n",
              "      <td>-0.020828</td>\n",
              "      <td>-0.033788</td>\n",
              "      <td>-0.007714</td>\n",
              "      <td>-0.036246</td>\n",
              "      <td>0.030735</td>\n",
              "      <td>0.004748</td>\n",
              "      <td>0.017974</td>\n",
              "      <td>-0.029957</td>\n",
              "      <td>-0.038265</td>\n",
              "      <td>0.011645</td>\n",
              "      <td>-0.013240</td>\n",
              "      <td>0.009234</td>\n",
              "      <td>0.026803</td>\n",
              "      <td>-0.007956</td>\n",
              "      <td>-0.028266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.473700</td>\n",
              "      <td>-0.035115</td>\n",
              "      <td>-0.115214</td>\n",
              "      <td>0.043739</td>\n",
              "      <td>0.077085</td>\n",
              "      <td>-0.010705</td>\n",
              "      <td>0.160666</td>\n",
              "      <td>-0.012081</td>\n",
              "      <td>0.214587</td>\n",
              "      <td>-0.061020</td>\n",
              "      <td>0.034972</td>\n",
              "      <td>0.426369</td>\n",
              "      <td>-0.067216</td>\n",
              "      <td>0.087028</td>\n",
              "      <td>-0.341544</td>\n",
              "      <td>-0.181459</td>\n",
              "      <td>0.068603</td>\n",
              "      <td>0.505857</td>\n",
              "      <td>-0.020465</td>\n",
              "      <td>0.257249</td>\n",
              "      <td>-0.158001</td>\n",
              "      <td>-0.265266</td>\n",
              "      <td>-0.209810</td>\n",
              "      <td>0.178999</td>\n",
              "      <td>0.008145</td>\n",
              "      <td>-0.128907</td>\n",
              "      <td>-0.623444</td>\n",
              "      <td>0.047955</td>\n",
              "      <td>-0.133734</td>\n",
              "      <td>0.271102</td>\n",
              "      <td>-0.010336</td>\n",
              "      <td>0.119353</td>\n",
              "      <td>0.005074</td>\n",
              "      <td>-0.213583</td>\n",
              "      <td>0.036529</td>\n",
              "      <td>0.028358</td>\n",
              "      <td>-0.097838</td>\n",
              "      <td>-0.313557</td>\n",
              "      <td>-0.159025</td>\n",
              "      <td>-0.056744</td>\n",
              "      <td>...</td>\n",
              "      <td>0.207484</td>\n",
              "      <td>0.034684</td>\n",
              "      <td>0.196952</td>\n",
              "      <td>0.310528</td>\n",
              "      <td>-0.096678</td>\n",
              "      <td>-0.144341</td>\n",
              "      <td>0.001143</td>\n",
              "      <td>0.164145</td>\n",
              "      <td>0.008508</td>\n",
              "      <td>0.019755</td>\n",
              "      <td>0.144409</td>\n",
              "      <td>0.128462</td>\n",
              "      <td>-0.313972</td>\n",
              "      <td>0.134786</td>\n",
              "      <td>-0.168139</td>\n",
              "      <td>0.189559</td>\n",
              "      <td>0.272065</td>\n",
              "      <td>-0.321633</td>\n",
              "      <td>-0.268011</td>\n",
              "      <td>-0.072079</td>\n",
              "      <td>0.032299</td>\n",
              "      <td>0.235308</td>\n",
              "      <td>0.013433</td>\n",
              "      <td>0.574690</td>\n",
              "      <td>-0.081237</td>\n",
              "      <td>0.569061</td>\n",
              "      <td>0.550606</td>\n",
              "      <td>0.006310</td>\n",
              "      <td>0.194956</td>\n",
              "      <td>0.202981</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>-0.615630</td>\n",
              "      <td>0.263323</td>\n",
              "      <td>0.201558</td>\n",
              "      <td>0.101438</td>\n",
              "      <td>-0.209405</td>\n",
              "      <td>0.078792</td>\n",
              "      <td>0.357427</td>\n",
              "      <td>0.317136</td>\n",
              "      <td>-0.040174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.440670</td>\n",
              "      <td>-0.098615</td>\n",
              "      <td>-0.116582</td>\n",
              "      <td>-0.102313</td>\n",
              "      <td>-0.000562</td>\n",
              "      <td>0.269414</td>\n",
              "      <td>-0.040820</td>\n",
              "      <td>-0.331258</td>\n",
              "      <td>-0.189788</td>\n",
              "      <td>0.281291</td>\n",
              "      <td>-0.089125</td>\n",
              "      <td>-0.139188</td>\n",
              "      <td>-0.078831</td>\n",
              "      <td>0.096149</td>\n",
              "      <td>-0.239318</td>\n",
              "      <td>-0.188039</td>\n",
              "      <td>0.823545</td>\n",
              "      <td>0.054818</td>\n",
              "      <td>0.152715</td>\n",
              "      <td>0.233068</td>\n",
              "      <td>0.064769</td>\n",
              "      <td>-0.059498</td>\n",
              "      <td>0.207232</td>\n",
              "      <td>-0.045204</td>\n",
              "      <td>-0.016295</td>\n",
              "      <td>-0.054589</td>\n",
              "      <td>-0.196149</td>\n",
              "      <td>0.190394</td>\n",
              "      <td>0.048291</td>\n",
              "      <td>0.098304</td>\n",
              "      <td>0.029888</td>\n",
              "      <td>-0.068148</td>\n",
              "      <td>0.032743</td>\n",
              "      <td>0.099608</td>\n",
              "      <td>0.099735</td>\n",
              "      <td>-0.135610</td>\n",
              "      <td>0.456175</td>\n",
              "      <td>0.175792</td>\n",
              "      <td>0.394563</td>\n",
              "      <td>0.117918</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.310030</td>\n",
              "      <td>0.029988</td>\n",
              "      <td>0.060547</td>\n",
              "      <td>-0.074243</td>\n",
              "      <td>-0.151402</td>\n",
              "      <td>0.209332</td>\n",
              "      <td>0.194599</td>\n",
              "      <td>-0.084671</td>\n",
              "      <td>-0.055989</td>\n",
              "      <td>0.111269</td>\n",
              "      <td>-0.017629</td>\n",
              "      <td>0.114655</td>\n",
              "      <td>-0.035944</td>\n",
              "      <td>-0.051640</td>\n",
              "      <td>-0.231223</td>\n",
              "      <td>-0.196331</td>\n",
              "      <td>0.084611</td>\n",
              "      <td>-0.381267</td>\n",
              "      <td>0.175043</td>\n",
              "      <td>0.265778</td>\n",
              "      <td>0.093578</td>\n",
              "      <td>0.033352</td>\n",
              "      <td>0.042853</td>\n",
              "      <td>-0.355492</td>\n",
              "      <td>0.218160</td>\n",
              "      <td>0.159865</td>\n",
              "      <td>-0.055890</td>\n",
              "      <td>0.023394</td>\n",
              "      <td>0.175363</td>\n",
              "      <td>-0.012095</td>\n",
              "      <td>0.356672</td>\n",
              "      <td>-0.146754</td>\n",
              "      <td>0.021964</td>\n",
              "      <td>-0.071147</td>\n",
              "      <td>0.186217</td>\n",
              "      <td>0.031064</td>\n",
              "      <td>0.187226</td>\n",
              "      <td>-0.061930</td>\n",
              "      <td>0.021166</td>\n",
              "      <td>0.251777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.208589</td>\n",
              "      <td>0.127709</td>\n",
              "      <td>0.344170</td>\n",
              "      <td>-0.000771</td>\n",
              "      <td>0.033800</td>\n",
              "      <td>0.439418</td>\n",
              "      <td>-0.116590</td>\n",
              "      <td>-0.198248</td>\n",
              "      <td>0.082710</td>\n",
              "      <td>0.018599</td>\n",
              "      <td>-0.261474</td>\n",
              "      <td>-0.339024</td>\n",
              "      <td>-0.289652</td>\n",
              "      <td>0.110530</td>\n",
              "      <td>0.211881</td>\n",
              "      <td>0.053806</td>\n",
              "      <td>0.413691</td>\n",
              "      <td>0.397127</td>\n",
              "      <td>0.246772</td>\n",
              "      <td>0.117306</td>\n",
              "      <td>-0.043235</td>\n",
              "      <td>0.153355</td>\n",
              "      <td>0.053782</td>\n",
              "      <td>0.004164</td>\n",
              "      <td>-0.163596</td>\n",
              "      <td>-0.132820</td>\n",
              "      <td>0.143645</td>\n",
              "      <td>-0.093748</td>\n",
              "      <td>-0.025149</td>\n",
              "      <td>0.019549</td>\n",
              "      <td>-0.346475</td>\n",
              "      <td>-0.188159</td>\n",
              "      <td>0.133641</td>\n",
              "      <td>0.029359</td>\n",
              "      <td>0.117848</td>\n",
              "      <td>0.046629</td>\n",
              "      <td>0.036872</td>\n",
              "      <td>-0.082961</td>\n",
              "      <td>0.188183</td>\n",
              "      <td>-0.153225</td>\n",
              "      <td>...</td>\n",
              "      <td>0.172842</td>\n",
              "      <td>-0.107519</td>\n",
              "      <td>0.235212</td>\n",
              "      <td>-0.271674</td>\n",
              "      <td>-0.108227</td>\n",
              "      <td>0.244141</td>\n",
              "      <td>0.096219</td>\n",
              "      <td>0.201705</td>\n",
              "      <td>0.244067</td>\n",
              "      <td>0.081439</td>\n",
              "      <td>0.303160</td>\n",
              "      <td>0.061275</td>\n",
              "      <td>-0.101075</td>\n",
              "      <td>0.563330</td>\n",
              "      <td>-0.271337</td>\n",
              "      <td>-0.081314</td>\n",
              "      <td>0.031425</td>\n",
              "      <td>-0.323617</td>\n",
              "      <td>-0.024409</td>\n",
              "      <td>0.167554</td>\n",
              "      <td>0.172466</td>\n",
              "      <td>0.207617</td>\n",
              "      <td>-0.241216</td>\n",
              "      <td>-0.273679</td>\n",
              "      <td>0.053117</td>\n",
              "      <td>0.121748</td>\n",
              "      <td>0.084459</td>\n",
              "      <td>-0.029616</td>\n",
              "      <td>0.118503</td>\n",
              "      <td>-0.300720</td>\n",
              "      <td>-0.018758</td>\n",
              "      <td>-0.296802</td>\n",
              "      <td>-0.377722</td>\n",
              "      <td>-0.003147</td>\n",
              "      <td>-0.004413</td>\n",
              "      <td>0.258336</td>\n",
              "      <td>0.100834</td>\n",
              "      <td>0.220300</td>\n",
              "      <td>-0.106326</td>\n",
              "      <td>0.193770</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 512 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2    ...       509       510       511\n",
              "0 -0.025069 -0.000066 -0.031654  ... -0.041829  0.043674  0.017925\n",
              "1  0.033276 -0.040155  0.039880  ...  0.026803 -0.007956 -0.028266\n",
              "2 -0.473700 -0.035115 -0.115214  ...  0.357427  0.317136 -0.040174\n",
              "3 -0.440670 -0.098615 -0.116582  ... -0.061930  0.021166  0.251777\n",
              "4 -0.208589  0.127709  0.344170  ...  0.220300 -0.106326  0.193770\n",
              "\n",
              "[5 rows x 512 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "TZgpxsLfueMn",
        "outputId": "dad94cb4-0aeb-41ca-e316-a61bb2ddb28c"
      },
      "source": [
        "pd.DataFrame(weights2).head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>472</th>\n",
              "      <th>473</th>\n",
              "      <th>474</th>\n",
              "      <th>475</th>\n",
              "      <th>476</th>\n",
              "      <th>477</th>\n",
              "      <th>478</th>\n",
              "      <th>479</th>\n",
              "      <th>480</th>\n",
              "      <th>481</th>\n",
              "      <th>482</th>\n",
              "      <th>483</th>\n",
              "      <th>484</th>\n",
              "      <th>485</th>\n",
              "      <th>486</th>\n",
              "      <th>487</th>\n",
              "      <th>488</th>\n",
              "      <th>489</th>\n",
              "      <th>490</th>\n",
              "      <th>491</th>\n",
              "      <th>492</th>\n",
              "      <th>493</th>\n",
              "      <th>494</th>\n",
              "      <th>495</th>\n",
              "      <th>496</th>\n",
              "      <th>497</th>\n",
              "      <th>498</th>\n",
              "      <th>499</th>\n",
              "      <th>500</th>\n",
              "      <th>501</th>\n",
              "      <th>502</th>\n",
              "      <th>503</th>\n",
              "      <th>504</th>\n",
              "      <th>505</th>\n",
              "      <th>506</th>\n",
              "      <th>507</th>\n",
              "      <th>508</th>\n",
              "      <th>509</th>\n",
              "      <th>510</th>\n",
              "      <th>511</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.444623</td>\n",
              "      <td>0.810958</td>\n",
              "      <td>-0.750832</td>\n",
              "      <td>0.955650</td>\n",
              "      <td>-0.254889</td>\n",
              "      <td>1.213572</td>\n",
              "      <td>-0.648940</td>\n",
              "      <td>0.598615</td>\n",
              "      <td>0.624597</td>\n",
              "      <td>-1.371282</td>\n",
              "      <td>0.751649</td>\n",
              "      <td>-0.399606</td>\n",
              "      <td>0.034596</td>\n",
              "      <td>1.053032</td>\n",
              "      <td>-0.827611</td>\n",
              "      <td>-0.742030</td>\n",
              "      <td>-0.357564</td>\n",
              "      <td>-0.906358</td>\n",
              "      <td>-0.780714</td>\n",
              "      <td>0.573354</td>\n",
              "      <td>0.274027</td>\n",
              "      <td>-0.538622</td>\n",
              "      <td>0.660641</td>\n",
              "      <td>-0.292488</td>\n",
              "      <td>-0.566459</td>\n",
              "      <td>-0.469488</td>\n",
              "      <td>-0.426814</td>\n",
              "      <td>0.782402</td>\n",
              "      <td>-1.028512</td>\n",
              "      <td>-0.749130</td>\n",
              "      <td>0.481691</td>\n",
              "      <td>0.790016</td>\n",
              "      <td>0.823890</td>\n",
              "      <td>-0.678396</td>\n",
              "      <td>-1.108863</td>\n",
              "      <td>-0.394120</td>\n",
              "      <td>-0.804331</td>\n",
              "      <td>0.987827</td>\n",
              "      <td>-0.789381</td>\n",
              "      <td>0.857046</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.643091</td>\n",
              "      <td>0.097824</td>\n",
              "      <td>0.827885</td>\n",
              "      <td>-0.694925</td>\n",
              "      <td>-1.269891</td>\n",
              "      <td>-0.703816</td>\n",
              "      <td>1.137534</td>\n",
              "      <td>0.255611</td>\n",
              "      <td>-0.708437</td>\n",
              "      <td>-0.787004</td>\n",
              "      <td>-1.008073</td>\n",
              "      <td>0.627863</td>\n",
              "      <td>0.805385</td>\n",
              "      <td>-0.678784</td>\n",
              "      <td>-1.069138</td>\n",
              "      <td>-0.375515</td>\n",
              "      <td>-0.203436</td>\n",
              "      <td>0.107998</td>\n",
              "      <td>-1.270033</td>\n",
              "      <td>0.705995</td>\n",
              "      <td>-0.491772</td>\n",
              "      <td>-0.613865</td>\n",
              "      <td>0.095244</td>\n",
              "      <td>0.388779</td>\n",
              "      <td>-0.261409</td>\n",
              "      <td>0.789035</td>\n",
              "      <td>-0.941272</td>\n",
              "      <td>-0.159288</td>\n",
              "      <td>-0.449129</td>\n",
              "      <td>-0.409235</td>\n",
              "      <td>0.545247</td>\n",
              "      <td>0.422500</td>\n",
              "      <td>0.188140</td>\n",
              "      <td>-0.717131</td>\n",
              "      <td>-0.360714</td>\n",
              "      <td>-0.998716</td>\n",
              "      <td>-0.805839</td>\n",
              "      <td>0.453878</td>\n",
              "      <td>0.716630</td>\n",
              "      <td>-0.845961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.393092</td>\n",
              "      <td>0.603314</td>\n",
              "      <td>-0.819966</td>\n",
              "      <td>0.895397</td>\n",
              "      <td>-0.398864</td>\n",
              "      <td>1.527663</td>\n",
              "      <td>-0.528527</td>\n",
              "      <td>0.358777</td>\n",
              "      <td>0.793830</td>\n",
              "      <td>-1.407863</td>\n",
              "      <td>0.777532</td>\n",
              "      <td>-0.289058</td>\n",
              "      <td>-0.158562</td>\n",
              "      <td>1.088840</td>\n",
              "      <td>-0.766056</td>\n",
              "      <td>-0.830809</td>\n",
              "      <td>-0.518678</td>\n",
              "      <td>-0.846421</td>\n",
              "      <td>-0.647482</td>\n",
              "      <td>0.370545</td>\n",
              "      <td>0.205762</td>\n",
              "      <td>-0.376097</td>\n",
              "      <td>0.736925</td>\n",
              "      <td>0.010941</td>\n",
              "      <td>-0.274318</td>\n",
              "      <td>-0.674925</td>\n",
              "      <td>-0.415391</td>\n",
              "      <td>0.847309</td>\n",
              "      <td>-0.850954</td>\n",
              "      <td>-0.876977</td>\n",
              "      <td>0.563391</td>\n",
              "      <td>0.391913</td>\n",
              "      <td>0.912937</td>\n",
              "      <td>-0.751362</td>\n",
              "      <td>-0.965184</td>\n",
              "      <td>-0.679597</td>\n",
              "      <td>-0.768671</td>\n",
              "      <td>0.758170</td>\n",
              "      <td>-0.454164</td>\n",
              "      <td>0.821657</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.674733</td>\n",
              "      <td>-0.135343</td>\n",
              "      <td>0.783492</td>\n",
              "      <td>-0.760632</td>\n",
              "      <td>-1.178650</td>\n",
              "      <td>-0.689048</td>\n",
              "      <td>0.819375</td>\n",
              "      <td>0.431352</td>\n",
              "      <td>-0.811626</td>\n",
              "      <td>-0.979967</td>\n",
              "      <td>-0.772746</td>\n",
              "      <td>0.535713</td>\n",
              "      <td>0.820978</td>\n",
              "      <td>-0.714431</td>\n",
              "      <td>-1.247898</td>\n",
              "      <td>-0.209809</td>\n",
              "      <td>-0.220841</td>\n",
              "      <td>-0.617422</td>\n",
              "      <td>-1.285105</td>\n",
              "      <td>0.739161</td>\n",
              "      <td>-0.529301</td>\n",
              "      <td>-0.796048</td>\n",
              "      <td>-0.083844</td>\n",
              "      <td>0.545154</td>\n",
              "      <td>-0.073787</td>\n",
              "      <td>0.587550</td>\n",
              "      <td>-0.924766</td>\n",
              "      <td>-0.060455</td>\n",
              "      <td>-0.465581</td>\n",
              "      <td>-0.290812</td>\n",
              "      <td>0.646953</td>\n",
              "      <td>0.122107</td>\n",
              "      <td>0.586022</td>\n",
              "      <td>-0.589051</td>\n",
              "      <td>-0.174033</td>\n",
              "      <td>-0.970290</td>\n",
              "      <td>-0.736400</td>\n",
              "      <td>0.381084</td>\n",
              "      <td>0.723749</td>\n",
              "      <td>-0.703829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.246474</td>\n",
              "      <td>-0.193639</td>\n",
              "      <td>0.232542</td>\n",
              "      <td>-0.122193</td>\n",
              "      <td>0.008201</td>\n",
              "      <td>-0.087867</td>\n",
              "      <td>0.165976</td>\n",
              "      <td>-0.086242</td>\n",
              "      <td>-0.180839</td>\n",
              "      <td>0.264083</td>\n",
              "      <td>-0.203172</td>\n",
              "      <td>0.065953</td>\n",
              "      <td>-0.139613</td>\n",
              "      <td>0.276780</td>\n",
              "      <td>0.047472</td>\n",
              "      <td>-0.028000</td>\n",
              "      <td>0.225432</td>\n",
              "      <td>-0.176592</td>\n",
              "      <td>0.200478</td>\n",
              "      <td>-0.166083</td>\n",
              "      <td>-0.243273</td>\n",
              "      <td>0.128417</td>\n",
              "      <td>-0.164053</td>\n",
              "      <td>-0.151665</td>\n",
              "      <td>0.193260</td>\n",
              "      <td>0.066070</td>\n",
              "      <td>0.108481</td>\n",
              "      <td>0.050754</td>\n",
              "      <td>-0.013653</td>\n",
              "      <td>0.189990</td>\n",
              "      <td>-0.141765</td>\n",
              "      <td>0.120640</td>\n",
              "      <td>-0.059648</td>\n",
              "      <td>0.083170</td>\n",
              "      <td>0.193854</td>\n",
              "      <td>-0.094022</td>\n",
              "      <td>-0.187544</td>\n",
              "      <td>-0.193294</td>\n",
              "      <td>-0.132810</td>\n",
              "      <td>-0.165086</td>\n",
              "      <td>...</td>\n",
              "      <td>0.029709</td>\n",
              "      <td>-0.193206</td>\n",
              "      <td>0.051139</td>\n",
              "      <td>-0.140400</td>\n",
              "      <td>0.172387</td>\n",
              "      <td>0.016075</td>\n",
              "      <td>-0.159961</td>\n",
              "      <td>0.076651</td>\n",
              "      <td>0.102553</td>\n",
              "      <td>0.260167</td>\n",
              "      <td>0.237367</td>\n",
              "      <td>-0.180988</td>\n",
              "      <td>-0.162108</td>\n",
              "      <td>0.183699</td>\n",
              "      <td>0.169127</td>\n",
              "      <td>0.069232</td>\n",
              "      <td>0.025220</td>\n",
              "      <td>-0.021993</td>\n",
              "      <td>0.179811</td>\n",
              "      <td>0.218455</td>\n",
              "      <td>0.171306</td>\n",
              "      <td>-0.262852</td>\n",
              "      <td>-0.067275</td>\n",
              "      <td>-0.285320</td>\n",
              "      <td>0.106092</td>\n",
              "      <td>-0.164239</td>\n",
              "      <td>-0.213621</td>\n",
              "      <td>0.042084</td>\n",
              "      <td>0.186407</td>\n",
              "      <td>-0.236555</td>\n",
              "      <td>0.004574</td>\n",
              "      <td>-0.147767</td>\n",
              "      <td>-0.257499</td>\n",
              "      <td>0.204626</td>\n",
              "      <td>-0.133505</td>\n",
              "      <td>0.245875</td>\n",
              "      <td>0.195944</td>\n",
              "      <td>-0.084696</td>\n",
              "      <td>-0.034688</td>\n",
              "      <td>0.104974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.350558</td>\n",
              "      <td>-0.206638</td>\n",
              "      <td>0.215164</td>\n",
              "      <td>-0.088487</td>\n",
              "      <td>-0.148930</td>\n",
              "      <td>-0.115120</td>\n",
              "      <td>0.281376</td>\n",
              "      <td>-0.002611</td>\n",
              "      <td>-0.205995</td>\n",
              "      <td>-0.375628</td>\n",
              "      <td>-0.104701</td>\n",
              "      <td>0.135402</td>\n",
              "      <td>0.092255</td>\n",
              "      <td>0.049911</td>\n",
              "      <td>0.153803</td>\n",
              "      <td>0.195808</td>\n",
              "      <td>0.224841</td>\n",
              "      <td>0.172075</td>\n",
              "      <td>0.102117</td>\n",
              "      <td>-0.173193</td>\n",
              "      <td>-0.213277</td>\n",
              "      <td>0.026280</td>\n",
              "      <td>-0.306434</td>\n",
              "      <td>-0.043726</td>\n",
              "      <td>0.096603</td>\n",
              "      <td>-0.158485</td>\n",
              "      <td>-0.169011</td>\n",
              "      <td>-0.159151</td>\n",
              "      <td>0.326042</td>\n",
              "      <td>0.022625</td>\n",
              "      <td>-0.208336</td>\n",
              "      <td>-0.074773</td>\n",
              "      <td>-0.254193</td>\n",
              "      <td>-0.229919</td>\n",
              "      <td>0.181000</td>\n",
              "      <td>-0.165214</td>\n",
              "      <td>0.237855</td>\n",
              "      <td>0.038554</td>\n",
              "      <td>0.071194</td>\n",
              "      <td>-0.250376</td>\n",
              "      <td>...</td>\n",
              "      <td>0.278788</td>\n",
              "      <td>-0.218672</td>\n",
              "      <td>0.085909</td>\n",
              "      <td>0.220482</td>\n",
              "      <td>0.186714</td>\n",
              "      <td>-0.007865</td>\n",
              "      <td>-0.205632</td>\n",
              "      <td>0.115612</td>\n",
              "      <td>0.015742</td>\n",
              "      <td>0.196920</td>\n",
              "      <td>0.169830</td>\n",
              "      <td>0.114591</td>\n",
              "      <td>-0.164163</td>\n",
              "      <td>-0.232623</td>\n",
              "      <td>-0.122320</td>\n",
              "      <td>0.188202</td>\n",
              "      <td>0.154491</td>\n",
              "      <td>0.086696</td>\n",
              "      <td>0.266326</td>\n",
              "      <td>0.134580</td>\n",
              "      <td>0.155011</td>\n",
              "      <td>0.179059</td>\n",
              "      <td>0.119007</td>\n",
              "      <td>-0.259358</td>\n",
              "      <td>0.300048</td>\n",
              "      <td>0.111986</td>\n",
              "      <td>0.150655</td>\n",
              "      <td>0.151758</td>\n",
              "      <td>0.229173</td>\n",
              "      <td>-0.077851</td>\n",
              "      <td>0.033436</td>\n",
              "      <td>-0.177980</td>\n",
              "      <td>0.148631</td>\n",
              "      <td>-0.010664</td>\n",
              "      <td>-0.161925</td>\n",
              "      <td>0.039811</td>\n",
              "      <td>0.090339</td>\n",
              "      <td>0.070135</td>\n",
              "      <td>-0.377030</td>\n",
              "      <td>0.061614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.131893</td>\n",
              "      <td>-0.227821</td>\n",
              "      <td>0.185253</td>\n",
              "      <td>-0.164132</td>\n",
              "      <td>-0.001951</td>\n",
              "      <td>-0.451601</td>\n",
              "      <td>0.256105</td>\n",
              "      <td>0.178472</td>\n",
              "      <td>-0.282647</td>\n",
              "      <td>0.036304</td>\n",
              "      <td>0.170287</td>\n",
              "      <td>-0.177574</td>\n",
              "      <td>0.154559</td>\n",
              "      <td>-0.201491</td>\n",
              "      <td>0.260715</td>\n",
              "      <td>0.059163</td>\n",
              "      <td>0.042714</td>\n",
              "      <td>-0.282087</td>\n",
              "      <td>0.115838</td>\n",
              "      <td>0.291523</td>\n",
              "      <td>0.183230</td>\n",
              "      <td>0.220605</td>\n",
              "      <td>0.321796</td>\n",
              "      <td>-0.080052</td>\n",
              "      <td>-0.171350</td>\n",
              "      <td>-0.331535</td>\n",
              "      <td>-0.176834</td>\n",
              "      <td>-0.092266</td>\n",
              "      <td>0.171652</td>\n",
              "      <td>0.034377</td>\n",
              "      <td>-0.059008</td>\n",
              "      <td>0.080070</td>\n",
              "      <td>-0.354884</td>\n",
              "      <td>0.188052</td>\n",
              "      <td>0.170061</td>\n",
              "      <td>0.213233</td>\n",
              "      <td>-0.011874</td>\n",
              "      <td>0.011553</td>\n",
              "      <td>0.044996</td>\n",
              "      <td>0.024332</td>\n",
              "      <td>...</td>\n",
              "      <td>0.064340</td>\n",
              "      <td>0.152798</td>\n",
              "      <td>0.096838</td>\n",
              "      <td>0.196622</td>\n",
              "      <td>-0.021392</td>\n",
              "      <td>0.122790</td>\n",
              "      <td>-0.342147</td>\n",
              "      <td>-0.185065</td>\n",
              "      <td>-0.112904</td>\n",
              "      <td>0.034726</td>\n",
              "      <td>0.177821</td>\n",
              "      <td>0.018191</td>\n",
              "      <td>-0.208537</td>\n",
              "      <td>0.123836</td>\n",
              "      <td>0.050656</td>\n",
              "      <td>-0.151419</td>\n",
              "      <td>-0.200857</td>\n",
              "      <td>0.245575</td>\n",
              "      <td>-0.254463</td>\n",
              "      <td>-0.077397</td>\n",
              "      <td>0.089280</td>\n",
              "      <td>0.120940</td>\n",
              "      <td>0.361106</td>\n",
              "      <td>0.011316</td>\n",
              "      <td>-0.246328</td>\n",
              "      <td>-0.125048</td>\n",
              "      <td>-0.041690</td>\n",
              "      <td>-0.178213</td>\n",
              "      <td>0.093052</td>\n",
              "      <td>0.126407</td>\n",
              "      <td>-0.363242</td>\n",
              "      <td>0.273367</td>\n",
              "      <td>0.303379</td>\n",
              "      <td>0.024663</td>\n",
              "      <td>-0.047317</td>\n",
              "      <td>-0.272584</td>\n",
              "      <td>0.117965</td>\n",
              "      <td>0.351622</td>\n",
              "      <td>0.087678</td>\n",
              "      <td>-0.134855</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 512 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2    ...       509       510       511\n",
              "0 -0.444623  0.810958 -0.750832  ...  0.453878  0.716630 -0.845961\n",
              "1 -0.393092  0.603314 -0.819966  ...  0.381084  0.723749 -0.703829\n",
              "2  0.246474 -0.193639  0.232542  ... -0.084696 -0.034688  0.104974\n",
              "3 -0.350558 -0.206638  0.215164  ...  0.070135 -0.377030  0.061614\n",
              "4 -0.131893 -0.227821  0.185253  ...  0.351622  0.087678 -0.134855\n",
              "\n",
              "[5 rows x 512 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq_YjlsuJK8z"
      },
      "source": [
        "### Dot Product a measure of similarity\n",
        "Just some testing code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaQafFGZyhYd",
        "outputId": "fe1da4b1-4275-4f67-b4df-1bc345ff3f96"
      },
      "source": [
        "tf.tensordot(weights1[133], weights1[105], 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=4.401949>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgLZJJ8O8-eD",
        "outputId": "ce9d7215-cf82-44e4-d7ca-d67a0290d9db"
      },
      "source": [
        "for i in range(1394):\n",
        "  print(i, tf.tensordot(weights1[7], weights1[i], 1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 tf.Tensor(0.035468355, shape=(), dtype=float32)\n",
            "1 tf.Tensor(0.13555601, shape=(), dtype=float32)\n",
            "2 tf.Tensor(3.7983415, shape=(), dtype=float32)\n",
            "3 tf.Tensor(-1.9745964, shape=(), dtype=float32)\n",
            "4 tf.Tensor(0.7268956, shape=(), dtype=float32)\n",
            "5 tf.Tensor(-4.384697, shape=(), dtype=float32)\n",
            "6 tf.Tensor(-3.6050618, shape=(), dtype=float32)\n",
            "7 tf.Tensor(29.412653, shape=(), dtype=float32)\n",
            "8 tf.Tensor(3.9979153, shape=(), dtype=float32)\n",
            "9 tf.Tensor(0.06951006, shape=(), dtype=float32)\n",
            "10 tf.Tensor(3.4140382, shape=(), dtype=float32)\n",
            "11 tf.Tensor(0.4783892, shape=(), dtype=float32)\n",
            "12 tf.Tensor(2.8890464, shape=(), dtype=float32)\n",
            "13 tf.Tensor(-2.6314285, shape=(), dtype=float32)\n",
            "14 tf.Tensor(-0.14892164, shape=(), dtype=float32)\n",
            "15 tf.Tensor(0.9513725, shape=(), dtype=float32)\n",
            "16 tf.Tensor(-0.6900875, shape=(), dtype=float32)\n",
            "17 tf.Tensor(0.9727448, shape=(), dtype=float32)\n",
            "18 tf.Tensor(6.526903, shape=(), dtype=float32)\n",
            "19 tf.Tensor(0.69749016, shape=(), dtype=float32)\n",
            "20 tf.Tensor(2.4256718, shape=(), dtype=float32)\n",
            "21 tf.Tensor(-1.9183007, shape=(), dtype=float32)\n",
            "22 tf.Tensor(3.0432177, shape=(), dtype=float32)\n",
            "23 tf.Tensor(0.22747186, shape=(), dtype=float32)\n",
            "24 tf.Tensor(1.3860552, shape=(), dtype=float32)\n",
            "25 tf.Tensor(2.243336, shape=(), dtype=float32)\n",
            "26 tf.Tensor(-0.08651993, shape=(), dtype=float32)\n",
            "27 tf.Tensor(2.8378253, shape=(), dtype=float32)\n",
            "28 tf.Tensor(6.212578, shape=(), dtype=float32)\n",
            "29 tf.Tensor(2.1017118, shape=(), dtype=float32)\n",
            "30 tf.Tensor(-2.1809142, shape=(), dtype=float32)\n",
            "31 tf.Tensor(3.7741554, shape=(), dtype=float32)\n",
            "32 tf.Tensor(4.2579665, shape=(), dtype=float32)\n",
            "33 tf.Tensor(-2.047464, shape=(), dtype=float32)\n",
            "34 tf.Tensor(3.759088, shape=(), dtype=float32)\n",
            "35 tf.Tensor(-3.6046586, shape=(), dtype=float32)\n",
            "36 tf.Tensor(-0.26049402, shape=(), dtype=float32)\n",
            "37 tf.Tensor(-2.518013, shape=(), dtype=float32)\n",
            "38 tf.Tensor(-4.077236, shape=(), dtype=float32)\n",
            "39 tf.Tensor(-0.7881005, shape=(), dtype=float32)\n",
            "40 tf.Tensor(-0.52727497, shape=(), dtype=float32)\n",
            "41 tf.Tensor(3.2726202, shape=(), dtype=float32)\n",
            "42 tf.Tensor(5.9906206, shape=(), dtype=float32)\n",
            "43 tf.Tensor(-2.435366, shape=(), dtype=float32)\n",
            "44 tf.Tensor(1.6193935, shape=(), dtype=float32)\n",
            "45 tf.Tensor(3.272019, shape=(), dtype=float32)\n",
            "46 tf.Tensor(2.0845156, shape=(), dtype=float32)\n",
            "47 tf.Tensor(3.6980798, shape=(), dtype=float32)\n",
            "48 tf.Tensor(4.5605626, shape=(), dtype=float32)\n",
            "49 tf.Tensor(4.002045, shape=(), dtype=float32)\n",
            "50 tf.Tensor(-2.01462, shape=(), dtype=float32)\n",
            "51 tf.Tensor(1.9213663, shape=(), dtype=float32)\n",
            "52 tf.Tensor(-2.3678622, shape=(), dtype=float32)\n",
            "53 tf.Tensor(9.490653, shape=(), dtype=float32)\n",
            "54 tf.Tensor(-4.9626694, shape=(), dtype=float32)\n",
            "55 tf.Tensor(2.6270795, shape=(), dtype=float32)\n",
            "56 tf.Tensor(7.015424, shape=(), dtype=float32)\n",
            "57 tf.Tensor(6.079799, shape=(), dtype=float32)\n",
            "58 tf.Tensor(1.4563757, shape=(), dtype=float32)\n",
            "59 tf.Tensor(4.515666, shape=(), dtype=float32)\n",
            "60 tf.Tensor(1.3227357, shape=(), dtype=float32)\n",
            "61 tf.Tensor(8.702321, shape=(), dtype=float32)\n",
            "62 tf.Tensor(7.2987585, shape=(), dtype=float32)\n",
            "63 tf.Tensor(-0.5134584, shape=(), dtype=float32)\n",
            "64 tf.Tensor(0.54224944, shape=(), dtype=float32)\n",
            "65 tf.Tensor(-2.5912087, shape=(), dtype=float32)\n",
            "66 tf.Tensor(1.2556773, shape=(), dtype=float32)\n",
            "67 tf.Tensor(1.3423635, shape=(), dtype=float32)\n",
            "68 tf.Tensor(5.249543, shape=(), dtype=float32)\n",
            "69 tf.Tensor(-0.30275348, shape=(), dtype=float32)\n",
            "70 tf.Tensor(-2.7062135, shape=(), dtype=float32)\n",
            "71 tf.Tensor(-0.50236094, shape=(), dtype=float32)\n",
            "72 tf.Tensor(-1.460197, shape=(), dtype=float32)\n",
            "73 tf.Tensor(-0.865896, shape=(), dtype=float32)\n",
            "74 tf.Tensor(0.2833912, shape=(), dtype=float32)\n",
            "75 tf.Tensor(6.4998713, shape=(), dtype=float32)\n",
            "76 tf.Tensor(13.551195, shape=(), dtype=float32)\n",
            "77 tf.Tensor(8.8845215, shape=(), dtype=float32)\n",
            "78 tf.Tensor(2.630217, shape=(), dtype=float32)\n",
            "79 tf.Tensor(-0.43510175, shape=(), dtype=float32)\n",
            "80 tf.Tensor(1.0536641, shape=(), dtype=float32)\n",
            "81 tf.Tensor(-1.0296161, shape=(), dtype=float32)\n",
            "82 tf.Tensor(1.2662203, shape=(), dtype=float32)\n",
            "83 tf.Tensor(1.4027694, shape=(), dtype=float32)\n",
            "84 tf.Tensor(1.8550792, shape=(), dtype=float32)\n",
            "85 tf.Tensor(4.3857183, shape=(), dtype=float32)\n",
            "86 tf.Tensor(1.3092219, shape=(), dtype=float32)\n",
            "87 tf.Tensor(0.59609556, shape=(), dtype=float32)\n",
            "88 tf.Tensor(3.7981565, shape=(), dtype=float32)\n",
            "89 tf.Tensor(5.728893, shape=(), dtype=float32)\n",
            "90 tf.Tensor(9.031247, shape=(), dtype=float32)\n",
            "91 tf.Tensor(-1.2256368, shape=(), dtype=float32)\n",
            "92 tf.Tensor(-0.048419714, shape=(), dtype=float32)\n",
            "93 tf.Tensor(6.080755, shape=(), dtype=float32)\n",
            "94 tf.Tensor(-0.89320904, shape=(), dtype=float32)\n",
            "95 tf.Tensor(-17.388134, shape=(), dtype=float32)\n",
            "96 tf.Tensor(7.8018184, shape=(), dtype=float32)\n",
            "97 tf.Tensor(-0.06391753, shape=(), dtype=float32)\n",
            "98 tf.Tensor(5.8206606, shape=(), dtype=float32)\n",
            "99 tf.Tensor(3.6977532, shape=(), dtype=float32)\n",
            "100 tf.Tensor(6.9537597, shape=(), dtype=float32)\n",
            "101 tf.Tensor(-1.3830798, shape=(), dtype=float32)\n",
            "102 tf.Tensor(-2.2698886, shape=(), dtype=float32)\n",
            "103 tf.Tensor(-0.19372866, shape=(), dtype=float32)\n",
            "104 tf.Tensor(11.115625, shape=(), dtype=float32)\n",
            "105 tf.Tensor(0.46399894, shape=(), dtype=float32)\n",
            "106 tf.Tensor(4.163419, shape=(), dtype=float32)\n",
            "107 tf.Tensor(-3.9437716, shape=(), dtype=float32)\n",
            "108 tf.Tensor(4.4872603, shape=(), dtype=float32)\n",
            "109 tf.Tensor(13.91952, shape=(), dtype=float32)\n",
            "110 tf.Tensor(4.2460814, shape=(), dtype=float32)\n",
            "111 tf.Tensor(3.36771, shape=(), dtype=float32)\n",
            "112 tf.Tensor(37.053852, shape=(), dtype=float32)\n",
            "113 tf.Tensor(-0.15766633, shape=(), dtype=float32)\n",
            "114 tf.Tensor(5.085872, shape=(), dtype=float32)\n",
            "115 tf.Tensor(-2.491954, shape=(), dtype=float32)\n",
            "116 tf.Tensor(-2.7085671, shape=(), dtype=float32)\n",
            "117 tf.Tensor(5.7986274, shape=(), dtype=float32)\n",
            "118 tf.Tensor(-3.9169688, shape=(), dtype=float32)\n",
            "119 tf.Tensor(9.364779, shape=(), dtype=float32)\n",
            "120 tf.Tensor(-3.9219122, shape=(), dtype=float32)\n",
            "121 tf.Tensor(0.22807866, shape=(), dtype=float32)\n",
            "122 tf.Tensor(-6.465365, shape=(), dtype=float32)\n",
            "123 tf.Tensor(1.9363345, shape=(), dtype=float32)\n",
            "124 tf.Tensor(9.317306, shape=(), dtype=float32)\n",
            "125 tf.Tensor(7.937173, shape=(), dtype=float32)\n",
            "126 tf.Tensor(-3.76316, shape=(), dtype=float32)\n",
            "127 tf.Tensor(0.29470646, shape=(), dtype=float32)\n",
            "128 tf.Tensor(2.7439983, shape=(), dtype=float32)\n",
            "129 tf.Tensor(13.17499, shape=(), dtype=float32)\n",
            "130 tf.Tensor(12.991261, shape=(), dtype=float32)\n",
            "131 tf.Tensor(-5.569546, shape=(), dtype=float32)\n",
            "132 tf.Tensor(1.8234007, shape=(), dtype=float32)\n",
            "133 tf.Tensor(1.9319792, shape=(), dtype=float32)\n",
            "134 tf.Tensor(13.391555, shape=(), dtype=float32)\n",
            "135 tf.Tensor(-0.47945043, shape=(), dtype=float32)\n",
            "136 tf.Tensor(-5.59393, shape=(), dtype=float32)\n",
            "137 tf.Tensor(2.95433, shape=(), dtype=float32)\n",
            "138 tf.Tensor(24.388554, shape=(), dtype=float32)\n",
            "139 tf.Tensor(22.496918, shape=(), dtype=float32)\n",
            "140 tf.Tensor(15.652537, shape=(), dtype=float32)\n",
            "141 tf.Tensor(17.022903, shape=(), dtype=float32)\n",
            "142 tf.Tensor(-11.894512, shape=(), dtype=float32)\n",
            "143 tf.Tensor(11.479067, shape=(), dtype=float32)\n",
            "144 tf.Tensor(0.011871591, shape=(), dtype=float32)\n",
            "145 tf.Tensor(-3.2640843, shape=(), dtype=float32)\n",
            "146 tf.Tensor(-1.5199788, shape=(), dtype=float32)\n",
            "147 tf.Tensor(-0.013395339, shape=(), dtype=float32)\n",
            "148 tf.Tensor(3.989549, shape=(), dtype=float32)\n",
            "149 tf.Tensor(4.6122527, shape=(), dtype=float32)\n",
            "150 tf.Tensor(8.236309, shape=(), dtype=float32)\n",
            "151 tf.Tensor(6.9809885, shape=(), dtype=float32)\n",
            "152 tf.Tensor(0.38969618, shape=(), dtype=float32)\n",
            "153 tf.Tensor(-2.0724535, shape=(), dtype=float32)\n",
            "154 tf.Tensor(-0.7974311, shape=(), dtype=float32)\n",
            "155 tf.Tensor(15.435722, shape=(), dtype=float32)\n",
            "156 tf.Tensor(0.76063496, shape=(), dtype=float32)\n",
            "157 tf.Tensor(3.8762183, shape=(), dtype=float32)\n",
            "158 tf.Tensor(13.234691, shape=(), dtype=float32)\n",
            "159 tf.Tensor(11.09952, shape=(), dtype=float32)\n",
            "160 tf.Tensor(3.717808, shape=(), dtype=float32)\n",
            "161 tf.Tensor(-8.557925, shape=(), dtype=float32)\n",
            "162 tf.Tensor(6.192724, shape=(), dtype=float32)\n",
            "163 tf.Tensor(1.4238855, shape=(), dtype=float32)\n",
            "164 tf.Tensor(-4.7936954, shape=(), dtype=float32)\n",
            "165 tf.Tensor(1.2682374, shape=(), dtype=float32)\n",
            "166 tf.Tensor(-3.3045242, shape=(), dtype=float32)\n",
            "167 tf.Tensor(3.112433, shape=(), dtype=float32)\n",
            "168 tf.Tensor(0.16060394, shape=(), dtype=float32)\n",
            "169 tf.Tensor(6.8459167, shape=(), dtype=float32)\n",
            "170 tf.Tensor(11.002834, shape=(), dtype=float32)\n",
            "171 tf.Tensor(-1.8546934, shape=(), dtype=float32)\n",
            "172 tf.Tensor(-0.3920486, shape=(), dtype=float32)\n",
            "173 tf.Tensor(-7.332038, shape=(), dtype=float32)\n",
            "174 tf.Tensor(3.3274431, shape=(), dtype=float32)\n",
            "175 tf.Tensor(0.5913412, shape=(), dtype=float32)\n",
            "176 tf.Tensor(12.952005, shape=(), dtype=float32)\n",
            "177 tf.Tensor(-10.000164, shape=(), dtype=float32)\n",
            "178 tf.Tensor(2.5034966, shape=(), dtype=float32)\n",
            "179 tf.Tensor(1.7672967, shape=(), dtype=float32)\n",
            "180 tf.Tensor(5.3842125, shape=(), dtype=float32)\n",
            "181 tf.Tensor(-1.7067688, shape=(), dtype=float32)\n",
            "182 tf.Tensor(8.656977, shape=(), dtype=float32)\n",
            "183 tf.Tensor(-5.968184, shape=(), dtype=float32)\n",
            "184 tf.Tensor(-1.4404933, shape=(), dtype=float32)\n",
            "185 tf.Tensor(5.350637, shape=(), dtype=float32)\n",
            "186 tf.Tensor(5.4348645, shape=(), dtype=float32)\n",
            "187 tf.Tensor(2.7276728, shape=(), dtype=float32)\n",
            "188 tf.Tensor(3.7639966, shape=(), dtype=float32)\n",
            "189 tf.Tensor(0.9173427, shape=(), dtype=float32)\n",
            "190 tf.Tensor(3.8605657, shape=(), dtype=float32)\n",
            "191 tf.Tensor(-3.5987148, shape=(), dtype=float32)\n",
            "192 tf.Tensor(9.889825, shape=(), dtype=float32)\n",
            "193 tf.Tensor(5.727522, shape=(), dtype=float32)\n",
            "194 tf.Tensor(9.991487, shape=(), dtype=float32)\n",
            "195 tf.Tensor(-3.2828329, shape=(), dtype=float32)\n",
            "196 tf.Tensor(15.395211, shape=(), dtype=float32)\n",
            "197 tf.Tensor(-0.19134948, shape=(), dtype=float32)\n",
            "198 tf.Tensor(1.8210174, shape=(), dtype=float32)\n",
            "199 tf.Tensor(11.318904, shape=(), dtype=float32)\n",
            "200 tf.Tensor(-8.718874, shape=(), dtype=float32)\n",
            "201 tf.Tensor(3.0036438, shape=(), dtype=float32)\n",
            "202 tf.Tensor(-0.7711795, shape=(), dtype=float32)\n",
            "203 tf.Tensor(-4.7058296, shape=(), dtype=float32)\n",
            "204 tf.Tensor(6.1984534, shape=(), dtype=float32)\n",
            "205 tf.Tensor(8.462721, shape=(), dtype=float32)\n",
            "206 tf.Tensor(-5.9449086, shape=(), dtype=float32)\n",
            "207 tf.Tensor(9.996793, shape=(), dtype=float32)\n",
            "208 tf.Tensor(5.387499, shape=(), dtype=float32)\n",
            "209 tf.Tensor(3.2143428, shape=(), dtype=float32)\n",
            "210 tf.Tensor(4.71813, shape=(), dtype=float32)\n",
            "211 tf.Tensor(13.835414, shape=(), dtype=float32)\n",
            "212 tf.Tensor(3.0001297, shape=(), dtype=float32)\n",
            "213 tf.Tensor(9.813018, shape=(), dtype=float32)\n",
            "214 tf.Tensor(2.6221461, shape=(), dtype=float32)\n",
            "215 tf.Tensor(14.091752, shape=(), dtype=float32)\n",
            "216 tf.Tensor(-14.754682, shape=(), dtype=float32)\n",
            "217 tf.Tensor(-15.540628, shape=(), dtype=float32)\n",
            "218 tf.Tensor(11.521645, shape=(), dtype=float32)\n",
            "219 tf.Tensor(5.5398803, shape=(), dtype=float32)\n",
            "220 tf.Tensor(-7.4284716, shape=(), dtype=float32)\n",
            "221 tf.Tensor(9.220893, shape=(), dtype=float32)\n",
            "222 tf.Tensor(4.234584, shape=(), dtype=float32)\n",
            "223 tf.Tensor(-1.8218415, shape=(), dtype=float32)\n",
            "224 tf.Tensor(-0.18925045, shape=(), dtype=float32)\n",
            "225 tf.Tensor(-4.0877748, shape=(), dtype=float32)\n",
            "226 tf.Tensor(10.432405, shape=(), dtype=float32)\n",
            "227 tf.Tensor(4.796674, shape=(), dtype=float32)\n",
            "228 tf.Tensor(14.518373, shape=(), dtype=float32)\n",
            "229 tf.Tensor(-2.5595496, shape=(), dtype=float32)\n",
            "230 tf.Tensor(11.134718, shape=(), dtype=float32)\n",
            "231 tf.Tensor(11.755795, shape=(), dtype=float32)\n",
            "232 tf.Tensor(8.842769, shape=(), dtype=float32)\n",
            "233 tf.Tensor(-5.513868, shape=(), dtype=float32)\n",
            "234 tf.Tensor(1.9405211, shape=(), dtype=float32)\n",
            "235 tf.Tensor(-7.9184647, shape=(), dtype=float32)\n",
            "236 tf.Tensor(2.5460043, shape=(), dtype=float32)\n",
            "237 tf.Tensor(8.217631, shape=(), dtype=float32)\n",
            "238 tf.Tensor(-4.770557, shape=(), dtype=float32)\n",
            "239 tf.Tensor(12.082801, shape=(), dtype=float32)\n",
            "240 tf.Tensor(0.6033708, shape=(), dtype=float32)\n",
            "241 tf.Tensor(17.578884, shape=(), dtype=float32)\n",
            "242 tf.Tensor(6.7818813, shape=(), dtype=float32)\n",
            "243 tf.Tensor(5.441447, shape=(), dtype=float32)\n",
            "244 tf.Tensor(0.45278096, shape=(), dtype=float32)\n",
            "245 tf.Tensor(4.517511, shape=(), dtype=float32)\n",
            "246 tf.Tensor(0.8507124, shape=(), dtype=float32)\n",
            "247 tf.Tensor(4.476932, shape=(), dtype=float32)\n",
            "248 tf.Tensor(19.22839, shape=(), dtype=float32)\n",
            "249 tf.Tensor(3.1834886, shape=(), dtype=float32)\n",
            "250 tf.Tensor(-2.640295, shape=(), dtype=float32)\n",
            "251 tf.Tensor(9.838265, shape=(), dtype=float32)\n",
            "252 tf.Tensor(2.4644635, shape=(), dtype=float32)\n",
            "253 tf.Tensor(3.4568758, shape=(), dtype=float32)\n",
            "254 tf.Tensor(-0.57939434, shape=(), dtype=float32)\n",
            "255 tf.Tensor(-5.7934823, shape=(), dtype=float32)\n",
            "256 tf.Tensor(9.796917, shape=(), dtype=float32)\n",
            "257 tf.Tensor(-7.9273834, shape=(), dtype=float32)\n",
            "258 tf.Tensor(-4.3090734, shape=(), dtype=float32)\n",
            "259 tf.Tensor(-4.4919505, shape=(), dtype=float32)\n",
            "260 tf.Tensor(4.756701, shape=(), dtype=float32)\n",
            "261 tf.Tensor(0.51640785, shape=(), dtype=float32)\n",
            "262 tf.Tensor(5.880917, shape=(), dtype=float32)\n",
            "263 tf.Tensor(6.0967584, shape=(), dtype=float32)\n",
            "264 tf.Tensor(12.765653, shape=(), dtype=float32)\n",
            "265 tf.Tensor(11.198539, shape=(), dtype=float32)\n",
            "266 tf.Tensor(-3.572961, shape=(), dtype=float32)\n",
            "267 tf.Tensor(-0.084884465, shape=(), dtype=float32)\n",
            "268 tf.Tensor(-2.2497635, shape=(), dtype=float32)\n",
            "269 tf.Tensor(1.0249962, shape=(), dtype=float32)\n",
            "270 tf.Tensor(6.5591016, shape=(), dtype=float32)\n",
            "271 tf.Tensor(7.301835, shape=(), dtype=float32)\n",
            "272 tf.Tensor(-6.292706, shape=(), dtype=float32)\n",
            "273 tf.Tensor(5.9896607, shape=(), dtype=float32)\n",
            "274 tf.Tensor(10.73965, shape=(), dtype=float32)\n",
            "275 tf.Tensor(4.7627225, shape=(), dtype=float32)\n",
            "276 tf.Tensor(-0.45921803, shape=(), dtype=float32)\n",
            "277 tf.Tensor(1.7266245, shape=(), dtype=float32)\n",
            "278 tf.Tensor(1.7084639, shape=(), dtype=float32)\n",
            "279 tf.Tensor(-2.4807942, shape=(), dtype=float32)\n",
            "280 tf.Tensor(6.552485, shape=(), dtype=float32)\n",
            "281 tf.Tensor(0.84999526, shape=(), dtype=float32)\n",
            "282 tf.Tensor(-0.29035968, shape=(), dtype=float32)\n",
            "283 tf.Tensor(-5.313639, shape=(), dtype=float32)\n",
            "284 tf.Tensor(-2.541531, shape=(), dtype=float32)\n",
            "285 tf.Tensor(-4.311481, shape=(), dtype=float32)\n",
            "286 tf.Tensor(4.5721087, shape=(), dtype=float32)\n",
            "287 tf.Tensor(15.139383, shape=(), dtype=float32)\n",
            "288 tf.Tensor(1.7259986, shape=(), dtype=float32)\n",
            "289 tf.Tensor(-3.1650794, shape=(), dtype=float32)\n",
            "290 tf.Tensor(-2.7413645, shape=(), dtype=float32)\n",
            "291 tf.Tensor(-2.257801, shape=(), dtype=float32)\n",
            "292 tf.Tensor(-3.59759, shape=(), dtype=float32)\n",
            "293 tf.Tensor(-0.2700799, shape=(), dtype=float32)\n",
            "294 tf.Tensor(4.904462, shape=(), dtype=float32)\n",
            "295 tf.Tensor(-4.9315996, shape=(), dtype=float32)\n",
            "296 tf.Tensor(3.6163623, shape=(), dtype=float32)\n",
            "297 tf.Tensor(-3.0507655, shape=(), dtype=float32)\n",
            "298 tf.Tensor(-0.6377274, shape=(), dtype=float32)\n",
            "299 tf.Tensor(1.5430379, shape=(), dtype=float32)\n",
            "300 tf.Tensor(15.683903, shape=(), dtype=float32)\n",
            "301 tf.Tensor(3.2281475, shape=(), dtype=float32)\n",
            "302 tf.Tensor(5.2193236, shape=(), dtype=float32)\n",
            "303 tf.Tensor(-2.8075442, shape=(), dtype=float32)\n",
            "304 tf.Tensor(8.652757, shape=(), dtype=float32)\n",
            "305 tf.Tensor(-0.8265259, shape=(), dtype=float32)\n",
            "306 tf.Tensor(9.121892, shape=(), dtype=float32)\n",
            "307 tf.Tensor(-1.0145217, shape=(), dtype=float32)\n",
            "308 tf.Tensor(3.9803069, shape=(), dtype=float32)\n",
            "309 tf.Tensor(2.7720108, shape=(), dtype=float32)\n",
            "310 tf.Tensor(14.937573, shape=(), dtype=float32)\n",
            "311 tf.Tensor(-5.2096496, shape=(), dtype=float32)\n",
            "312 tf.Tensor(1.0059844, shape=(), dtype=float32)\n",
            "313 tf.Tensor(-3.1448898, shape=(), dtype=float32)\n",
            "314 tf.Tensor(4.098102, shape=(), dtype=float32)\n",
            "315 tf.Tensor(3.8168485, shape=(), dtype=float32)\n",
            "316 tf.Tensor(-3.648764, shape=(), dtype=float32)\n",
            "317 tf.Tensor(-1.3357403, shape=(), dtype=float32)\n",
            "318 tf.Tensor(-4.906901, shape=(), dtype=float32)\n",
            "319 tf.Tensor(-0.16175827, shape=(), dtype=float32)\n",
            "320 tf.Tensor(8.337231, shape=(), dtype=float32)\n",
            "321 tf.Tensor(9.576816, shape=(), dtype=float32)\n",
            "322 tf.Tensor(3.9797246, shape=(), dtype=float32)\n",
            "323 tf.Tensor(3.7775455, shape=(), dtype=float32)\n",
            "324 tf.Tensor(0.9822112, shape=(), dtype=float32)\n",
            "325 tf.Tensor(1.6486409, shape=(), dtype=float32)\n",
            "326 tf.Tensor(0.78477603, shape=(), dtype=float32)\n",
            "327 tf.Tensor(1.5890975, shape=(), dtype=float32)\n",
            "328 tf.Tensor(-1.2292544, shape=(), dtype=float32)\n",
            "329 tf.Tensor(7.495718, shape=(), dtype=float32)\n",
            "330 tf.Tensor(-2.0017126, shape=(), dtype=float32)\n",
            "331 tf.Tensor(0.06994203, shape=(), dtype=float32)\n",
            "332 tf.Tensor(3.9425564, shape=(), dtype=float32)\n",
            "333 tf.Tensor(-0.23754855, shape=(), dtype=float32)\n",
            "334 tf.Tensor(2.5331135, shape=(), dtype=float32)\n",
            "335 tf.Tensor(-1.7628386, shape=(), dtype=float32)\n",
            "336 tf.Tensor(0.69054127, shape=(), dtype=float32)\n",
            "337 tf.Tensor(-3.0774472, shape=(), dtype=float32)\n",
            "338 tf.Tensor(-0.53938097, shape=(), dtype=float32)\n",
            "339 tf.Tensor(0.619453, shape=(), dtype=float32)\n",
            "340 tf.Tensor(-3.4053352, shape=(), dtype=float32)\n",
            "341 tf.Tensor(0.11979855, shape=(), dtype=float32)\n",
            "342 tf.Tensor(-0.73244905, shape=(), dtype=float32)\n",
            "343 tf.Tensor(-1.8431191, shape=(), dtype=float32)\n",
            "344 tf.Tensor(-5.7624145, shape=(), dtype=float32)\n",
            "345 tf.Tensor(-1.0026598, shape=(), dtype=float32)\n",
            "346 tf.Tensor(-2.041955, shape=(), dtype=float32)\n",
            "347 tf.Tensor(3.6176765, shape=(), dtype=float32)\n",
            "348 tf.Tensor(-1.9346437, shape=(), dtype=float32)\n",
            "349 tf.Tensor(-2.472337, shape=(), dtype=float32)\n",
            "350 tf.Tensor(2.6427891, shape=(), dtype=float32)\n",
            "351 tf.Tensor(-0.16539618, shape=(), dtype=float32)\n",
            "352 tf.Tensor(8.415486, shape=(), dtype=float32)\n",
            "353 tf.Tensor(-4.0145383, shape=(), dtype=float32)\n",
            "354 tf.Tensor(7.863198, shape=(), dtype=float32)\n",
            "355 tf.Tensor(-8.284804, shape=(), dtype=float32)\n",
            "356 tf.Tensor(3.1820712, shape=(), dtype=float32)\n",
            "357 tf.Tensor(15.987645, shape=(), dtype=float32)\n",
            "358 tf.Tensor(1.7116199, shape=(), dtype=float32)\n",
            "359 tf.Tensor(-0.45639488, shape=(), dtype=float32)\n",
            "360 tf.Tensor(6.5059905, shape=(), dtype=float32)\n",
            "361 tf.Tensor(-0.28718212, shape=(), dtype=float32)\n",
            "362 tf.Tensor(5.1303253, shape=(), dtype=float32)\n",
            "363 tf.Tensor(8.278719, shape=(), dtype=float32)\n",
            "364 tf.Tensor(10.679101, shape=(), dtype=float32)\n",
            "365 tf.Tensor(7.021645, shape=(), dtype=float32)\n",
            "366 tf.Tensor(-2.879289, shape=(), dtype=float32)\n",
            "367 tf.Tensor(-1.9163272, shape=(), dtype=float32)\n",
            "368 tf.Tensor(-0.1518225, shape=(), dtype=float32)\n",
            "369 tf.Tensor(-5.640037, shape=(), dtype=float32)\n",
            "370 tf.Tensor(-5.718549, shape=(), dtype=float32)\n",
            "371 tf.Tensor(-1.381091, shape=(), dtype=float32)\n",
            "372 tf.Tensor(1.8239257, shape=(), dtype=float32)\n",
            "373 tf.Tensor(3.3521187, shape=(), dtype=float32)\n",
            "374 tf.Tensor(2.254996, shape=(), dtype=float32)\n",
            "375 tf.Tensor(0.6423774, shape=(), dtype=float32)\n",
            "376 tf.Tensor(-0.41358763, shape=(), dtype=float32)\n",
            "377 tf.Tensor(-3.939257, shape=(), dtype=float32)\n",
            "378 tf.Tensor(-0.07728262, shape=(), dtype=float32)\n",
            "379 tf.Tensor(4.708008, shape=(), dtype=float32)\n",
            "380 tf.Tensor(3.2624857, shape=(), dtype=float32)\n",
            "381 tf.Tensor(0.79034644, shape=(), dtype=float32)\n",
            "382 tf.Tensor(-0.8011982, shape=(), dtype=float32)\n",
            "383 tf.Tensor(-1.8248599, shape=(), dtype=float32)\n",
            "384 tf.Tensor(1.636084, shape=(), dtype=float32)\n",
            "385 tf.Tensor(11.222107, shape=(), dtype=float32)\n",
            "386 tf.Tensor(1.549299, shape=(), dtype=float32)\n",
            "387 tf.Tensor(2.389722, shape=(), dtype=float32)\n",
            "388 tf.Tensor(1.7803187, shape=(), dtype=float32)\n",
            "389 tf.Tensor(-3.7731915, shape=(), dtype=float32)\n",
            "390 tf.Tensor(1.8968658, shape=(), dtype=float32)\n",
            "391 tf.Tensor(-0.07158519, shape=(), dtype=float32)\n",
            "392 tf.Tensor(9.663893, shape=(), dtype=float32)\n",
            "393 tf.Tensor(-18.696331, shape=(), dtype=float32)\n",
            "394 tf.Tensor(-1.8629382, shape=(), dtype=float32)\n",
            "395 tf.Tensor(15.352095, shape=(), dtype=float32)\n",
            "396 tf.Tensor(-1.0100912, shape=(), dtype=float32)\n",
            "397 tf.Tensor(-0.06066823, shape=(), dtype=float32)\n",
            "398 tf.Tensor(3.5869503, shape=(), dtype=float32)\n",
            "399 tf.Tensor(9.99865, shape=(), dtype=float32)\n",
            "400 tf.Tensor(0.9835923, shape=(), dtype=float32)\n",
            "401 tf.Tensor(-0.77903587, shape=(), dtype=float32)\n",
            "402 tf.Tensor(4.3175893, shape=(), dtype=float32)\n",
            "403 tf.Tensor(-4.790343, shape=(), dtype=float32)\n",
            "404 tf.Tensor(8.991952, shape=(), dtype=float32)\n",
            "405 tf.Tensor(1.8784685, shape=(), dtype=float32)\n",
            "406 tf.Tensor(-4.110037, shape=(), dtype=float32)\n",
            "407 tf.Tensor(-2.856091, shape=(), dtype=float32)\n",
            "408 tf.Tensor(-12.515427, shape=(), dtype=float32)\n",
            "409 tf.Tensor(3.458562, shape=(), dtype=float32)\n",
            "410 tf.Tensor(7.5921884, shape=(), dtype=float32)\n",
            "411 tf.Tensor(-3.3789203, shape=(), dtype=float32)\n",
            "412 tf.Tensor(4.4242964, shape=(), dtype=float32)\n",
            "413 tf.Tensor(14.3858385, shape=(), dtype=float32)\n",
            "414 tf.Tensor(7.5850925, shape=(), dtype=float32)\n",
            "415 tf.Tensor(6.472547, shape=(), dtype=float32)\n",
            "416 tf.Tensor(4.013329, shape=(), dtype=float32)\n",
            "417 tf.Tensor(2.297886, shape=(), dtype=float32)\n",
            "418 tf.Tensor(6.3986454, shape=(), dtype=float32)\n",
            "419 tf.Tensor(2.2128074, shape=(), dtype=float32)\n",
            "420 tf.Tensor(-0.04796683, shape=(), dtype=float32)\n",
            "421 tf.Tensor(-9.7132015, shape=(), dtype=float32)\n",
            "422 tf.Tensor(5.904686, shape=(), dtype=float32)\n",
            "423 tf.Tensor(0.4626103, shape=(), dtype=float32)\n",
            "424 tf.Tensor(-3.1681068, shape=(), dtype=float32)\n",
            "425 tf.Tensor(-4.160388, shape=(), dtype=float32)\n",
            "426 tf.Tensor(4.5087695, shape=(), dtype=float32)\n",
            "427 tf.Tensor(7.7495747, shape=(), dtype=float32)\n",
            "428 tf.Tensor(4.187199, shape=(), dtype=float32)\n",
            "429 tf.Tensor(-0.1464919, shape=(), dtype=float32)\n",
            "430 tf.Tensor(8.662213, shape=(), dtype=float32)\n",
            "431 tf.Tensor(-4.6937814, shape=(), dtype=float32)\n",
            "432 tf.Tensor(1.5725806, shape=(), dtype=float32)\n",
            "433 tf.Tensor(3.679296, shape=(), dtype=float32)\n",
            "434 tf.Tensor(0.6379981, shape=(), dtype=float32)\n",
            "435 tf.Tensor(0.042673215, shape=(), dtype=float32)\n",
            "436 tf.Tensor(6.04947, shape=(), dtype=float32)\n",
            "437 tf.Tensor(-0.01721359, shape=(), dtype=float32)\n",
            "438 tf.Tensor(9.783065, shape=(), dtype=float32)\n",
            "439 tf.Tensor(1.8075233, shape=(), dtype=float32)\n",
            "440 tf.Tensor(-1.8584955, shape=(), dtype=float32)\n",
            "441 tf.Tensor(2.1122901, shape=(), dtype=float32)\n",
            "442 tf.Tensor(-2.7052202, shape=(), dtype=float32)\n",
            "443 tf.Tensor(1.2781448, shape=(), dtype=float32)\n",
            "444 tf.Tensor(-0.06601278, shape=(), dtype=float32)\n",
            "445 tf.Tensor(-1.7333063, shape=(), dtype=float32)\n",
            "446 tf.Tensor(-8.287207, shape=(), dtype=float32)\n",
            "447 tf.Tensor(1.5526907, shape=(), dtype=float32)\n",
            "448 tf.Tensor(-2.042607, shape=(), dtype=float32)\n",
            "449 tf.Tensor(4.6726527, shape=(), dtype=float32)\n",
            "450 tf.Tensor(-0.057704866, shape=(), dtype=float32)\n",
            "451 tf.Tensor(-4.110796, shape=(), dtype=float32)\n",
            "452 tf.Tensor(14.164766, shape=(), dtype=float32)\n",
            "453 tf.Tensor(0.13949122, shape=(), dtype=float32)\n",
            "454 tf.Tensor(-4.8888526, shape=(), dtype=float32)\n",
            "455 tf.Tensor(0.6468187, shape=(), dtype=float32)\n",
            "456 tf.Tensor(9.148345, shape=(), dtype=float32)\n",
            "457 tf.Tensor(0.5266857, shape=(), dtype=float32)\n",
            "458 tf.Tensor(-1.5663993, shape=(), dtype=float32)\n",
            "459 tf.Tensor(3.4899223, shape=(), dtype=float32)\n",
            "460 tf.Tensor(-0.96402526, shape=(), dtype=float32)\n",
            "461 tf.Tensor(1.2931492, shape=(), dtype=float32)\n",
            "462 tf.Tensor(3.9271638, shape=(), dtype=float32)\n",
            "463 tf.Tensor(4.769847, shape=(), dtype=float32)\n",
            "464 tf.Tensor(-1.4645729, shape=(), dtype=float32)\n",
            "465 tf.Tensor(-4.309384, shape=(), dtype=float32)\n",
            "466 tf.Tensor(7.0999064, shape=(), dtype=float32)\n",
            "467 tf.Tensor(4.9076424, shape=(), dtype=float32)\n",
            "468 tf.Tensor(5.3248405, shape=(), dtype=float32)\n",
            "469 tf.Tensor(-4.726617, shape=(), dtype=float32)\n",
            "470 tf.Tensor(2.6668432, shape=(), dtype=float32)\n",
            "471 tf.Tensor(2.6495893, shape=(), dtype=float32)\n",
            "472 tf.Tensor(-5.6537037, shape=(), dtype=float32)\n",
            "473 tf.Tensor(7.994323, shape=(), dtype=float32)\n",
            "474 tf.Tensor(-2.0328944, shape=(), dtype=float32)\n",
            "475 tf.Tensor(-1.7072514, shape=(), dtype=float32)\n",
            "476 tf.Tensor(5.036428, shape=(), dtype=float32)\n",
            "477 tf.Tensor(-0.7153585, shape=(), dtype=float32)\n",
            "478 tf.Tensor(2.485881, shape=(), dtype=float32)\n",
            "479 tf.Tensor(-1.2521868, shape=(), dtype=float32)\n",
            "480 tf.Tensor(2.9747312, shape=(), dtype=float32)\n",
            "481 tf.Tensor(-0.17501995, shape=(), dtype=float32)\n",
            "482 tf.Tensor(4.9141674, shape=(), dtype=float32)\n",
            "483 tf.Tensor(4.5289865, shape=(), dtype=float32)\n",
            "484 tf.Tensor(-0.8697194, shape=(), dtype=float32)\n",
            "485 tf.Tensor(3.2071023, shape=(), dtype=float32)\n",
            "486 tf.Tensor(-0.1505483, shape=(), dtype=float32)\n",
            "487 tf.Tensor(0.47782868, shape=(), dtype=float32)\n",
            "488 tf.Tensor(-0.007962102, shape=(), dtype=float32)\n",
            "489 tf.Tensor(-3.665472, shape=(), dtype=float32)\n",
            "490 tf.Tensor(-2.4625192, shape=(), dtype=float32)\n",
            "491 tf.Tensor(-0.951988, shape=(), dtype=float32)\n",
            "492 tf.Tensor(2.0655441, shape=(), dtype=float32)\n",
            "493 tf.Tensor(-0.048518833, shape=(), dtype=float32)\n",
            "494 tf.Tensor(-5.059003, shape=(), dtype=float32)\n",
            "495 tf.Tensor(7.8170385, shape=(), dtype=float32)\n",
            "496 tf.Tensor(5.110509, shape=(), dtype=float32)\n",
            "497 tf.Tensor(-1.0637808, shape=(), dtype=float32)\n",
            "498 tf.Tensor(1.4961816, shape=(), dtype=float32)\n",
            "499 tf.Tensor(3.1586516, shape=(), dtype=float32)\n",
            "500 tf.Tensor(1.7019854, shape=(), dtype=float32)\n",
            "501 tf.Tensor(1.6692388, shape=(), dtype=float32)\n",
            "502 tf.Tensor(0.15875138, shape=(), dtype=float32)\n",
            "503 tf.Tensor(0.8136273, shape=(), dtype=float32)\n",
            "504 tf.Tensor(0.7580291, shape=(), dtype=float32)\n",
            "505 tf.Tensor(0.10445762, shape=(), dtype=float32)\n",
            "506 tf.Tensor(2.3171976, shape=(), dtype=float32)\n",
            "507 tf.Tensor(10.794801, shape=(), dtype=float32)\n",
            "508 tf.Tensor(-1.4995921, shape=(), dtype=float32)\n",
            "509 tf.Tensor(4.9767795, shape=(), dtype=float32)\n",
            "510 tf.Tensor(-3.1026502, shape=(), dtype=float32)\n",
            "511 tf.Tensor(4.014047, shape=(), dtype=float32)\n",
            "512 tf.Tensor(0.4462811, shape=(), dtype=float32)\n",
            "513 tf.Tensor(1.3386414, shape=(), dtype=float32)\n",
            "514 tf.Tensor(9.805572, shape=(), dtype=float32)\n",
            "515 tf.Tensor(-1.5538507, shape=(), dtype=float32)\n",
            "516 tf.Tensor(1.4145569, shape=(), dtype=float32)\n",
            "517 tf.Tensor(0.6798754, shape=(), dtype=float32)\n",
            "518 tf.Tensor(5.6624384, shape=(), dtype=float32)\n",
            "519 tf.Tensor(-6.446778, shape=(), dtype=float32)\n",
            "520 tf.Tensor(-5.1266136, shape=(), dtype=float32)\n",
            "521 tf.Tensor(-3.3567865, shape=(), dtype=float32)\n",
            "522 tf.Tensor(0.23173304, shape=(), dtype=float32)\n",
            "523 tf.Tensor(2.637533, shape=(), dtype=float32)\n",
            "524 tf.Tensor(7.567603, shape=(), dtype=float32)\n",
            "525 tf.Tensor(0.3135986, shape=(), dtype=float32)\n",
            "526 tf.Tensor(-5.873131, shape=(), dtype=float32)\n",
            "527 tf.Tensor(3.5590558, shape=(), dtype=float32)\n",
            "528 tf.Tensor(-2.2505121, shape=(), dtype=float32)\n",
            "529 tf.Tensor(-0.02099223, shape=(), dtype=float32)\n",
            "530 tf.Tensor(-0.24149722, shape=(), dtype=float32)\n",
            "531 tf.Tensor(0.053203195, shape=(), dtype=float32)\n",
            "532 tf.Tensor(4.8030396, shape=(), dtype=float32)\n",
            "533 tf.Tensor(1.2401197, shape=(), dtype=float32)\n",
            "534 tf.Tensor(0.08488162, shape=(), dtype=float32)\n",
            "535 tf.Tensor(0.65544665, shape=(), dtype=float32)\n",
            "536 tf.Tensor(-4.9110327, shape=(), dtype=float32)\n",
            "537 tf.Tensor(0.6326351, shape=(), dtype=float32)\n",
            "538 tf.Tensor(2.3491132, shape=(), dtype=float32)\n",
            "539 tf.Tensor(6.0086765, shape=(), dtype=float32)\n",
            "540 tf.Tensor(1.5514405, shape=(), dtype=float32)\n",
            "541 tf.Tensor(3.1770575, shape=(), dtype=float32)\n",
            "542 tf.Tensor(-6.317619, shape=(), dtype=float32)\n",
            "543 tf.Tensor(2.440946, shape=(), dtype=float32)\n",
            "544 tf.Tensor(5.921375, shape=(), dtype=float32)\n",
            "545 tf.Tensor(-2.4535527, shape=(), dtype=float32)\n",
            "546 tf.Tensor(1.9893793, shape=(), dtype=float32)\n",
            "547 tf.Tensor(-6.585286, shape=(), dtype=float32)\n",
            "548 tf.Tensor(-0.5346622, shape=(), dtype=float32)\n",
            "549 tf.Tensor(-0.17553613, shape=(), dtype=float32)\n",
            "550 tf.Tensor(5.808174, shape=(), dtype=float32)\n",
            "551 tf.Tensor(9.852599, shape=(), dtype=float32)\n",
            "552 tf.Tensor(3.6608043, shape=(), dtype=float32)\n",
            "553 tf.Tensor(-4.357628, shape=(), dtype=float32)\n",
            "554 tf.Tensor(3.6264162, shape=(), dtype=float32)\n",
            "555 tf.Tensor(1.8022311, shape=(), dtype=float32)\n",
            "556 tf.Tensor(-0.046059024, shape=(), dtype=float32)\n",
            "557 tf.Tensor(8.573162, shape=(), dtype=float32)\n",
            "558 tf.Tensor(-2.1422489, shape=(), dtype=float32)\n",
            "559 tf.Tensor(3.2215447, shape=(), dtype=float32)\n",
            "560 tf.Tensor(-0.5140679, shape=(), dtype=float32)\n",
            "561 tf.Tensor(-2.790307, shape=(), dtype=float32)\n",
            "562 tf.Tensor(-0.8858598, shape=(), dtype=float32)\n",
            "563 tf.Tensor(-0.2934026, shape=(), dtype=float32)\n",
            "564 tf.Tensor(3.1855435, shape=(), dtype=float32)\n",
            "565 tf.Tensor(3.0170174, shape=(), dtype=float32)\n",
            "566 tf.Tensor(0.858698, shape=(), dtype=float32)\n",
            "567 tf.Tensor(3.7718513, shape=(), dtype=float32)\n",
            "568 tf.Tensor(5.190568, shape=(), dtype=float32)\n",
            "569 tf.Tensor(0.5205198, shape=(), dtype=float32)\n",
            "570 tf.Tensor(-2.1818023, shape=(), dtype=float32)\n",
            "571 tf.Tensor(-4.294553, shape=(), dtype=float32)\n",
            "572 tf.Tensor(-4.4012966, shape=(), dtype=float32)\n",
            "573 tf.Tensor(1.6827928, shape=(), dtype=float32)\n",
            "574 tf.Tensor(0.9605726, shape=(), dtype=float32)\n",
            "575 tf.Tensor(-3.7298365, shape=(), dtype=float32)\n",
            "576 tf.Tensor(3.6071792, shape=(), dtype=float32)\n",
            "577 tf.Tensor(5.6556273, shape=(), dtype=float32)\n",
            "578 tf.Tensor(0.14701782, shape=(), dtype=float32)\n",
            "579 tf.Tensor(1.2347316, shape=(), dtype=float32)\n",
            "580 tf.Tensor(1.3183132, shape=(), dtype=float32)\n",
            "581 tf.Tensor(-0.11385085, shape=(), dtype=float32)\n",
            "582 tf.Tensor(1.8927995, shape=(), dtype=float32)\n",
            "583 tf.Tensor(-1.6598136, shape=(), dtype=float32)\n",
            "584 tf.Tensor(1.7372015, shape=(), dtype=float32)\n",
            "585 tf.Tensor(0.21207708, shape=(), dtype=float32)\n",
            "586 tf.Tensor(4.244534, shape=(), dtype=float32)\n",
            "587 tf.Tensor(1.492335, shape=(), dtype=float32)\n",
            "588 tf.Tensor(-2.8844426, shape=(), dtype=float32)\n",
            "589 tf.Tensor(-0.6468342, shape=(), dtype=float32)\n",
            "590 tf.Tensor(-0.34752536, shape=(), dtype=float32)\n",
            "591 tf.Tensor(-0.0076451767, shape=(), dtype=float32)\n",
            "592 tf.Tensor(2.075968, shape=(), dtype=float32)\n",
            "593 tf.Tensor(-0.94475126, shape=(), dtype=float32)\n",
            "594 tf.Tensor(3.678286, shape=(), dtype=float32)\n",
            "595 tf.Tensor(2.6222577, shape=(), dtype=float32)\n",
            "596 tf.Tensor(7.3692718, shape=(), dtype=float32)\n",
            "597 tf.Tensor(-4.802257, shape=(), dtype=float32)\n",
            "598 tf.Tensor(1.9177761, shape=(), dtype=float32)\n",
            "599 tf.Tensor(9.990736, shape=(), dtype=float32)\n",
            "600 tf.Tensor(-0.037711635, shape=(), dtype=float32)\n",
            "601 tf.Tensor(-0.27832615, shape=(), dtype=float32)\n",
            "602 tf.Tensor(2.0245857, shape=(), dtype=float32)\n",
            "603 tf.Tensor(2.378195, shape=(), dtype=float32)\n",
            "604 tf.Tensor(2.136051, shape=(), dtype=float32)\n",
            "605 tf.Tensor(-0.025519524, shape=(), dtype=float32)\n",
            "606 tf.Tensor(-1.030706, shape=(), dtype=float32)\n",
            "607 tf.Tensor(-0.16022618, shape=(), dtype=float32)\n",
            "608 tf.Tensor(-3.260993, shape=(), dtype=float32)\n",
            "609 tf.Tensor(0.6321022, shape=(), dtype=float32)\n",
            "610 tf.Tensor(0.28150746, shape=(), dtype=float32)\n",
            "611 tf.Tensor(6.064717, shape=(), dtype=float32)\n",
            "612 tf.Tensor(-0.063270435, shape=(), dtype=float32)\n",
            "613 tf.Tensor(-2.2579112, shape=(), dtype=float32)\n",
            "614 tf.Tensor(-0.1132394, shape=(), dtype=float32)\n",
            "615 tf.Tensor(1.0083158, shape=(), dtype=float32)\n",
            "616 tf.Tensor(0.23062822, shape=(), dtype=float32)\n",
            "617 tf.Tensor(2.0875795, shape=(), dtype=float32)\n",
            "618 tf.Tensor(0.5691415, shape=(), dtype=float32)\n",
            "619 tf.Tensor(3.2446406, shape=(), dtype=float32)\n",
            "620 tf.Tensor(-1.8615276, shape=(), dtype=float32)\n",
            "621 tf.Tensor(-0.0009226352, shape=(), dtype=float32)\n",
            "622 tf.Tensor(-7.41666, shape=(), dtype=float32)\n",
            "623 tf.Tensor(-6.0787144, shape=(), dtype=float32)\n",
            "624 tf.Tensor(-1.1908554, shape=(), dtype=float32)\n",
            "625 tf.Tensor(7.8170857, shape=(), dtype=float32)\n",
            "626 tf.Tensor(0.12225221, shape=(), dtype=float32)\n",
            "627 tf.Tensor(0.106114626, shape=(), dtype=float32)\n",
            "628 tf.Tensor(-8.353943, shape=(), dtype=float32)\n",
            "629 tf.Tensor(0.3731686, shape=(), dtype=float32)\n",
            "630 tf.Tensor(7.0203753, shape=(), dtype=float32)\n",
            "631 tf.Tensor(0.06801422, shape=(), dtype=float32)\n",
            "632 tf.Tensor(5.2967563, shape=(), dtype=float32)\n",
            "633 tf.Tensor(3.5019345, shape=(), dtype=float32)\n",
            "634 tf.Tensor(-4.094888, shape=(), dtype=float32)\n",
            "635 tf.Tensor(8.065174, shape=(), dtype=float32)\n",
            "636 tf.Tensor(-0.10342856, shape=(), dtype=float32)\n",
            "637 tf.Tensor(0.258187, shape=(), dtype=float32)\n",
            "638 tf.Tensor(12.315514, shape=(), dtype=float32)\n",
            "639 tf.Tensor(-1.3588363, shape=(), dtype=float32)\n",
            "640 tf.Tensor(0.031476982, shape=(), dtype=float32)\n",
            "641 tf.Tensor(-4.8237796, shape=(), dtype=float32)\n",
            "642 tf.Tensor(6.725624, shape=(), dtype=float32)\n",
            "643 tf.Tensor(0.39070472, shape=(), dtype=float32)\n",
            "644 tf.Tensor(-0.3484149, shape=(), dtype=float32)\n",
            "645 tf.Tensor(0.07631022, shape=(), dtype=float32)\n",
            "646 tf.Tensor(3.0394583, shape=(), dtype=float32)\n",
            "647 tf.Tensor(1.0382533, shape=(), dtype=float32)\n",
            "648 tf.Tensor(-6.538539, shape=(), dtype=float32)\n",
            "649 tf.Tensor(-4.8704476, shape=(), dtype=float32)\n",
            "650 tf.Tensor(2.8744254, shape=(), dtype=float32)\n",
            "651 tf.Tensor(-2.8535776, shape=(), dtype=float32)\n",
            "652 tf.Tensor(-2.3555048, shape=(), dtype=float32)\n",
            "653 tf.Tensor(-0.22752993, shape=(), dtype=float32)\n",
            "654 tf.Tensor(-3.2922525, shape=(), dtype=float32)\n",
            "655 tf.Tensor(0.06643408, shape=(), dtype=float32)\n",
            "656 tf.Tensor(0.13277736, shape=(), dtype=float32)\n",
            "657 tf.Tensor(-3.5041792, shape=(), dtype=float32)\n",
            "658 tf.Tensor(5.228651, shape=(), dtype=float32)\n",
            "659 tf.Tensor(12.009606, shape=(), dtype=float32)\n",
            "660 tf.Tensor(5.4539466, shape=(), dtype=float32)\n",
            "661 tf.Tensor(-0.04647962, shape=(), dtype=float32)\n",
            "662 tf.Tensor(-0.31338778, shape=(), dtype=float32)\n",
            "663 tf.Tensor(-1.6234791, shape=(), dtype=float32)\n",
            "664 tf.Tensor(6.2185774, shape=(), dtype=float32)\n",
            "665 tf.Tensor(3.89766, shape=(), dtype=float32)\n",
            "666 tf.Tensor(-0.0053701764, shape=(), dtype=float32)\n",
            "667 tf.Tensor(1.0840832, shape=(), dtype=float32)\n",
            "668 tf.Tensor(0.26961544, shape=(), dtype=float32)\n",
            "669 tf.Tensor(0.25471613, shape=(), dtype=float32)\n",
            "670 tf.Tensor(-0.07193336, shape=(), dtype=float32)\n",
            "671 tf.Tensor(-0.03190811, shape=(), dtype=float32)\n",
            "672 tf.Tensor(3.1245062, shape=(), dtype=float32)\n",
            "673 tf.Tensor(0.31391284, shape=(), dtype=float32)\n",
            "674 tf.Tensor(1.9745181, shape=(), dtype=float32)\n",
            "675 tf.Tensor(-1.0835799, shape=(), dtype=float32)\n",
            "676 tf.Tensor(-0.9702326, shape=(), dtype=float32)\n",
            "677 tf.Tensor(8.482314, shape=(), dtype=float32)\n",
            "678 tf.Tensor(0.16670945, shape=(), dtype=float32)\n",
            "679 tf.Tensor(0.90446526, shape=(), dtype=float32)\n",
            "680 tf.Tensor(-2.8791528, shape=(), dtype=float32)\n",
            "681 tf.Tensor(-2.2788255, shape=(), dtype=float32)\n",
            "682 tf.Tensor(-3.9034162, shape=(), dtype=float32)\n",
            "683 tf.Tensor(-0.18434942, shape=(), dtype=float32)\n",
            "684 tf.Tensor(-0.021679377, shape=(), dtype=float32)\n",
            "685 tf.Tensor(2.069023, shape=(), dtype=float32)\n",
            "686 tf.Tensor(-2.0542603, shape=(), dtype=float32)\n",
            "687 tf.Tensor(0.15311462, shape=(), dtype=float32)\n",
            "688 tf.Tensor(-0.8571837, shape=(), dtype=float32)\n",
            "689 tf.Tensor(-5.9510827, shape=(), dtype=float32)\n",
            "690 tf.Tensor(7.3685718, shape=(), dtype=float32)\n",
            "691 tf.Tensor(-0.03879486, shape=(), dtype=float32)\n",
            "692 tf.Tensor(5.2184796, shape=(), dtype=float32)\n",
            "693 tf.Tensor(-4.9828935, shape=(), dtype=float32)\n",
            "694 tf.Tensor(0.7492624, shape=(), dtype=float32)\n",
            "695 tf.Tensor(0.40352827, shape=(), dtype=float32)\n",
            "696 tf.Tensor(8.063762, shape=(), dtype=float32)\n",
            "697 tf.Tensor(2.8624587, shape=(), dtype=float32)\n",
            "698 tf.Tensor(0.021201381, shape=(), dtype=float32)\n",
            "699 tf.Tensor(-1.2393628, shape=(), dtype=float32)\n",
            "700 tf.Tensor(0.03270195, shape=(), dtype=float32)\n",
            "701 tf.Tensor(3.12905, shape=(), dtype=float32)\n",
            "702 tf.Tensor(1.8215197, shape=(), dtype=float32)\n",
            "703 tf.Tensor(-1.4723408, shape=(), dtype=float32)\n",
            "704 tf.Tensor(-4.367559, shape=(), dtype=float32)\n",
            "705 tf.Tensor(2.2285, shape=(), dtype=float32)\n",
            "706 tf.Tensor(0.05051847, shape=(), dtype=float32)\n",
            "707 tf.Tensor(-0.5410545, shape=(), dtype=float32)\n",
            "708 tf.Tensor(-0.214934, shape=(), dtype=float32)\n",
            "709 tf.Tensor(-0.6106617, shape=(), dtype=float32)\n",
            "710 tf.Tensor(0.7508925, shape=(), dtype=float32)\n",
            "711 tf.Tensor(4.0131316, shape=(), dtype=float32)\n",
            "712 tf.Tensor(8.423989, shape=(), dtype=float32)\n",
            "713 tf.Tensor(-0.31540716, shape=(), dtype=float32)\n",
            "714 tf.Tensor(-0.07866351, shape=(), dtype=float32)\n",
            "715 tf.Tensor(0.07699494, shape=(), dtype=float32)\n",
            "716 tf.Tensor(0.00013709534, shape=(), dtype=float32)\n",
            "717 tf.Tensor(1.8383498, shape=(), dtype=float32)\n",
            "718 tf.Tensor(0.060201623, shape=(), dtype=float32)\n",
            "719 tf.Tensor(4.3837366, shape=(), dtype=float32)\n",
            "720 tf.Tensor(2.7061734, shape=(), dtype=float32)\n",
            "721 tf.Tensor(0.36987418, shape=(), dtype=float32)\n",
            "722 tf.Tensor(-1.3575866, shape=(), dtype=float32)\n",
            "723 tf.Tensor(-0.12672335, shape=(), dtype=float32)\n",
            "724 tf.Tensor(-1.257065, shape=(), dtype=float32)\n",
            "725 tf.Tensor(-4.372142, shape=(), dtype=float32)\n",
            "726 tf.Tensor(4.7833147, shape=(), dtype=float32)\n",
            "727 tf.Tensor(-3.1276176, shape=(), dtype=float32)\n",
            "728 tf.Tensor(-4.5766387, shape=(), dtype=float32)\n",
            "729 tf.Tensor(0.932689, shape=(), dtype=float32)\n",
            "730 tf.Tensor(-0.84536505, shape=(), dtype=float32)\n",
            "731 tf.Tensor(-0.23975208, shape=(), dtype=float32)\n",
            "732 tf.Tensor(0.91282713, shape=(), dtype=float32)\n",
            "733 tf.Tensor(4.0129113, shape=(), dtype=float32)\n",
            "734 tf.Tensor(0.88662416, shape=(), dtype=float32)\n",
            "735 tf.Tensor(-0.73338646, shape=(), dtype=float32)\n",
            "736 tf.Tensor(-4.7241955, shape=(), dtype=float32)\n",
            "737 tf.Tensor(-4.1087728, shape=(), dtype=float32)\n",
            "738 tf.Tensor(-0.14347827, shape=(), dtype=float32)\n",
            "739 tf.Tensor(-4.847624, shape=(), dtype=float32)\n",
            "740 tf.Tensor(2.5141757, shape=(), dtype=float32)\n",
            "741 tf.Tensor(0.27101696, shape=(), dtype=float32)\n",
            "742 tf.Tensor(-0.0033604233, shape=(), dtype=float32)\n",
            "743 tf.Tensor(-0.27455312, shape=(), dtype=float32)\n",
            "744 tf.Tensor(0.10696491, shape=(), dtype=float32)\n",
            "745 tf.Tensor(-1.0092721, shape=(), dtype=float32)\n",
            "746 tf.Tensor(-0.19185477, shape=(), dtype=float32)\n",
            "747 tf.Tensor(-0.054823883, shape=(), dtype=float32)\n",
            "748 tf.Tensor(-2.8455648, shape=(), dtype=float32)\n",
            "749 tf.Tensor(0.04835917, shape=(), dtype=float32)\n",
            "750 tf.Tensor(0.07439404, shape=(), dtype=float32)\n",
            "751 tf.Tensor(0.031106826, shape=(), dtype=float32)\n",
            "752 tf.Tensor(-5.1509295, shape=(), dtype=float32)\n",
            "753 tf.Tensor(-4.0772676, shape=(), dtype=float32)\n",
            "754 tf.Tensor(1.0135978, shape=(), dtype=float32)\n",
            "755 tf.Tensor(0.06877992, shape=(), dtype=float32)\n",
            "756 tf.Tensor(0.23690337, shape=(), dtype=float32)\n",
            "757 tf.Tensor(-0.0763884, shape=(), dtype=float32)\n",
            "758 tf.Tensor(3.4022818, shape=(), dtype=float32)\n",
            "759 tf.Tensor(-2.795669, shape=(), dtype=float32)\n",
            "760 tf.Tensor(-0.1898834, shape=(), dtype=float32)\n",
            "761 tf.Tensor(0.30314094, shape=(), dtype=float32)\n",
            "762 tf.Tensor(1.992104, shape=(), dtype=float32)\n",
            "763 tf.Tensor(-1.2109711, shape=(), dtype=float32)\n",
            "764 tf.Tensor(-2.065624, shape=(), dtype=float32)\n",
            "765 tf.Tensor(0.91560733, shape=(), dtype=float32)\n",
            "766 tf.Tensor(-6.9509478, shape=(), dtype=float32)\n",
            "767 tf.Tensor(0.12080252, shape=(), dtype=float32)\n",
            "768 tf.Tensor(0.11981425, shape=(), dtype=float32)\n",
            "769 tf.Tensor(-0.13255459, shape=(), dtype=float32)\n",
            "770 tf.Tensor(-0.046047624, shape=(), dtype=float32)\n",
            "771 tf.Tensor(2.243895, shape=(), dtype=float32)\n",
            "772 tf.Tensor(9.931829, shape=(), dtype=float32)\n",
            "773 tf.Tensor(-0.16471732, shape=(), dtype=float32)\n",
            "774 tf.Tensor(7.6815968, shape=(), dtype=float32)\n",
            "775 tf.Tensor(-0.9743634, shape=(), dtype=float32)\n",
            "776 tf.Tensor(2.4525502, shape=(), dtype=float32)\n",
            "777 tf.Tensor(0.08321215, shape=(), dtype=float32)\n",
            "778 tf.Tensor(0.2836442, shape=(), dtype=float32)\n",
            "779 tf.Tensor(3.0215623, shape=(), dtype=float32)\n",
            "780 tf.Tensor(-0.028698096, shape=(), dtype=float32)\n",
            "781 tf.Tensor(0.0024288278, shape=(), dtype=float32)\n",
            "782 tf.Tensor(-2.6726298, shape=(), dtype=float32)\n",
            "783 tf.Tensor(10.352079, shape=(), dtype=float32)\n",
            "784 tf.Tensor(-0.28168678, shape=(), dtype=float32)\n",
            "785 tf.Tensor(-0.35762033, shape=(), dtype=float32)\n",
            "786 tf.Tensor(3.1173105, shape=(), dtype=float32)\n",
            "787 tf.Tensor(4.8410654, shape=(), dtype=float32)\n",
            "788 tf.Tensor(0.09081281, shape=(), dtype=float32)\n",
            "789 tf.Tensor(-3.7235372, shape=(), dtype=float32)\n",
            "790 tf.Tensor(1.0868279, shape=(), dtype=float32)\n",
            "791 tf.Tensor(3.0596251, shape=(), dtype=float32)\n",
            "792 tf.Tensor(0.052209467, shape=(), dtype=float32)\n",
            "793 tf.Tensor(-0.05302773, shape=(), dtype=float32)\n",
            "794 tf.Tensor(-0.09831796, shape=(), dtype=float32)\n",
            "795 tf.Tensor(-0.014422739, shape=(), dtype=float32)\n",
            "796 tf.Tensor(-0.12907286, shape=(), dtype=float32)\n",
            "797 tf.Tensor(0.25231954, shape=(), dtype=float32)\n",
            "798 tf.Tensor(-0.10276002, shape=(), dtype=float32)\n",
            "799 tf.Tensor(-0.15889971, shape=(), dtype=float32)\n",
            "800 tf.Tensor(0.009008627, shape=(), dtype=float32)\n",
            "801 tf.Tensor(0.42637962, shape=(), dtype=float32)\n",
            "802 tf.Tensor(-1.780688, shape=(), dtype=float32)\n",
            "803 tf.Tensor(-0.012620224, shape=(), dtype=float32)\n",
            "804 tf.Tensor(0.30975652, shape=(), dtype=float32)\n",
            "805 tf.Tensor(2.4571464, shape=(), dtype=float32)\n",
            "806 tf.Tensor(0.4451237, shape=(), dtype=float32)\n",
            "807 tf.Tensor(0.4942383, shape=(), dtype=float32)\n",
            "808 tf.Tensor(-1.2791594, shape=(), dtype=float32)\n",
            "809 tf.Tensor(1.9829733, shape=(), dtype=float32)\n",
            "810 tf.Tensor(2.8570015, shape=(), dtype=float32)\n",
            "811 tf.Tensor(-0.08338209, shape=(), dtype=float32)\n",
            "812 tf.Tensor(2.3082588, shape=(), dtype=float32)\n",
            "813 tf.Tensor(1.4644043, shape=(), dtype=float32)\n",
            "814 tf.Tensor(0.00024880096, shape=(), dtype=float32)\n",
            "815 tf.Tensor(-3.14132, shape=(), dtype=float32)\n",
            "816 tf.Tensor(0.028488142, shape=(), dtype=float32)\n",
            "817 tf.Tensor(-0.26886532, shape=(), dtype=float32)\n",
            "818 tf.Tensor(0.3926648, shape=(), dtype=float32)\n",
            "819 tf.Tensor(5.1987963, shape=(), dtype=float32)\n",
            "820 tf.Tensor(4.092924, shape=(), dtype=float32)\n",
            "821 tf.Tensor(0.04398347, shape=(), dtype=float32)\n",
            "822 tf.Tensor(-0.12174355, shape=(), dtype=float32)\n",
            "823 tf.Tensor(1.6336027, shape=(), dtype=float32)\n",
            "824 tf.Tensor(0.103512555, shape=(), dtype=float32)\n",
            "825 tf.Tensor(1.9443947, shape=(), dtype=float32)\n",
            "826 tf.Tensor(0.34962267, shape=(), dtype=float32)\n",
            "827 tf.Tensor(-0.13551344, shape=(), dtype=float32)\n",
            "828 tf.Tensor(2.0424213, shape=(), dtype=float32)\n",
            "829 tf.Tensor(-3.165858, shape=(), dtype=float32)\n",
            "830 tf.Tensor(0.3451659, shape=(), dtype=float32)\n",
            "831 tf.Tensor(-10.044669, shape=(), dtype=float32)\n",
            "832 tf.Tensor(0.06096403, shape=(), dtype=float32)\n",
            "833 tf.Tensor(-0.10958616, shape=(), dtype=float32)\n",
            "834 tf.Tensor(-1.1457629, shape=(), dtype=float32)\n",
            "835 tf.Tensor(-0.116107374, shape=(), dtype=float32)\n",
            "836 tf.Tensor(0.016859362, shape=(), dtype=float32)\n",
            "837 tf.Tensor(2.5566018, shape=(), dtype=float32)\n",
            "838 tf.Tensor(-0.153391, shape=(), dtype=float32)\n",
            "839 tf.Tensor(4.740625, shape=(), dtype=float32)\n",
            "840 tf.Tensor(-0.079509825, shape=(), dtype=float32)\n",
            "841 tf.Tensor(0.92701495, shape=(), dtype=float32)\n",
            "842 tf.Tensor(0.0019061863, shape=(), dtype=float32)\n",
            "843 tf.Tensor(0.051081583, shape=(), dtype=float32)\n",
            "844 tf.Tensor(0.265675, shape=(), dtype=float32)\n",
            "845 tf.Tensor(1.2233024, shape=(), dtype=float32)\n",
            "846 tf.Tensor(-4.1999955, shape=(), dtype=float32)\n",
            "847 tf.Tensor(-0.10197914, shape=(), dtype=float32)\n",
            "848 tf.Tensor(0.06483367, shape=(), dtype=float32)\n",
            "849 tf.Tensor(0.087145254, shape=(), dtype=float32)\n",
            "850 tf.Tensor(0.20290351, shape=(), dtype=float32)\n",
            "851 tf.Tensor(0.17781554, shape=(), dtype=float32)\n",
            "852 tf.Tensor(0.06403594, shape=(), dtype=float32)\n",
            "853 tf.Tensor(0.3879703, shape=(), dtype=float32)\n",
            "854 tf.Tensor(0.042898603, shape=(), dtype=float32)\n",
            "855 tf.Tensor(0.09520965, shape=(), dtype=float32)\n",
            "856 tf.Tensor(-3.0435922, shape=(), dtype=float32)\n",
            "857 tf.Tensor(-5.7016683, shape=(), dtype=float32)\n",
            "858 tf.Tensor(-0.2375639, shape=(), dtype=float32)\n",
            "859 tf.Tensor(-1.2175634, shape=(), dtype=float32)\n",
            "860 tf.Tensor(-0.20882683, shape=(), dtype=float32)\n",
            "861 tf.Tensor(0.011955399, shape=(), dtype=float32)\n",
            "862 tf.Tensor(-0.2268991, shape=(), dtype=float32)\n",
            "863 tf.Tensor(-0.13412237, shape=(), dtype=float32)\n",
            "864 tf.Tensor(-0.034430627, shape=(), dtype=float32)\n",
            "865 tf.Tensor(-0.18455884, shape=(), dtype=float32)\n",
            "866 tf.Tensor(0.08420825, shape=(), dtype=float32)\n",
            "867 tf.Tensor(4.6196446, shape=(), dtype=float32)\n",
            "868 tf.Tensor(-0.16944322, shape=(), dtype=float32)\n",
            "869 tf.Tensor(-3.241364, shape=(), dtype=float32)\n",
            "870 tf.Tensor(2.079857, shape=(), dtype=float32)\n",
            "871 tf.Tensor(0.19210587, shape=(), dtype=float32)\n",
            "872 tf.Tensor(2.8573244, shape=(), dtype=float32)\n",
            "873 tf.Tensor(0.24607354, shape=(), dtype=float32)\n",
            "874 tf.Tensor(-0.085427575, shape=(), dtype=float32)\n",
            "875 tf.Tensor(-0.07314528, shape=(), dtype=float32)\n",
            "876 tf.Tensor(-0.074585095, shape=(), dtype=float32)\n",
            "877 tf.Tensor(-0.08134614, shape=(), dtype=float32)\n",
            "878 tf.Tensor(0.29692703, shape=(), dtype=float32)\n",
            "879 tf.Tensor(5.4385934, shape=(), dtype=float32)\n",
            "880 tf.Tensor(-3.2464886, shape=(), dtype=float32)\n",
            "881 tf.Tensor(6.075705, shape=(), dtype=float32)\n",
            "882 tf.Tensor(0.21851665, shape=(), dtype=float32)\n",
            "883 tf.Tensor(-3.3265748, shape=(), dtype=float32)\n",
            "884 tf.Tensor(1.4225507, shape=(), dtype=float32)\n",
            "885 tf.Tensor(2.6324587, shape=(), dtype=float32)\n",
            "886 tf.Tensor(-0.03171152, shape=(), dtype=float32)\n",
            "887 tf.Tensor(0.030322075, shape=(), dtype=float32)\n",
            "888 tf.Tensor(3.8536146, shape=(), dtype=float32)\n",
            "889 tf.Tensor(0.052424338, shape=(), dtype=float32)\n",
            "890 tf.Tensor(-0.9496867, shape=(), dtype=float32)\n",
            "891 tf.Tensor(-0.17258662, shape=(), dtype=float32)\n",
            "892 tf.Tensor(-1.7141925, shape=(), dtype=float32)\n",
            "893 tf.Tensor(-0.023563012, shape=(), dtype=float32)\n",
            "894 tf.Tensor(-1.031744, shape=(), dtype=float32)\n",
            "895 tf.Tensor(-0.05073832, shape=(), dtype=float32)\n",
            "896 tf.Tensor(4.494373, shape=(), dtype=float32)\n",
            "897 tf.Tensor(0.28783807, shape=(), dtype=float32)\n",
            "898 tf.Tensor(0.19185396, shape=(), dtype=float32)\n",
            "899 tf.Tensor(-0.15889373, shape=(), dtype=float32)\n",
            "900 tf.Tensor(2.8426611, shape=(), dtype=float32)\n",
            "901 tf.Tensor(0.3313111, shape=(), dtype=float32)\n",
            "902 tf.Tensor(-0.04323143, shape=(), dtype=float32)\n",
            "903 tf.Tensor(2.7919438, shape=(), dtype=float32)\n",
            "904 tf.Tensor(7.304443, shape=(), dtype=float32)\n",
            "905 tf.Tensor(-0.6591914, shape=(), dtype=float32)\n",
            "906 tf.Tensor(3.67012, shape=(), dtype=float32)\n",
            "907 tf.Tensor(-0.048404023, shape=(), dtype=float32)\n",
            "908 tf.Tensor(-0.26764175, shape=(), dtype=float32)\n",
            "909 tf.Tensor(-0.004346637, shape=(), dtype=float32)\n",
            "910 tf.Tensor(0.67397696, shape=(), dtype=float32)\n",
            "911 tf.Tensor(0.12682503, shape=(), dtype=float32)\n",
            "912 tf.Tensor(-3.4732065, shape=(), dtype=float32)\n",
            "913 tf.Tensor(0.10948877, shape=(), dtype=float32)\n",
            "914 tf.Tensor(-0.35348764, shape=(), dtype=float32)\n",
            "915 tf.Tensor(0.19743484, shape=(), dtype=float32)\n",
            "916 tf.Tensor(-0.7293707, shape=(), dtype=float32)\n",
            "917 tf.Tensor(-7.3545, shape=(), dtype=float32)\n",
            "918 tf.Tensor(-1.9959933, shape=(), dtype=float32)\n",
            "919 tf.Tensor(-0.12708125, shape=(), dtype=float32)\n",
            "920 tf.Tensor(6.4488797, shape=(), dtype=float32)\n",
            "921 tf.Tensor(0.1770015, shape=(), dtype=float32)\n",
            "922 tf.Tensor(0.009099849, shape=(), dtype=float32)\n",
            "923 tf.Tensor(0.17301835, shape=(), dtype=float32)\n",
            "924 tf.Tensor(0.07510507, shape=(), dtype=float32)\n",
            "925 tf.Tensor(0.05188696, shape=(), dtype=float32)\n",
            "926 tf.Tensor(-4.9629416, shape=(), dtype=float32)\n",
            "927 tf.Tensor(-0.14901227, shape=(), dtype=float32)\n",
            "928 tf.Tensor(-0.08857414, shape=(), dtype=float32)\n",
            "929 tf.Tensor(0.018201498, shape=(), dtype=float32)\n",
            "930 tf.Tensor(-0.05078855, shape=(), dtype=float32)\n",
            "931 tf.Tensor(0.058216527, shape=(), dtype=float32)\n",
            "932 tf.Tensor(-0.17396061, shape=(), dtype=float32)\n",
            "933 tf.Tensor(0.015332899, shape=(), dtype=float32)\n",
            "934 tf.Tensor(-1.3910555, shape=(), dtype=float32)\n",
            "935 tf.Tensor(-0.15440153, shape=(), dtype=float32)\n",
            "936 tf.Tensor(-3.4198213, shape=(), dtype=float32)\n",
            "937 tf.Tensor(-0.16811074, shape=(), dtype=float32)\n",
            "938 tf.Tensor(-2.89667, shape=(), dtype=float32)\n",
            "939 tf.Tensor(-2.0998359, shape=(), dtype=float32)\n",
            "940 tf.Tensor(0.11387657, shape=(), dtype=float32)\n",
            "941 tf.Tensor(0.92959, shape=(), dtype=float32)\n",
            "942 tf.Tensor(0.11423737, shape=(), dtype=float32)\n",
            "943 tf.Tensor(-0.15621984, shape=(), dtype=float32)\n",
            "944 tf.Tensor(12.577022, shape=(), dtype=float32)\n",
            "945 tf.Tensor(-0.31212243, shape=(), dtype=float32)\n",
            "946 tf.Tensor(7.40249, shape=(), dtype=float32)\n",
            "947 tf.Tensor(-0.3321616, shape=(), dtype=float32)\n",
            "948 tf.Tensor(1.8573219, shape=(), dtype=float32)\n",
            "949 tf.Tensor(-0.6229043, shape=(), dtype=float32)\n",
            "950 tf.Tensor(0.652153, shape=(), dtype=float32)\n",
            "951 tf.Tensor(0.007943489, shape=(), dtype=float32)\n",
            "952 tf.Tensor(-0.3683376, shape=(), dtype=float32)\n",
            "953 tf.Tensor(0.3983132, shape=(), dtype=float32)\n",
            "954 tf.Tensor(0.012449028, shape=(), dtype=float32)\n",
            "955 tf.Tensor(0.14966016, shape=(), dtype=float32)\n",
            "956 tf.Tensor(-0.39465207, shape=(), dtype=float32)\n",
            "957 tf.Tensor(-4.1605573, shape=(), dtype=float32)\n",
            "958 tf.Tensor(-0.09249024, shape=(), dtype=float32)\n",
            "959 tf.Tensor(0.031599294, shape=(), dtype=float32)\n",
            "960 tf.Tensor(0.011713738, shape=(), dtype=float32)\n",
            "961 tf.Tensor(1.2474889, shape=(), dtype=float32)\n",
            "962 tf.Tensor(-0.6408472, shape=(), dtype=float32)\n",
            "963 tf.Tensor(-0.07136668, shape=(), dtype=float32)\n",
            "964 tf.Tensor(-0.27226675, shape=(), dtype=float32)\n",
            "965 tf.Tensor(-3.0997512, shape=(), dtype=float32)\n",
            "966 tf.Tensor(-2.0762596, shape=(), dtype=float32)\n",
            "967 tf.Tensor(0.07702228, shape=(), dtype=float32)\n",
            "968 tf.Tensor(-0.18142119, shape=(), dtype=float32)\n",
            "969 tf.Tensor(3.6723118, shape=(), dtype=float32)\n",
            "970 tf.Tensor(0.029175907, shape=(), dtype=float32)\n",
            "971 tf.Tensor(-0.027894609, shape=(), dtype=float32)\n",
            "972 tf.Tensor(-3.6196072, shape=(), dtype=float32)\n",
            "973 tf.Tensor(-1.4633951, shape=(), dtype=float32)\n",
            "974 tf.Tensor(-0.002119447, shape=(), dtype=float32)\n",
            "975 tf.Tensor(-0.1213798, shape=(), dtype=float32)\n",
            "976 tf.Tensor(-3.3676198, shape=(), dtype=float32)\n",
            "977 tf.Tensor(0.25513113, shape=(), dtype=float32)\n",
            "978 tf.Tensor(-0.07950569, shape=(), dtype=float32)\n",
            "979 tf.Tensor(-0.25389, shape=(), dtype=float32)\n",
            "980 tf.Tensor(-3.2526388, shape=(), dtype=float32)\n",
            "981 tf.Tensor(-0.009409208, shape=(), dtype=float32)\n",
            "982 tf.Tensor(0.1280116, shape=(), dtype=float32)\n",
            "983 tf.Tensor(5.304775, shape=(), dtype=float32)\n",
            "984 tf.Tensor(-4.4144826, shape=(), dtype=float32)\n",
            "985 tf.Tensor(4.284054, shape=(), dtype=float32)\n",
            "986 tf.Tensor(0.865677, shape=(), dtype=float32)\n",
            "987 tf.Tensor(-0.25044918, shape=(), dtype=float32)\n",
            "988 tf.Tensor(-0.15865065, shape=(), dtype=float32)\n",
            "989 tf.Tensor(13.753186, shape=(), dtype=float32)\n",
            "990 tf.Tensor(2.1577384, shape=(), dtype=float32)\n",
            "991 tf.Tensor(1.2558637, shape=(), dtype=float32)\n",
            "992 tf.Tensor(4.2505274, shape=(), dtype=float32)\n",
            "993 tf.Tensor(0.019990737, shape=(), dtype=float32)\n",
            "994 tf.Tensor(0.08517133, shape=(), dtype=float32)\n",
            "995 tf.Tensor(0.06181324, shape=(), dtype=float32)\n",
            "996 tf.Tensor(-7.449807, shape=(), dtype=float32)\n",
            "997 tf.Tensor(12.529592, shape=(), dtype=float32)\n",
            "998 tf.Tensor(1.7456826, shape=(), dtype=float32)\n",
            "999 tf.Tensor(3.5926197, shape=(), dtype=float32)\n",
            "1000 tf.Tensor(0.23764594, shape=(), dtype=float32)\n",
            "1001 tf.Tensor(6.493974, shape=(), dtype=float32)\n",
            "1002 tf.Tensor(-0.07324621, shape=(), dtype=float32)\n",
            "1003 tf.Tensor(-0.7057329, shape=(), dtype=float32)\n",
            "1004 tf.Tensor(-1.8252907, shape=(), dtype=float32)\n",
            "1005 tf.Tensor(0.16366279, shape=(), dtype=float32)\n",
            "1006 tf.Tensor(-2.988029, shape=(), dtype=float32)\n",
            "1007 tf.Tensor(0.5632413, shape=(), dtype=float32)\n",
            "1008 tf.Tensor(-0.08176697, shape=(), dtype=float32)\n",
            "1009 tf.Tensor(-0.31952503, shape=(), dtype=float32)\n",
            "1010 tf.Tensor(-1.5335277, shape=(), dtype=float32)\n",
            "1011 tf.Tensor(5.722531, shape=(), dtype=float32)\n",
            "1012 tf.Tensor(-0.0023449287, shape=(), dtype=float32)\n",
            "1013 tf.Tensor(-0.060794115, shape=(), dtype=float32)\n",
            "1014 tf.Tensor(-4.6269774, shape=(), dtype=float32)\n",
            "1015 tf.Tensor(-0.1942271, shape=(), dtype=float32)\n",
            "1016 tf.Tensor(-0.3494806, shape=(), dtype=float32)\n",
            "1017 tf.Tensor(-0.2783818, shape=(), dtype=float32)\n",
            "1018 tf.Tensor(-0.01080382, shape=(), dtype=float32)\n",
            "1019 tf.Tensor(-2.0835578, shape=(), dtype=float32)\n",
            "1020 tf.Tensor(0.6915758, shape=(), dtype=float32)\n",
            "1021 tf.Tensor(-0.29054698, shape=(), dtype=float32)\n",
            "1022 tf.Tensor(0.10774102, shape=(), dtype=float32)\n",
            "1023 tf.Tensor(-6.9268208, shape=(), dtype=float32)\n",
            "1024 tf.Tensor(-0.011464079, shape=(), dtype=float32)\n",
            "1025 tf.Tensor(0.023380097, shape=(), dtype=float32)\n",
            "1026 tf.Tensor(0.0073720645, shape=(), dtype=float32)\n",
            "1027 tf.Tensor(-0.113836415, shape=(), dtype=float32)\n",
            "1028 tf.Tensor(-1.1224003, shape=(), dtype=float32)\n",
            "1029 tf.Tensor(0.115411505, shape=(), dtype=float32)\n",
            "1030 tf.Tensor(-0.41154277, shape=(), dtype=float32)\n",
            "1031 tf.Tensor(0.04393522, shape=(), dtype=float32)\n",
            "1032 tf.Tensor(-0.089887835, shape=(), dtype=float32)\n",
            "1033 tf.Tensor(-0.1454021, shape=(), dtype=float32)\n",
            "1034 tf.Tensor(-0.07007232, shape=(), dtype=float32)\n",
            "1035 tf.Tensor(-2.114057, shape=(), dtype=float32)\n",
            "1036 tf.Tensor(0.08447025, shape=(), dtype=float32)\n",
            "1037 tf.Tensor(-0.04685311, shape=(), dtype=float32)\n",
            "1038 tf.Tensor(2.0550053, shape=(), dtype=float32)\n",
            "1039 tf.Tensor(-0.055896986, shape=(), dtype=float32)\n",
            "1040 tf.Tensor(0.05398067, shape=(), dtype=float32)\n",
            "1041 tf.Tensor(-0.2231488, shape=(), dtype=float32)\n",
            "1042 tf.Tensor(0.024551895, shape=(), dtype=float32)\n",
            "1043 tf.Tensor(-0.57933366, shape=(), dtype=float32)\n",
            "1044 tf.Tensor(0.6015664, shape=(), dtype=float32)\n",
            "1045 tf.Tensor(2.9021897, shape=(), dtype=float32)\n",
            "1046 tf.Tensor(0.10317442, shape=(), dtype=float32)\n",
            "1047 tf.Tensor(-0.003911465, shape=(), dtype=float32)\n",
            "1048 tf.Tensor(0.2391655, shape=(), dtype=float32)\n",
            "1049 tf.Tensor(3.363867, shape=(), dtype=float32)\n",
            "1050 tf.Tensor(0.034020223, shape=(), dtype=float32)\n",
            "1051 tf.Tensor(-1.6915044, shape=(), dtype=float32)\n",
            "1052 tf.Tensor(-3.1470194, shape=(), dtype=float32)\n",
            "1053 tf.Tensor(0.05766405, shape=(), dtype=float32)\n",
            "1054 tf.Tensor(0.10862768, shape=(), dtype=float32)\n",
            "1055 tf.Tensor(0.058419317, shape=(), dtype=float32)\n",
            "1056 tf.Tensor(0.26958, shape=(), dtype=float32)\n",
            "1057 tf.Tensor(-0.18225895, shape=(), dtype=float32)\n",
            "1058 tf.Tensor(-0.22987139, shape=(), dtype=float32)\n",
            "1059 tf.Tensor(-0.18445852, shape=(), dtype=float32)\n",
            "1060 tf.Tensor(0.9889544, shape=(), dtype=float32)\n",
            "1061 tf.Tensor(-0.011664711, shape=(), dtype=float32)\n",
            "1062 tf.Tensor(0.20633292, shape=(), dtype=float32)\n",
            "1063 tf.Tensor(-2.7930462, shape=(), dtype=float32)\n",
            "1064 tf.Tensor(0.015652988, shape=(), dtype=float32)\n",
            "1065 tf.Tensor(0.010281157, shape=(), dtype=float32)\n",
            "1066 tf.Tensor(0.043194883, shape=(), dtype=float32)\n",
            "1067 tf.Tensor(-0.0481802, shape=(), dtype=float32)\n",
            "1068 tf.Tensor(0.56643504, shape=(), dtype=float32)\n",
            "1069 tf.Tensor(0.24591883, shape=(), dtype=float32)\n",
            "1070 tf.Tensor(-0.10497406, shape=(), dtype=float32)\n",
            "1071 tf.Tensor(-0.039104894, shape=(), dtype=float32)\n",
            "1072 tf.Tensor(-0.047528006, shape=(), dtype=float32)\n",
            "1073 tf.Tensor(0.009968923, shape=(), dtype=float32)\n",
            "1074 tf.Tensor(2.8686824, shape=(), dtype=float32)\n",
            "1075 tf.Tensor(0.16593093, shape=(), dtype=float32)\n",
            "1076 tf.Tensor(1.2926037, shape=(), dtype=float32)\n",
            "1077 tf.Tensor(-0.011912177, shape=(), dtype=float32)\n",
            "1078 tf.Tensor(-0.0066016335, shape=(), dtype=float32)\n",
            "1079 tf.Tensor(0.22265252, shape=(), dtype=float32)\n",
            "1080 tf.Tensor(-0.017602611, shape=(), dtype=float32)\n",
            "1081 tf.Tensor(0.01837583, shape=(), dtype=float32)\n",
            "1082 tf.Tensor(-0.084460184, shape=(), dtype=float32)\n",
            "1083 tf.Tensor(-0.16566397, shape=(), dtype=float32)\n",
            "1084 tf.Tensor(0.06410236, shape=(), dtype=float32)\n",
            "1085 tf.Tensor(-0.13153549, shape=(), dtype=float32)\n",
            "1086 tf.Tensor(0.07872133, shape=(), dtype=float32)\n",
            "1087 tf.Tensor(0.5247608, shape=(), dtype=float32)\n",
            "1088 tf.Tensor(-0.23626995, shape=(), dtype=float32)\n",
            "1089 tf.Tensor(-1.8028222, shape=(), dtype=float32)\n",
            "1090 tf.Tensor(-0.088924475, shape=(), dtype=float32)\n",
            "1091 tf.Tensor(0.058914177, shape=(), dtype=float32)\n",
            "1092 tf.Tensor(0.28267843, shape=(), dtype=float32)\n",
            "1093 tf.Tensor(-0.23091923, shape=(), dtype=float32)\n",
            "1094 tf.Tensor(0.24718341, shape=(), dtype=float32)\n",
            "1095 tf.Tensor(0.09027677, shape=(), dtype=float32)\n",
            "1096 tf.Tensor(0.06434189, shape=(), dtype=float32)\n",
            "1097 tf.Tensor(-0.06764941, shape=(), dtype=float32)\n",
            "1098 tf.Tensor(0.16867585, shape=(), dtype=float32)\n",
            "1099 tf.Tensor(-0.08420067, shape=(), dtype=float32)\n",
            "1100 tf.Tensor(-0.1537032, shape=(), dtype=float32)\n",
            "1101 tf.Tensor(0.14772251, shape=(), dtype=float32)\n",
            "1102 tf.Tensor(-0.24763243, shape=(), dtype=float32)\n",
            "1103 tf.Tensor(-0.4893971, shape=(), dtype=float32)\n",
            "1104 tf.Tensor(0.14151141, shape=(), dtype=float32)\n",
            "1105 tf.Tensor(0.3217864, shape=(), dtype=float32)\n",
            "1106 tf.Tensor(-0.14981076, shape=(), dtype=float32)\n",
            "1107 tf.Tensor(0.37922987, shape=(), dtype=float32)\n",
            "1108 tf.Tensor(0.04968273, shape=(), dtype=float32)\n",
            "1109 tf.Tensor(-1.5814857, shape=(), dtype=float32)\n",
            "1110 tf.Tensor(-3.8692667, shape=(), dtype=float32)\n",
            "1111 tf.Tensor(0.056727417, shape=(), dtype=float32)\n",
            "1112 tf.Tensor(-0.15327954, shape=(), dtype=float32)\n",
            "1113 tf.Tensor(0.119262934, shape=(), dtype=float32)\n",
            "1114 tf.Tensor(-0.19608074, shape=(), dtype=float32)\n",
            "1115 tf.Tensor(4.838052, shape=(), dtype=float32)\n",
            "1116 tf.Tensor(2.3396716, shape=(), dtype=float32)\n",
            "1117 tf.Tensor(-3.3338299, shape=(), dtype=float32)\n",
            "1118 tf.Tensor(0.16499054, shape=(), dtype=float32)\n",
            "1119 tf.Tensor(-0.10282614, shape=(), dtype=float32)\n",
            "1120 tf.Tensor(-0.2585954, shape=(), dtype=float32)\n",
            "1121 tf.Tensor(4.0519524, shape=(), dtype=float32)\n",
            "1122 tf.Tensor(-0.062948465, shape=(), dtype=float32)\n",
            "1123 tf.Tensor(0.12479097, shape=(), dtype=float32)\n",
            "1124 tf.Tensor(12.22772, shape=(), dtype=float32)\n",
            "1125 tf.Tensor(-0.16489486, shape=(), dtype=float32)\n",
            "1126 tf.Tensor(0.22478257, shape=(), dtype=float32)\n",
            "1127 tf.Tensor(0.22941193, shape=(), dtype=float32)\n",
            "1128 tf.Tensor(0.24922395, shape=(), dtype=float32)\n",
            "1129 tf.Tensor(-0.082243025, shape=(), dtype=float32)\n",
            "1130 tf.Tensor(-0.14640899, shape=(), dtype=float32)\n",
            "1131 tf.Tensor(0.21958466, shape=(), dtype=float32)\n",
            "1132 tf.Tensor(0.060344957, shape=(), dtype=float32)\n",
            "1133 tf.Tensor(0.23077336, shape=(), dtype=float32)\n",
            "1134 tf.Tensor(-0.13079274, shape=(), dtype=float32)\n",
            "1135 tf.Tensor(-2.05139, shape=(), dtype=float32)\n",
            "1136 tf.Tensor(7.816837, shape=(), dtype=float32)\n",
            "1137 tf.Tensor(0.12929551, shape=(), dtype=float32)\n",
            "1138 tf.Tensor(0.13402951, shape=(), dtype=float32)\n",
            "1139 tf.Tensor(0.0048709465, shape=(), dtype=float32)\n",
            "1140 tf.Tensor(4.3461676, shape=(), dtype=float32)\n",
            "1141 tf.Tensor(-0.1085987, shape=(), dtype=float32)\n",
            "1142 tf.Tensor(-0.23777975, shape=(), dtype=float32)\n",
            "1143 tf.Tensor(-0.22974561, shape=(), dtype=float32)\n",
            "1144 tf.Tensor(-0.65355974, shape=(), dtype=float32)\n",
            "1145 tf.Tensor(0.13126077, shape=(), dtype=float32)\n",
            "1146 tf.Tensor(0.21120135, shape=(), dtype=float32)\n",
            "1147 tf.Tensor(0.10462623, shape=(), dtype=float32)\n",
            "1148 tf.Tensor(-0.10195137, shape=(), dtype=float32)\n",
            "1149 tf.Tensor(-0.049265884, shape=(), dtype=float32)\n",
            "1150 tf.Tensor(-0.18843749, shape=(), dtype=float32)\n",
            "1151 tf.Tensor(0.08381207, shape=(), dtype=float32)\n",
            "1152 tf.Tensor(-0.059595585, shape=(), dtype=float32)\n",
            "1153 tf.Tensor(-0.016440375, shape=(), dtype=float32)\n",
            "1154 tf.Tensor(0.1712784, shape=(), dtype=float32)\n",
            "1155 tf.Tensor(-3.6864946, shape=(), dtype=float32)\n",
            "1156 tf.Tensor(-3.3802378, shape=(), dtype=float32)\n",
            "1157 tf.Tensor(-0.13361439, shape=(), dtype=float32)\n",
            "1158 tf.Tensor(0.33472037, shape=(), dtype=float32)\n",
            "1159 tf.Tensor(6.990828, shape=(), dtype=float32)\n",
            "1160 tf.Tensor(4.767835, shape=(), dtype=float32)\n",
            "1161 tf.Tensor(-0.048652872, shape=(), dtype=float32)\n",
            "1162 tf.Tensor(2.5556614, shape=(), dtype=float32)\n",
            "1163 tf.Tensor(-0.3404308, shape=(), dtype=float32)\n",
            "1164 tf.Tensor(-0.21647121, shape=(), dtype=float32)\n",
            "1165 tf.Tensor(-2.8624916, shape=(), dtype=float32)\n",
            "1166 tf.Tensor(2.2024868, shape=(), dtype=float32)\n",
            "1167 tf.Tensor(0.17252405, shape=(), dtype=float32)\n",
            "1168 tf.Tensor(0.18668488, shape=(), dtype=float32)\n",
            "1169 tf.Tensor(0.010270674, shape=(), dtype=float32)\n",
            "1170 tf.Tensor(0.14722806, shape=(), dtype=float32)\n",
            "1171 tf.Tensor(-0.00482706, shape=(), dtype=float32)\n",
            "1172 tf.Tensor(0.24099012, shape=(), dtype=float32)\n",
            "1173 tf.Tensor(-0.0057994453, shape=(), dtype=float32)\n",
            "1174 tf.Tensor(0.02490646, shape=(), dtype=float32)\n",
            "1175 tf.Tensor(-0.0778186, shape=(), dtype=float32)\n",
            "1176 tf.Tensor(0.12086637, shape=(), dtype=float32)\n",
            "1177 tf.Tensor(-2.5200841, shape=(), dtype=float32)\n",
            "1178 tf.Tensor(0.04788113, shape=(), dtype=float32)\n",
            "1179 tf.Tensor(0.080134794, shape=(), dtype=float32)\n",
            "1180 tf.Tensor(0.08591223, shape=(), dtype=float32)\n",
            "1181 tf.Tensor(0.15415725, shape=(), dtype=float32)\n",
            "1182 tf.Tensor(-0.10838206, shape=(), dtype=float32)\n",
            "1183 tf.Tensor(0.020176489, shape=(), dtype=float32)\n",
            "1184 tf.Tensor(-0.2899159, shape=(), dtype=float32)\n",
            "1185 tf.Tensor(-0.27771023, shape=(), dtype=float32)\n",
            "1186 tf.Tensor(-2.990878, shape=(), dtype=float32)\n",
            "1187 tf.Tensor(-0.09508698, shape=(), dtype=float32)\n",
            "1188 tf.Tensor(0.18105091, shape=(), dtype=float32)\n",
            "1189 tf.Tensor(0.14961968, shape=(), dtype=float32)\n",
            "1190 tf.Tensor(-2.1038966, shape=(), dtype=float32)\n",
            "1191 tf.Tensor(-10.387186, shape=(), dtype=float32)\n",
            "1192 tf.Tensor(-0.12412596, shape=(), dtype=float32)\n",
            "1193 tf.Tensor(-0.017113395, shape=(), dtype=float32)\n",
            "1194 tf.Tensor(-0.25360066, shape=(), dtype=float32)\n",
            "1195 tf.Tensor(-4.1359406, shape=(), dtype=float32)\n",
            "1196 tf.Tensor(4.054164, shape=(), dtype=float32)\n",
            "1197 tf.Tensor(-0.93180954, shape=(), dtype=float32)\n",
            "1198 tf.Tensor(-0.0039804187, shape=(), dtype=float32)\n",
            "1199 tf.Tensor(0.10165729, shape=(), dtype=float32)\n",
            "1200 tf.Tensor(-0.13367313, shape=(), dtype=float32)\n",
            "1201 tf.Tensor(8.298491, shape=(), dtype=float32)\n",
            "1202 tf.Tensor(0.034783844, shape=(), dtype=float32)\n",
            "1203 tf.Tensor(-8.484211, shape=(), dtype=float32)\n",
            "1204 tf.Tensor(-0.30574518, shape=(), dtype=float32)\n",
            "1205 tf.Tensor(-6.534924, shape=(), dtype=float32)\n",
            "1206 tf.Tensor(2.0291395, shape=(), dtype=float32)\n",
            "1207 tf.Tensor(-1.5001608, shape=(), dtype=float32)\n",
            "1208 tf.Tensor(0.020581342, shape=(), dtype=float32)\n",
            "1209 tf.Tensor(0.086492494, shape=(), dtype=float32)\n",
            "1210 tf.Tensor(-0.19221818, shape=(), dtype=float32)\n",
            "1211 tf.Tensor(0.89738005, shape=(), dtype=float32)\n",
            "1212 tf.Tensor(0.099916175, shape=(), dtype=float32)\n",
            "1213 tf.Tensor(3.002956, shape=(), dtype=float32)\n",
            "1214 tf.Tensor(-0.09531061, shape=(), dtype=float32)\n",
            "1215 tf.Tensor(0.21128964, shape=(), dtype=float32)\n",
            "1216 tf.Tensor(0.18927749, shape=(), dtype=float32)\n",
            "1217 tf.Tensor(0.11282773, shape=(), dtype=float32)\n",
            "1218 tf.Tensor(5.2505665, shape=(), dtype=float32)\n",
            "1219 tf.Tensor(0.17406507, shape=(), dtype=float32)\n",
            "1220 tf.Tensor(0.10858675, shape=(), dtype=float32)\n",
            "1221 tf.Tensor(1.3807788, shape=(), dtype=float32)\n",
            "1222 tf.Tensor(-0.1033066, shape=(), dtype=float32)\n",
            "1223 tf.Tensor(-1.6547773, shape=(), dtype=float32)\n",
            "1224 tf.Tensor(0.77308667, shape=(), dtype=float32)\n",
            "1225 tf.Tensor(-0.0142728565, shape=(), dtype=float32)\n",
            "1226 tf.Tensor(-0.06902811, shape=(), dtype=float32)\n",
            "1227 tf.Tensor(3.9196997, shape=(), dtype=float32)\n",
            "1228 tf.Tensor(1.9180117, shape=(), dtype=float32)\n",
            "1229 tf.Tensor(0.034941543, shape=(), dtype=float32)\n",
            "1230 tf.Tensor(0.052359443, shape=(), dtype=float32)\n",
            "1231 tf.Tensor(-0.121566, shape=(), dtype=float32)\n",
            "1232 tf.Tensor(0.054160904, shape=(), dtype=float32)\n",
            "1233 tf.Tensor(0.35966033, shape=(), dtype=float32)\n",
            "1234 tf.Tensor(-0.21055636, shape=(), dtype=float32)\n",
            "1235 tf.Tensor(-0.2089434, shape=(), dtype=float32)\n",
            "1236 tf.Tensor(0.006343187, shape=(), dtype=float32)\n",
            "1237 tf.Tensor(-5.3274317, shape=(), dtype=float32)\n",
            "1238 tf.Tensor(3.0883994, shape=(), dtype=float32)\n",
            "1239 tf.Tensor(0.047359742, shape=(), dtype=float32)\n",
            "1240 tf.Tensor(-3.5064132, shape=(), dtype=float32)\n",
            "1241 tf.Tensor(0.2249095, shape=(), dtype=float32)\n",
            "1242 tf.Tensor(-0.061013166, shape=(), dtype=float32)\n",
            "1243 tf.Tensor(0.1071486, shape=(), dtype=float32)\n",
            "1244 tf.Tensor(0.26139328, shape=(), dtype=float32)\n",
            "1245 tf.Tensor(-2.4188225, shape=(), dtype=float32)\n",
            "1246 tf.Tensor(0.51938885, shape=(), dtype=float32)\n",
            "1247 tf.Tensor(-0.44024765, shape=(), dtype=float32)\n",
            "1248 tf.Tensor(-0.20467463, shape=(), dtype=float32)\n",
            "1249 tf.Tensor(-0.031005729, shape=(), dtype=float32)\n",
            "1250 tf.Tensor(-0.20419429, shape=(), dtype=float32)\n",
            "1251 tf.Tensor(-2.0134382, shape=(), dtype=float32)\n",
            "1252 tf.Tensor(0.25158364, shape=(), dtype=float32)\n",
            "1253 tf.Tensor(0.17721759, shape=(), dtype=float32)\n",
            "1254 tf.Tensor(-0.09612247, shape=(), dtype=float32)\n",
            "1255 tf.Tensor(-0.09305975, shape=(), dtype=float32)\n",
            "1256 tf.Tensor(0.20628652, shape=(), dtype=float32)\n",
            "1257 tf.Tensor(-0.0013564327, shape=(), dtype=float32)\n",
            "1258 tf.Tensor(0.11423347, shape=(), dtype=float32)\n",
            "1259 tf.Tensor(0.077943824, shape=(), dtype=float32)\n",
            "1260 tf.Tensor(0.03240189, shape=(), dtype=float32)\n",
            "1261 tf.Tensor(-3.3003795, shape=(), dtype=float32)\n",
            "1262 tf.Tensor(-0.07399144, shape=(), dtype=float32)\n",
            "1263 tf.Tensor(-0.09093972, shape=(), dtype=float32)\n",
            "1264 tf.Tensor(3.9489527, shape=(), dtype=float32)\n",
            "1265 tf.Tensor(-0.21240859, shape=(), dtype=float32)\n",
            "1266 tf.Tensor(0.00066495314, shape=(), dtype=float32)\n",
            "1267 tf.Tensor(-2.2351303, shape=(), dtype=float32)\n",
            "1268 tf.Tensor(-0.0056885276, shape=(), dtype=float32)\n",
            "1269 tf.Tensor(-0.18149553, shape=(), dtype=float32)\n",
            "1270 tf.Tensor(3.8284807, shape=(), dtype=float32)\n",
            "1271 tf.Tensor(0.10428924, shape=(), dtype=float32)\n",
            "1272 tf.Tensor(-0.10362664, shape=(), dtype=float32)\n",
            "1273 tf.Tensor(0.011822686, shape=(), dtype=float32)\n",
            "1274 tf.Tensor(-6.0725513, shape=(), dtype=float32)\n",
            "1275 tf.Tensor(-4.511736, shape=(), dtype=float32)\n",
            "1276 tf.Tensor(2.204169, shape=(), dtype=float32)\n",
            "1277 tf.Tensor(0.03033907, shape=(), dtype=float32)\n",
            "1278 tf.Tensor(6.647594e-05, shape=(), dtype=float32)\n",
            "1279 tf.Tensor(0.73635566, shape=(), dtype=float32)\n",
            "1280 tf.Tensor(-1.2902404, shape=(), dtype=float32)\n",
            "1281 tf.Tensor(-2.5900292, shape=(), dtype=float32)\n",
            "1282 tf.Tensor(0.044574693, shape=(), dtype=float32)\n",
            "1283 tf.Tensor(0.0006709732, shape=(), dtype=float32)\n",
            "1284 tf.Tensor(-0.090104565, shape=(), dtype=float32)\n",
            "1285 tf.Tensor(-0.08890473, shape=(), dtype=float32)\n",
            "1286 tf.Tensor(0.2707015, shape=(), dtype=float32)\n",
            "1287 tf.Tensor(0.03854929, shape=(), dtype=float32)\n",
            "1288 tf.Tensor(0.27094898, shape=(), dtype=float32)\n",
            "1289 tf.Tensor(-0.2801036, shape=(), dtype=float32)\n",
            "1290 tf.Tensor(-3.4139526, shape=(), dtype=float32)\n",
            "1291 tf.Tensor(0.014395112, shape=(), dtype=float32)\n",
            "1292 tf.Tensor(-0.24064042, shape=(), dtype=float32)\n",
            "1293 tf.Tensor(-0.088260695, shape=(), dtype=float32)\n",
            "1294 tf.Tensor(-5.4322047, shape=(), dtype=float32)\n",
            "1295 tf.Tensor(0.06275334, shape=(), dtype=float32)\n",
            "1296 tf.Tensor(-3.2615702, shape=(), dtype=float32)\n",
            "1297 tf.Tensor(-0.3954495, shape=(), dtype=float32)\n",
            "1298 tf.Tensor(8.594497, shape=(), dtype=float32)\n",
            "1299 tf.Tensor(0.05760401, shape=(), dtype=float32)\n",
            "1300 tf.Tensor(8.459822, shape=(), dtype=float32)\n",
            "1301 tf.Tensor(0.09562533, shape=(), dtype=float32)\n",
            "1302 tf.Tensor(0.19108564, shape=(), dtype=float32)\n",
            "1303 tf.Tensor(-0.011122638, shape=(), dtype=float32)\n",
            "1304 tf.Tensor(-0.15549399, shape=(), dtype=float32)\n",
            "1305 tf.Tensor(0.07924289, shape=(), dtype=float32)\n",
            "1306 tf.Tensor(-0.57053494, shape=(), dtype=float32)\n",
            "1307 tf.Tensor(-0.6121407, shape=(), dtype=float32)\n",
            "1308 tf.Tensor(-0.038975134, shape=(), dtype=float32)\n",
            "1309 tf.Tensor(7.371218, shape=(), dtype=float32)\n",
            "1310 tf.Tensor(-0.1646519, shape=(), dtype=float32)\n",
            "1311 tf.Tensor(-2.7821705, shape=(), dtype=float32)\n",
            "1312 tf.Tensor(-0.2934913, shape=(), dtype=float32)\n",
            "1313 tf.Tensor(0.031310856, shape=(), dtype=float32)\n",
            "1314 tf.Tensor(-0.045026973, shape=(), dtype=float32)\n",
            "1315 tf.Tensor(-1.4395746, shape=(), dtype=float32)\n",
            "1316 tf.Tensor(-0.12913162, shape=(), dtype=float32)\n",
            "1317 tf.Tensor(-0.21394892, shape=(), dtype=float32)\n",
            "1318 tf.Tensor(-0.06024676, shape=(), dtype=float32)\n",
            "1319 tf.Tensor(-0.23611079, shape=(), dtype=float32)\n",
            "1320 tf.Tensor(-4.8170505, shape=(), dtype=float32)\n",
            "1321 tf.Tensor(0.14251094, shape=(), dtype=float32)\n",
            "1322 tf.Tensor(-0.13782914, shape=(), dtype=float32)\n",
            "1323 tf.Tensor(-4.5730367, shape=(), dtype=float32)\n",
            "1324 tf.Tensor(0.33825782, shape=(), dtype=float32)\n",
            "1325 tf.Tensor(1.8152165, shape=(), dtype=float32)\n",
            "1326 tf.Tensor(8.656945, shape=(), dtype=float32)\n",
            "1327 tf.Tensor(-0.059114456, shape=(), dtype=float32)\n",
            "1328 tf.Tensor(-4.4648886, shape=(), dtype=float32)\n",
            "1329 tf.Tensor(0.11670532, shape=(), dtype=float32)\n",
            "1330 tf.Tensor(0.08507471, shape=(), dtype=float32)\n",
            "1331 tf.Tensor(0.15171534, shape=(), dtype=float32)\n",
            "1332 tf.Tensor(-0.23616265, shape=(), dtype=float32)\n",
            "1333 tf.Tensor(-0.11321844, shape=(), dtype=float32)\n",
            "1334 tf.Tensor(5.8480573, shape=(), dtype=float32)\n",
            "1335 tf.Tensor(-3.4270852, shape=(), dtype=float32)\n",
            "1336 tf.Tensor(0.13042198, shape=(), dtype=float32)\n",
            "1337 tf.Tensor(-0.0667301, shape=(), dtype=float32)\n",
            "1338 tf.Tensor(0.038897384, shape=(), dtype=float32)\n",
            "1339 tf.Tensor(0.04449836, shape=(), dtype=float32)\n",
            "1340 tf.Tensor(0.1965588, shape=(), dtype=float32)\n",
            "1341 tf.Tensor(0.04375372, shape=(), dtype=float32)\n",
            "1342 tf.Tensor(0.028418522, shape=(), dtype=float32)\n",
            "1343 tf.Tensor(-0.13823642, shape=(), dtype=float32)\n",
            "1344 tf.Tensor(-0.016923707, shape=(), dtype=float32)\n",
            "1345 tf.Tensor(7.358983, shape=(), dtype=float32)\n",
            "1346 tf.Tensor(0.08054981, shape=(), dtype=float32)\n",
            "1347 tf.Tensor(0.06182404, shape=(), dtype=float32)\n",
            "1348 tf.Tensor(0.23097105, shape=(), dtype=float32)\n",
            "1349 tf.Tensor(-0.102081835, shape=(), dtype=float32)\n",
            "1350 tf.Tensor(-0.14959523, shape=(), dtype=float32)\n",
            "1351 tf.Tensor(0.053948596, shape=(), dtype=float32)\n",
            "1352 tf.Tensor(0.12235854, shape=(), dtype=float32)\n",
            "1353 tf.Tensor(0.16875139, shape=(), dtype=float32)\n",
            "1354 tf.Tensor(0.31191775, shape=(), dtype=float32)\n",
            "1355 tf.Tensor(-1.5346515, shape=(), dtype=float32)\n",
            "1356 tf.Tensor(-0.37635997, shape=(), dtype=float32)\n",
            "1357 tf.Tensor(0.22131994, shape=(), dtype=float32)\n",
            "1358 tf.Tensor(-0.08599323, shape=(), dtype=float32)\n",
            "1359 tf.Tensor(-0.21103382, shape=(), dtype=float32)\n",
            "1360 tf.Tensor(0.08558882, shape=(), dtype=float32)\n",
            "1361 tf.Tensor(0.0069796974, shape=(), dtype=float32)\n",
            "1362 tf.Tensor(-0.69439757, shape=(), dtype=float32)\n",
            "1363 tf.Tensor(-0.32975292, shape=(), dtype=float32)\n",
            "1364 tf.Tensor(0.062305428, shape=(), dtype=float32)\n",
            "1365 tf.Tensor(-0.17366935, shape=(), dtype=float32)\n",
            "1366 tf.Tensor(0.034528278, shape=(), dtype=float32)\n",
            "1367 tf.Tensor(-0.03403426, shape=(), dtype=float32)\n",
            "1368 tf.Tensor(0.309207, shape=(), dtype=float32)\n",
            "1369 tf.Tensor(-2.7687566, shape=(), dtype=float32)\n",
            "1370 tf.Tensor(-10.5910845, shape=(), dtype=float32)\n",
            "1371 tf.Tensor(-0.11303319, shape=(), dtype=float32)\n",
            "1372 tf.Tensor(0.018657824, shape=(), dtype=float32)\n",
            "1373 tf.Tensor(0.07931528, shape=(), dtype=float32)\n",
            "1374 tf.Tensor(0.32868543, shape=(), dtype=float32)\n",
            "1375 tf.Tensor(-0.25312245, shape=(), dtype=float32)\n",
            "1376 tf.Tensor(-0.08144202, shape=(), dtype=float32)\n",
            "1377 tf.Tensor(-0.037249677, shape=(), dtype=float32)\n",
            "1378 tf.Tensor(-0.049198665, shape=(), dtype=float32)\n",
            "1379 tf.Tensor(-0.03049372, shape=(), dtype=float32)\n",
            "1380 tf.Tensor(-0.070780456, shape=(), dtype=float32)\n",
            "1381 tf.Tensor(-3.5325186, shape=(), dtype=float32)\n",
            "1382 tf.Tensor(-3.6392035, shape=(), dtype=float32)\n",
            "1383 tf.Tensor(0.01625482, shape=(), dtype=float32)\n",
            "1384 tf.Tensor(0.078921914, shape=(), dtype=float32)\n",
            "1385 tf.Tensor(-0.06154035, shape=(), dtype=float32)\n",
            "1386 tf.Tensor(-4.3124, shape=(), dtype=float32)\n",
            "1387 tf.Tensor(-0.28756994, shape=(), dtype=float32)\n",
            "1388 tf.Tensor(-0.040843558, shape=(), dtype=float32)\n",
            "1389 tf.Tensor(-0.014511734, shape=(), dtype=float32)\n",
            "1390 tf.Tensor(-0.057539865, shape=(), dtype=float32)\n",
            "1391 tf.Tensor(0.11803438, shape=(), dtype=float32)\n",
            "1392 tf.Tensor(-0.00878256, shape=(), dtype=float32)\n",
            "1393 tf.Tensor(-0.13901526, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iuila7WhK5tk",
        "outputId": "10cb18d0-d23e-4259-cb1d-0694832bd185"
      },
      "source": [
        "tf.norm(weights1[105] - weights1[133], ord='euclidean')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=7.0594516>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "a5IgQLIzMgNQ",
        "outputId": "bf9f9c77-00d0-40e1-ff41-d6e71f4c08db"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>link</th>\n",
              "      <th>description</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://assets.myntassets.com/dpr_2,q_60,w_210...</td>\n",
              "      <td>Nayo Women Orange &amp; Off-White Striped Straight...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://assets.myntassets.com/dpr_2,q_60,w_210...</td>\n",
              "      <td>Jaipur Kurti Women Navy Blue Yoke Design Kurta...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://assets.myntassets.com/dpr_2,q_60,w_210...</td>\n",
              "      <td>AHIKA Women Green &amp; Off-White Printed Straight...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://assets.myntassets.com/dpr_2,q_60,w_210...</td>\n",
              "      <td>ADA Women Yellow &amp; White Chikankari Hand Embro...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://assets.myntassets.com/dpr_2,q_60,w_210...</td>\n",
              "      <td>Soch Women Navy Blue &amp; Grey Dyed Straight Kurta</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                link  ... id\n",
              "0  https://assets.myntassets.com/dpr_2,q_60,w_210...  ...  0\n",
              "1  https://assets.myntassets.com/dpr_2,q_60,w_210...  ...  1\n",
              "2  https://assets.myntassets.com/dpr_2,q_60,w_210...  ...  2\n",
              "3  https://assets.myntassets.com/dpr_2,q_60,w_210...  ...  3\n",
              "4  https://assets.myntassets.com/dpr_2,q_60,w_210...  ...  4\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyhrjcDNJcNG"
      },
      "source": [
        "## Writing to CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDVr3fSkWfm6"
      },
      "source": [
        "import csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJHrjw09cNpU",
        "outputId": "c3466422-00cf-4fc6-af64-8b5f4d2c99ba"
      },
      "source": [
        "df_stand.size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12305"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66nMTM5XcecB"
      },
      "source": [
        "vocab = {}\n",
        "for i in range(len(inverse_vocab)):\n",
        "  vocab[inverse_vocab[i]] = i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIRBZTatXN9t"
      },
      "source": [
        "lst = []\n",
        "for i in range(df_stand.size):\n",
        "  v = np.array([0. for i in range(embedding_dim)])\n",
        "  l = 0\n",
        "  for word in (df_stand[i].numpy()).decode('utf-8').split():\n",
        "    v += weights1[vocab[word]]\n",
        "    l += 1\n",
        "  lst.append(v / l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lkJb3niW-61"
      },
      "source": [
        "with open(\"word2vec.csv\", \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(lst)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKewXX4lJi12"
      },
      "source": [
        "### Checking the CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcsbGhiVe5V7"
      },
      "source": [
        "dfx = pd.read_csv('word2vec.csv', header = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4-i9yyie-Lv",
        "outputId": "51a02062-332d-40f8-800a-9b14df7d841d"
      },
      "source": [
        "dfx.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12305, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOxR5sKo71De"
      },
      "source": [
        "t_weights = weights1[2:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-amkxSYyQ-9l",
        "outputId": "5299e557-f86f-4976-98fe-bb01c5210e69"
      },
      "source": [
        "t_weights.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1392, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "jxSwSFUoRBsn",
        "outputId": "22328502-416c-468b-9e9b-21699cfc4791"
      },
      "source": [
        "weights_df = (pd.DataFrame(t_weights, index = inverse_vocab[2:])).head()\n",
        "weights_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>472</th>\n",
              "      <th>473</th>\n",
              "      <th>474</th>\n",
              "      <th>475</th>\n",
              "      <th>476</th>\n",
              "      <th>477</th>\n",
              "      <th>478</th>\n",
              "      <th>479</th>\n",
              "      <th>480</th>\n",
              "      <th>481</th>\n",
              "      <th>482</th>\n",
              "      <th>483</th>\n",
              "      <th>484</th>\n",
              "      <th>485</th>\n",
              "      <th>486</th>\n",
              "      <th>487</th>\n",
              "      <th>488</th>\n",
              "      <th>489</th>\n",
              "      <th>490</th>\n",
              "      <th>491</th>\n",
              "      <th>492</th>\n",
              "      <th>493</th>\n",
              "      <th>494</th>\n",
              "      <th>495</th>\n",
              "      <th>496</th>\n",
              "      <th>497</th>\n",
              "      <th>498</th>\n",
              "      <th>499</th>\n",
              "      <th>500</th>\n",
              "      <th>501</th>\n",
              "      <th>502</th>\n",
              "      <th>503</th>\n",
              "      <th>504</th>\n",
              "      <th>505</th>\n",
              "      <th>506</th>\n",
              "      <th>507</th>\n",
              "      <th>508</th>\n",
              "      <th>509</th>\n",
              "      <th>510</th>\n",
              "      <th>511</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>men</th>\n",
              "      <td>-0.473700</td>\n",
              "      <td>-0.035115</td>\n",
              "      <td>-0.115214</td>\n",
              "      <td>0.043739</td>\n",
              "      <td>0.077085</td>\n",
              "      <td>-0.010705</td>\n",
              "      <td>0.160666</td>\n",
              "      <td>-0.012081</td>\n",
              "      <td>0.214587</td>\n",
              "      <td>-0.061020</td>\n",
              "      <td>0.034972</td>\n",
              "      <td>0.426369</td>\n",
              "      <td>-0.067216</td>\n",
              "      <td>0.087028</td>\n",
              "      <td>-0.341544</td>\n",
              "      <td>-0.181459</td>\n",
              "      <td>0.068603</td>\n",
              "      <td>0.505857</td>\n",
              "      <td>-0.020465</td>\n",
              "      <td>0.257249</td>\n",
              "      <td>-0.158001</td>\n",
              "      <td>-0.265266</td>\n",
              "      <td>-0.209810</td>\n",
              "      <td>0.178999</td>\n",
              "      <td>0.008145</td>\n",
              "      <td>-0.128907</td>\n",
              "      <td>-0.623444</td>\n",
              "      <td>0.047955</td>\n",
              "      <td>-0.133734</td>\n",
              "      <td>0.271102</td>\n",
              "      <td>-0.010336</td>\n",
              "      <td>0.119353</td>\n",
              "      <td>0.005074</td>\n",
              "      <td>-0.213583</td>\n",
              "      <td>0.036529</td>\n",
              "      <td>0.028358</td>\n",
              "      <td>-0.097838</td>\n",
              "      <td>-0.313557</td>\n",
              "      <td>-0.159025</td>\n",
              "      <td>-0.056744</td>\n",
              "      <td>...</td>\n",
              "      <td>0.207484</td>\n",
              "      <td>0.034684</td>\n",
              "      <td>0.196952</td>\n",
              "      <td>0.310528</td>\n",
              "      <td>-0.096678</td>\n",
              "      <td>-0.144341</td>\n",
              "      <td>0.001143</td>\n",
              "      <td>0.164145</td>\n",
              "      <td>0.008508</td>\n",
              "      <td>0.019755</td>\n",
              "      <td>0.144409</td>\n",
              "      <td>0.128462</td>\n",
              "      <td>-0.313972</td>\n",
              "      <td>0.134786</td>\n",
              "      <td>-0.168139</td>\n",
              "      <td>0.189559</td>\n",
              "      <td>0.272065</td>\n",
              "      <td>-0.321633</td>\n",
              "      <td>-0.268011</td>\n",
              "      <td>-0.072079</td>\n",
              "      <td>0.032299</td>\n",
              "      <td>0.235308</td>\n",
              "      <td>0.013433</td>\n",
              "      <td>0.574690</td>\n",
              "      <td>-0.081237</td>\n",
              "      <td>0.569061</td>\n",
              "      <td>0.550606</td>\n",
              "      <td>0.006310</td>\n",
              "      <td>0.194956</td>\n",
              "      <td>0.202981</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>-0.615630</td>\n",
              "      <td>0.263323</td>\n",
              "      <td>0.201558</td>\n",
              "      <td>0.101438</td>\n",
              "      <td>-0.209405</td>\n",
              "      <td>0.078792</td>\n",
              "      <td>0.357427</td>\n",
              "      <td>0.317136</td>\n",
              "      <td>-0.040174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>solid</th>\n",
              "      <td>-0.440670</td>\n",
              "      <td>-0.098615</td>\n",
              "      <td>-0.116582</td>\n",
              "      <td>-0.102313</td>\n",
              "      <td>-0.000562</td>\n",
              "      <td>0.269414</td>\n",
              "      <td>-0.040820</td>\n",
              "      <td>-0.331258</td>\n",
              "      <td>-0.189788</td>\n",
              "      <td>0.281291</td>\n",
              "      <td>-0.089125</td>\n",
              "      <td>-0.139188</td>\n",
              "      <td>-0.078831</td>\n",
              "      <td>0.096149</td>\n",
              "      <td>-0.239318</td>\n",
              "      <td>-0.188039</td>\n",
              "      <td>0.823545</td>\n",
              "      <td>0.054818</td>\n",
              "      <td>0.152715</td>\n",
              "      <td>0.233068</td>\n",
              "      <td>0.064769</td>\n",
              "      <td>-0.059498</td>\n",
              "      <td>0.207232</td>\n",
              "      <td>-0.045204</td>\n",
              "      <td>-0.016295</td>\n",
              "      <td>-0.054589</td>\n",
              "      <td>-0.196149</td>\n",
              "      <td>0.190394</td>\n",
              "      <td>0.048291</td>\n",
              "      <td>0.098304</td>\n",
              "      <td>0.029888</td>\n",
              "      <td>-0.068148</td>\n",
              "      <td>0.032743</td>\n",
              "      <td>0.099608</td>\n",
              "      <td>0.099735</td>\n",
              "      <td>-0.135610</td>\n",
              "      <td>0.456175</td>\n",
              "      <td>0.175792</td>\n",
              "      <td>0.394563</td>\n",
              "      <td>0.117918</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.310030</td>\n",
              "      <td>0.029988</td>\n",
              "      <td>0.060547</td>\n",
              "      <td>-0.074243</td>\n",
              "      <td>-0.151402</td>\n",
              "      <td>0.209332</td>\n",
              "      <td>0.194599</td>\n",
              "      <td>-0.084671</td>\n",
              "      <td>-0.055989</td>\n",
              "      <td>0.111269</td>\n",
              "      <td>-0.017629</td>\n",
              "      <td>0.114655</td>\n",
              "      <td>-0.035944</td>\n",
              "      <td>-0.051640</td>\n",
              "      <td>-0.231223</td>\n",
              "      <td>-0.196331</td>\n",
              "      <td>0.084611</td>\n",
              "      <td>-0.381267</td>\n",
              "      <td>0.175043</td>\n",
              "      <td>0.265778</td>\n",
              "      <td>0.093578</td>\n",
              "      <td>0.033352</td>\n",
              "      <td>0.042853</td>\n",
              "      <td>-0.355492</td>\n",
              "      <td>0.218160</td>\n",
              "      <td>0.159865</td>\n",
              "      <td>-0.055890</td>\n",
              "      <td>0.023394</td>\n",
              "      <td>0.175363</td>\n",
              "      <td>-0.012095</td>\n",
              "      <td>0.356672</td>\n",
              "      <td>-0.146754</td>\n",
              "      <td>0.021964</td>\n",
              "      <td>-0.071147</td>\n",
              "      <td>0.186217</td>\n",
              "      <td>0.031064</td>\n",
              "      <td>0.187226</td>\n",
              "      <td>-0.061930</td>\n",
              "      <td>0.021166</td>\n",
              "      <td>0.251777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>kurta</th>\n",
              "      <td>-0.208589</td>\n",
              "      <td>0.127709</td>\n",
              "      <td>0.344170</td>\n",
              "      <td>-0.000771</td>\n",
              "      <td>0.033800</td>\n",
              "      <td>0.439418</td>\n",
              "      <td>-0.116590</td>\n",
              "      <td>-0.198248</td>\n",
              "      <td>0.082710</td>\n",
              "      <td>0.018599</td>\n",
              "      <td>-0.261474</td>\n",
              "      <td>-0.339024</td>\n",
              "      <td>-0.289652</td>\n",
              "      <td>0.110530</td>\n",
              "      <td>0.211881</td>\n",
              "      <td>0.053806</td>\n",
              "      <td>0.413691</td>\n",
              "      <td>0.397127</td>\n",
              "      <td>0.246772</td>\n",
              "      <td>0.117306</td>\n",
              "      <td>-0.043235</td>\n",
              "      <td>0.153355</td>\n",
              "      <td>0.053782</td>\n",
              "      <td>0.004164</td>\n",
              "      <td>-0.163596</td>\n",
              "      <td>-0.132820</td>\n",
              "      <td>0.143645</td>\n",
              "      <td>-0.093748</td>\n",
              "      <td>-0.025149</td>\n",
              "      <td>0.019549</td>\n",
              "      <td>-0.346475</td>\n",
              "      <td>-0.188159</td>\n",
              "      <td>0.133641</td>\n",
              "      <td>0.029359</td>\n",
              "      <td>0.117848</td>\n",
              "      <td>0.046629</td>\n",
              "      <td>0.036872</td>\n",
              "      <td>-0.082961</td>\n",
              "      <td>0.188183</td>\n",
              "      <td>-0.153225</td>\n",
              "      <td>...</td>\n",
              "      <td>0.172842</td>\n",
              "      <td>-0.107519</td>\n",
              "      <td>0.235212</td>\n",
              "      <td>-0.271674</td>\n",
              "      <td>-0.108227</td>\n",
              "      <td>0.244141</td>\n",
              "      <td>0.096219</td>\n",
              "      <td>0.201705</td>\n",
              "      <td>0.244067</td>\n",
              "      <td>0.081439</td>\n",
              "      <td>0.303160</td>\n",
              "      <td>0.061275</td>\n",
              "      <td>-0.101075</td>\n",
              "      <td>0.563330</td>\n",
              "      <td>-0.271337</td>\n",
              "      <td>-0.081314</td>\n",
              "      <td>0.031425</td>\n",
              "      <td>-0.323617</td>\n",
              "      <td>-0.024409</td>\n",
              "      <td>0.167554</td>\n",
              "      <td>0.172466</td>\n",
              "      <td>0.207617</td>\n",
              "      <td>-0.241216</td>\n",
              "      <td>-0.273679</td>\n",
              "      <td>0.053117</td>\n",
              "      <td>0.121748</td>\n",
              "      <td>0.084459</td>\n",
              "      <td>-0.029616</td>\n",
              "      <td>0.118503</td>\n",
              "      <td>-0.300720</td>\n",
              "      <td>-0.018758</td>\n",
              "      <td>-0.296802</td>\n",
              "      <td>-0.377722</td>\n",
              "      <td>-0.003147</td>\n",
              "      <td>-0.004413</td>\n",
              "      <td>0.258336</td>\n",
              "      <td>0.100834</td>\n",
              "      <td>0.220300</td>\n",
              "      <td>-0.106326</td>\n",
              "      <td>0.193770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>printed</th>\n",
              "      <td>0.171397</td>\n",
              "      <td>-0.200712</td>\n",
              "      <td>-0.169800</td>\n",
              "      <td>-0.014701</td>\n",
              "      <td>0.292950</td>\n",
              "      <td>0.206223</td>\n",
              "      <td>-0.059015</td>\n",
              "      <td>-0.077796</td>\n",
              "      <td>-0.224980</td>\n",
              "      <td>-0.165258</td>\n",
              "      <td>0.074298</td>\n",
              "      <td>0.104461</td>\n",
              "      <td>-0.095551</td>\n",
              "      <td>-0.057716</td>\n",
              "      <td>0.000209</td>\n",
              "      <td>-0.171252</td>\n",
              "      <td>0.047446</td>\n",
              "      <td>0.214982</td>\n",
              "      <td>0.062520</td>\n",
              "      <td>0.319596</td>\n",
              "      <td>-0.011830</td>\n",
              "      <td>-0.079752</td>\n",
              "      <td>0.490754</td>\n",
              "      <td>0.111242</td>\n",
              "      <td>-0.006771</td>\n",
              "      <td>0.213066</td>\n",
              "      <td>-0.074521</td>\n",
              "      <td>0.119697</td>\n",
              "      <td>-0.490196</td>\n",
              "      <td>-0.058114</td>\n",
              "      <td>-0.235794</td>\n",
              "      <td>-0.022860</td>\n",
              "      <td>0.001575</td>\n",
              "      <td>0.141597</td>\n",
              "      <td>0.234795</td>\n",
              "      <td>0.158370</td>\n",
              "      <td>0.180651</td>\n",
              "      <td>0.113439</td>\n",
              "      <td>0.136260</td>\n",
              "      <td>-0.265974</td>\n",
              "      <td>...</td>\n",
              "      <td>0.391571</td>\n",
              "      <td>0.022990</td>\n",
              "      <td>0.152037</td>\n",
              "      <td>-0.095028</td>\n",
              "      <td>-0.081476</td>\n",
              "      <td>0.210132</td>\n",
              "      <td>-0.073424</td>\n",
              "      <td>-0.265589</td>\n",
              "      <td>-0.226271</td>\n",
              "      <td>0.316752</td>\n",
              "      <td>0.127964</td>\n",
              "      <td>-0.285439</td>\n",
              "      <td>0.019113</td>\n",
              "      <td>0.395418</td>\n",
              "      <td>0.056962</td>\n",
              "      <td>0.250611</td>\n",
              "      <td>-0.074656</td>\n",
              "      <td>-0.320228</td>\n",
              "      <td>0.238221</td>\n",
              "      <td>-0.057094</td>\n",
              "      <td>0.346149</td>\n",
              "      <td>-0.101523</td>\n",
              "      <td>0.051815</td>\n",
              "      <td>-0.016396</td>\n",
              "      <td>-0.297843</td>\n",
              "      <td>-0.075725</td>\n",
              "      <td>-0.191067</td>\n",
              "      <td>0.102835</td>\n",
              "      <td>-0.114867</td>\n",
              "      <td>0.167139</td>\n",
              "      <td>-0.020935</td>\n",
              "      <td>-0.320540</td>\n",
              "      <td>-0.160017</td>\n",
              "      <td>0.264616</td>\n",
              "      <td>0.034519</td>\n",
              "      <td>-0.073503</td>\n",
              "      <td>0.281344</td>\n",
              "      <td>0.067199</td>\n",
              "      <td>0.161602</td>\n",
              "      <td>0.248263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>blue</th>\n",
              "      <td>-0.048822</td>\n",
              "      <td>0.065541</td>\n",
              "      <td>0.315622</td>\n",
              "      <td>0.002914</td>\n",
              "      <td>-0.503755</td>\n",
              "      <td>-0.144743</td>\n",
              "      <td>-0.025347</td>\n",
              "      <td>0.279507</td>\n",
              "      <td>-0.342884</td>\n",
              "      <td>0.036490</td>\n",
              "      <td>-0.040894</td>\n",
              "      <td>-0.168990</td>\n",
              "      <td>-0.071197</td>\n",
              "      <td>-0.041088</td>\n",
              "      <td>0.023724</td>\n",
              "      <td>-0.063554</td>\n",
              "      <td>0.259025</td>\n",
              "      <td>-0.093667</td>\n",
              "      <td>0.176310</td>\n",
              "      <td>-0.034657</td>\n",
              "      <td>-0.026057</td>\n",
              "      <td>-0.116707</td>\n",
              "      <td>0.014594</td>\n",
              "      <td>-0.372869</td>\n",
              "      <td>0.096336</td>\n",
              "      <td>0.011561</td>\n",
              "      <td>0.321129</td>\n",
              "      <td>0.095900</td>\n",
              "      <td>-0.101154</td>\n",
              "      <td>-0.022526</td>\n",
              "      <td>0.260584</td>\n",
              "      <td>0.018813</td>\n",
              "      <td>-0.010026</td>\n",
              "      <td>-0.157517</td>\n",
              "      <td>-0.159760</td>\n",
              "      <td>0.146262</td>\n",
              "      <td>0.101527</td>\n",
              "      <td>-0.215774</td>\n",
              "      <td>-0.234825</td>\n",
              "      <td>0.289686</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.347018</td>\n",
              "      <td>0.092523</td>\n",
              "      <td>-0.027614</td>\n",
              "      <td>-0.011189</td>\n",
              "      <td>0.351052</td>\n",
              "      <td>-0.246396</td>\n",
              "      <td>0.043056</td>\n",
              "      <td>0.122993</td>\n",
              "      <td>-0.004046</td>\n",
              "      <td>0.050666</td>\n",
              "      <td>0.044574</td>\n",
              "      <td>0.131487</td>\n",
              "      <td>0.093810</td>\n",
              "      <td>0.061925</td>\n",
              "      <td>0.233302</td>\n",
              "      <td>-0.162129</td>\n",
              "      <td>-0.517684</td>\n",
              "      <td>0.041564</td>\n",
              "      <td>0.198838</td>\n",
              "      <td>0.093740</td>\n",
              "      <td>-0.487564</td>\n",
              "      <td>0.101604</td>\n",
              "      <td>-0.046061</td>\n",
              "      <td>0.287561</td>\n",
              "      <td>0.101220</td>\n",
              "      <td>0.124269</td>\n",
              "      <td>-0.065962</td>\n",
              "      <td>0.050895</td>\n",
              "      <td>0.312919</td>\n",
              "      <td>-0.228706</td>\n",
              "      <td>-0.055053</td>\n",
              "      <td>0.003897</td>\n",
              "      <td>-0.013818</td>\n",
              "      <td>0.059708</td>\n",
              "      <td>0.025522</td>\n",
              "      <td>0.230575</td>\n",
              "      <td>-0.047572</td>\n",
              "      <td>-0.065017</td>\n",
              "      <td>-0.145356</td>\n",
              "      <td>-0.050838</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 512 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              0         1         2    ...       509       510       511\n",
              "men     -0.473700 -0.035115 -0.115214  ...  0.357427  0.317136 -0.040174\n",
              "solid   -0.440670 -0.098615 -0.116582  ... -0.061930  0.021166  0.251777\n",
              "kurta   -0.208589  0.127709  0.344170  ...  0.220300 -0.106326  0.193770\n",
              "printed  0.171397 -0.200712 -0.169800  ...  0.067199  0.161602  0.248263\n",
              "blue    -0.048822  0.065541  0.315622  ... -0.065017 -0.145356 -0.050838\n",
              "\n",
              "[5 rows x 512 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUJaYWs0Jpym"
      },
      "source": [
        "## TSNE plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5xcYXRDRzpL",
        "outputId": "4bc75995-e160-489c-9a25-e9be5f80b2d6"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "distance_matrix = cosine_distances(weights1[2:])\n",
        "print(distance_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1392, 1392)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXKEGaKfSe8O"
      },
      "source": [
        "similar_words = {search_term: [inverse_vocab[idx + 1] for idx in distance_matrix[vocab[search_term]-1].argsort()[1:5]+1]\n",
        "                 for search_term in inverse_vocab[2:9]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjX1Kk0PTXyf",
        "outputId": "7d98623a-f4e6-4747-e5a2-4917d98faa73"
      },
      "source": [
        "similar_words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'blue': ['gosriki', 'venkatgiri', 'khadi', 'baluchari'],\n",
              " 'kurta': ['checked', 'solid', 'floral', 'striped'],\n",
              " 'men': ['printed', 'bronzetoned', 'selfchecked', 'pyjama'],\n",
              " 'printed': ['jompers', 'peachcoloured', 'westclo', 'anti'],\n",
              " 'saree': ['maxi', 'wrap', 'contrast', 'aline'],\n",
              " 'solid': ['nakshi', 'vastramay', 'anghrakha', 'gerua'],\n",
              " 'women': ['unstitched', 'sleeveless', 'detailing', 'peachcooloured']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I882MZRPUuUp",
        "outputId": "3ffc8d05-6572-44b8-cfc9-5be1bd37c236"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "words = sum([[k] + v for k, v in similar_words.items()], [])\n",
        "words_ids = [vocab[w] for w in words]\n",
        "word_vectors = np.array([weights1[idx] for idx in words_ids])\n",
        "print('Total words:', len(words), '\\tWord Embedding shapes:', word_vectors.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total words: 35 \tWord Embedding shapes: (35, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "livsoA9LVhu6"
      },
      "source": [
        "tsne = TSNE(n_components=2, random_state=0, n_iter=10000, perplexity=3)\n",
        "np.set_printoptions(suppress=True)\n",
        "T = tsne.fit_transform(word_vectors)\n",
        "labels = words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "_tETkn0vVkc7",
        "outputId": "584cfdde-6479-4821-ad37-84ed6288a11c"
      },
      "source": [
        "plt.figure(figsize=(14, 8))\n",
        "plt.scatter(T[:, 0], T[:, 1], c='steelblue', edgecolors='k')\n",
        "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
        "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')\n",
        "plt.savefig('w1-w1dash.png')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1008x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}