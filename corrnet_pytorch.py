# -*- coding: utf-8 -*-
"""Corrnet_Pytorch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z8IoEphat6weKNujc904tUPWpWfYOt7n

"""


"""Import libraries"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
from PIL import Image
from torchvision.utils import save_image
from torchvision import datasets, transforms
import numpy as np
import pandas as pd
from scipy.spatial import distance
from statistics import median
from torch.utils.data import Dataset, DataLoader
import torchvision.models as models

train_size = 12000
test_size = 300

"""Load data from csv"""

image_vectors = pd.read_csv('/content/drive/Shareddrives/Image-Text-Retrieval/features.csv', header=None)
image_vectors = np.array(image_vectors)

text_vectors = pd.read_csv('/content/drive/Shareddrives/Image-Text-Retrieval/out.csv', header=None)
text_vectors = np.array(text_vectors)

model_save_path = '/content/drive/Shareddrives/Image-Text-Retrieval/Checkpoints/model_state.pt'

print(image_vectors.shape, text_vectors.shape)

# class Encoder2(nn.Module):

#     def __init__(self,input_vec_dim):
#         super(Encoder,self).__init__()
#         # self.img_dim=img_dim
#         # self.txt_dim=txt_dim
#         self.fc1=nn.Linear(input_vec_dim,50)
#         self.fc2=nn.Linear(input_vec_dim,50)
#         # self.fc3=nn.Linear(300,50)

#         # self.fc4=nn.Linear(512,500)
#         # self.fc5=nn.Linear(500,300)
#         # self.fc6=nn.Linear(300,50)

#     def forward(self,img,txt):
#         # print("encode_forward")
#         x=F.relu(self.fc1(img))
#         y=F.relu(self.fc2(txt))
#         # x=F.relu(self.fc3(x))

#         # y=F.relu(self.fc4(txt))
#         # y=F.relu(self.fc5(y))
#         # y=F.relu(self.fc6(y))
#         return F.relu(torch.add(x,y))

class Encoder(nn.Module):

    def __init__(self,input_vec_dim):
        
        super(Encoder,self).__init__()
        
        self.img_dims=[input_vec_dim,300,200,50]
        self.fc1_img=nn.Linear(self.img_dims[0],self.img_dims[1])
        self.fc2_img=nn.Linear(self.img_dims[1],self.img_dims[2])
        self.fc3_img=nn.Linear(self.img_dims[2],self.img_dims[3])
        
        self.txt_dims=[input_vec_dim,300,200,50]
        self.fc1_txt=nn.Linear(self.txt_dims[0],self.txt_dims[1])
        self.fc2_txt=nn.Linear(self.txt_dims[1],self.txt_dims[2])
        self.fc3_txt=nn.Linear(self.txt_dims[2],self.txt_dims[3])

    def forward(self,img,txt):

        x=F.relu(self.fc1_img(img))
        x=F.relu(self.fc2_img(x))
        x=F.relu(self.fc3_img(x))

        y=F.relu(self.fc1_txt(txt))
        y=F.relu(self.fc2_txt(y))
        y=F.relu(self.fc3_txt(y))

        return F.relu(torch.add(x,y))

# class Decoder(nn.Module):

#     def __init__(self,output_vec_dim):
#         super(Decoder,self).__init__()
#         # self.img_dim=img_dim
#         # self.txt_dim=txt_dim
#         self.fc1=nn.Linear(50,output_vec_dim)
#         self.fc2=nn.Linear(50,output_vec_dim)


#     def forward(self,rep):
#         # print("decoded_forward")
#         x=F.relu(self.fc1(rep))
#         y=F.relu(self.fc2(rep))

#         # print("decoder")
#         # print(x.shape)
#         # print(y.shape)
#         # print(ret.shape)
#         combined=F.relu(torch.cat((x,y),1))
#         return combined

class Decoder(nn.Module):

    def __init__(self,output_vec_dim):
        
        super(Decoder,self).__init__()
        
        self.img_dims=[output_vec_dim,200,300,512]
        self.fc1_img=nn.Linear(self.img_dims[0],self.img_dims[1])
        self.fc2_img=nn.Linear(self.img_dims[1],self.img_dims[2])
        self.fc3_img=nn.Linear(self.img_dims[2],self.img_dims[3])
        
        self.txt_dims=[output_vec_dim,200,300,512]
        self.fc1_txt=nn.Linear(self.txt_dims[0],self.txt_dims[1])
        self.fc2_txt=nn.Linear(self.txt_dims[1],self.txt_dims[2])
        self.fc3_txt=nn.Linear(self.txt_dims[2],self.txt_dims[3])

    def forward(self,rep):

        x=F.relu(self.fc1_img(rep))
        x=F.relu(self.fc2_img(x))
        x=F.relu(self.fc3_img(x))

        y=F.relu(self.fc1_txt(rep))
        y=F.relu(self.fc2_txt(y))
        y=F.relu(self.fc3_txt(y))

        combined=F.relu(torch.cat((x,y),1))
        return combined

class Corrnet(nn.Module):
    def __init__(self,input_vec_dim,common_rep_dim):
        super(Corrnet,self).__init__()
        self.encoder=Encoder(input_vec_dim)
        self.decoder=Decoder(common_rep_dim)

    def forward(self,img,txt):
        common_rep=self.encoder(img,txt)
        combined=self.decoder(common_rep)
        return combined


class corrnet_dataset(Dataset):
  def __init__(self,img,txt):
    self.img = img
    self.txt = txt
    if self.img.shape[0]!=self.txt.shape[0]:
      raise Exception("Different no. of samples")

  def __len__(self):
    return self.img.shape[0]

  def __getitem__(self, index):
    # note that this isn't randomly selecting. It's a simple get a single item that represents an x and y
    _img = self.img[index]
    _txt = self.txt[index]

    return _img, _txt

def preprocess_data():
  image_paths = ['dataset/' + str(i) + '.jpg' for i in range(12305)]
  images = []
  for image_path in image_paths:
    input_image = Image.open(image_path)
    preprocess = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])
    input_tensor = preprocess(input_image)
    images.append(input_tensor)
  images = np.array(images)
  images = torch.from_numpy(images.astype(np.float32))

  # do text preprocessing

  texts = []

  return images, texts


def get_data_loader():
    img, txt = preprocess_data()

    loader = DataLoader(corrnet_dataset(img, txt), batch_size=32, shuffle=True)

    return loader

def make_model(load_pretrained=True):

    Model=Corrnet(512,50)
    optimizer = optim.Adam(Model.parameters(), lr=0.001)

    if (load_pretrained and os.path.exists(model_save_path)):
      Model.load_state_dict(torch.load(model_save_path))

    return Model,optimizer
    # print(Model)

def correlation(x, y, lamda=0.02):

  '''
    x, y are n x 50 dimensional vectors obtained from the respective n x 512 embeddings
  '''

  x_mean = torch.mean(x, dim = 0) # Along the y-axis, that is, average of all feature vectors
  y_mean = torch.mean(y, dim = 0) # 1 x 50 dimensional
  x_centered = torch.sub(x, torch.mean(x)) # calculates xi - X_mean n x 50 dimensional
  y_centered = torch.sub(y,torch.mean(y)) # calculates xi - X_mean
  corr_nr = torch.sum(torch.mul(x_centered, y_centered)) # The numerator
  # print(list(corr_nr.shape))
  corr_dr1 = torch.sqrt(torch.sum(torch.square(x_centered)))
  corr_dr2 = torch.sqrt(torch.sum(torch.square(y_centered)))
  corr_dr = corr_dr1 * corr_dr2
  corr = -lamda * corr_nr / corr_dr
  # print(corr.item()) # Should decrease ideally
  return corr


def l2error(x,y):
    print(list(x.size()))
    h=torch.sub(x,y)
    g=torch.square(h)
    f=torch.sum(g)
    return f

# Similar to how Prof Nipun described it during class
def own_mse_loss(inp, target):
  L = (inp - target) ** 2
  return torch.mean(L)

criterion = nn.MSELoss()


# Just like in ML lecture

def train():
    epochs = 100
    for e in range(epochs):
        ind = 1
        L = []
        err=[[],[],[],[]]
        for img,txt in dataset:

            # img-> 224*224*3 array
            # txt -> string

            concat_inputs=torch.cat((img,txt),1)

            opt.zero_grad()

            res_combined_input=corrnet(img,txt)
            res_img_input=corrnet(img,torch.zeros_like(txt))
            res_txt_input=corrnet(torch.zeros_like(img),txt)

            err1 = criterion(res_combined_input,concat_inputs)
            # err_dash1 = own_mse_loss(res_combined_input, concat_inputs)
            # if ind == 1 and e == 1:
            #     print(err1.item() == err_dash1.item())

            err2 = criterion(res_img_input,concat_inputs)
            err3 = criterion(res_txt_input,concat_inputs)
            err4 = correlation(
                corrnet.encoder(img, torch.zeros_like(txt)),
                corrnet.encoder(torch.zeros_like(img), txt)
            )
            
            loss = (err1 + err2 + err3 + err4)

            loss.backward()

            
            L.append(loss.item())
            err[0].append(err1.item())
            err[1].append(err2.item())
            err[2].append(err3.item())
            err[3].append(err4.item())
            opt.step()

            ind+=1
        
        print("Epoch: {}:, Loss: {}".format(e, np.mean(L)))
        for i in range(len(err)):
          print("err{}: {}".format(i+1,np.mean(err[i])),end="\t")
        print("\n")

        if(e%10==0):
          torch.save(corrnet.state_dict(),model_save_path)

def predict(img, txt):
  img_vecs = corrnet.encoder(img, torch.zeros_like(txt))
  txt_vecs = corrnet.encoder(torch.zeros_like(img), txt)

  euc = []
  cos = []
  # x = 0
  for img_vec, txt_vec in zip(img_vecs, txt_vecs):
    euc.append(distance.euclidean(img_vec.cpu().detach().numpy(), txt_vec.cpu().detach().numpy()))
    cos.append(distance.cosine(img_vec.cpu().detach().numpy(), txt_vec.cpu().detach().numpy()))
    # if x < 100:
    #   print(distance.cityblock(img_vec.cpu().detach().numpy(), txt_vec.cpu().detach().numpy()))
    # x += 1

  return np.array(euc), np.array(cos)

def print_metrics():
  img_test = torch.from_numpy(image_vectors[train_size: train_size+test_size].astype(np.float32))
  txt_test = torch.from_numpy(text_vectors[train_size: train_size+test_size].astype(np.float32))

  mr = []
  top_5_count = 0
  top_10_count = 0

  for i in range(test_size):
    img_array = np.zeros((test_size, 512))
    for k in range(test_size):
      img_array[k] = img_test[i]
        
    txt_array = np.zeros((test_size, 512))
    for j in range(test_size):
      txt_array[j] = txt_test[j]
    
    predictions = list(predict(torch.from_numpy(txt_array.astype(np.float32)), torch.from_numpy(img_array.astype(np.float32)))[1])
    pred_i = predictions[i]
    predictions.sort()
    rank = predictions.index(pred_i)
    if rank < (0.1*test_size):
      top_10_count += 1
    if rank < (0.05*test_size):
      top_5_count += 1
    mr.append(rank+1)     

  print('Median Rank(img->txt):', median(mr)*100/test_size, '%')
  print('f@5%(img->txt):', top_5_count*100/test_size, '%')
  print('f@10%(img->txt):', top_10_count*100/test_size, '%')

  mr = []
  top_5_count = 0
  top_10_count = 0

  for i in range(test_size):
    img_array = np.zeros((test_size, 512))
    for k in range(test_size):
      img_array[k] = img_test[k]
        
    txt_array = np.zeros((test_size, 512))
    for j in range(test_size):
      txt_array[j] = txt_test[i]
    
    predictions = list(predict(torch.from_numpy(txt_array.astype(np.float32)), torch.from_numpy(img_array.astype(np.float32)))[1])
    pred_i = predictions[i]
    predictions.sort()
    rank = predictions.index(pred_i)
    if rank < (0.1*test_size):
      top_10_count += 1
    if rank < (0.05*test_size):
      top_5_count += 1
    mr.append(rank+1)     

  print('Median Rank(txt->img):', median(mr)*100/test_size, '%')
  print('f@5%(txt->img):', top_5_count*100/test_size, '%')
  print('f@10%(txt->img):', top_10_count*100/test_size, '%')


corrnet, opt = make_model()
dataset = get_data_loader() # Insert the embeddings inside this function
train()

print_metrics()


class image_processor(nn.Module):
  def __init__(self):
    # super.
    self.Resnet = models.resnet152()

  def forward(self,img):
    # img-> 224*224*3 
    x=self.Resnet(img)
    return x

class word_processor(nn.Module):
  def __init__(self):
    # super.

  self.Resnet=resnet()

  def forward(self,img):
    # img-> 224*224*3 
    x=self.Resnet(img)
    return x



class Corrnet(nn.Module):
    def __init__(self,input_vec_dim,common_rep_dim):
        super(Corrnet,self).__init__()

        self.img_prcr=image_processor()
        self.wrd_prcr=text_processor()

        self.encoder
        self.decoder

    def forward(self,img,txt):

        # img-> 224*224*3
        # txt-> string

        self.img_vector=self.img_prcr(img)
        self.txt_vector=self.txt_prcr(txt)


        common_rep=self.encoder(img,txt)
        combined=self.decoder(common_rep)
        return combined

# raw-> images(250,250,3)
# txt-> list
