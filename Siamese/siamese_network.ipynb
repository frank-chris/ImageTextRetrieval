{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "siamese-network.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t37o4I-3obHH"
      },
      "source": [
        "## Importing libraries, initialising global variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "4df1608c-a2a9-48f9-8812-af1b5cdf1b16",
        "_cell_guid": "b9556bde-e5b6-4643-9afe-82298daf444e",
        "trusted": true,
        "id": "7CNvDBi7obHL"
      },
      "source": [
        "import imageio\n",
        "from statistics import median\n",
        "from random import randint\n",
        "from glob import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.layers.core import Flatten, Dropout\n",
        "from keras.layers import Input, Dense, Lambda, Layer\n",
        "from keras import backend as K\n",
        "from keras import applications\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import RMSprop, Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.applications.resnet import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.applications.resnet import ResNet152\n",
        "\n",
        "# Path to folder containing images\n",
        "DATASET_PATH = './Images'\n",
        "\n",
        "num_samples = 12000"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPpnmyfIobHO"
      },
      "source": [
        "## Generator function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "T37PuCDJobHQ"
      },
      "source": [
        "\n",
        "# data generator for neural network\n",
        "# forms correct and incorrect pairings of images with text descriptions and labels them as correct (1) or incorrect (0)\n",
        "\n",
        "def generator(batch_size, df):\n",
        "    \n",
        "    batch_img = np.zeros((batch_size, 224, 224, 3))\n",
        "    batch_txt = np.zeros((batch_size, 512))\n",
        "    batch_labels = np.zeros((batch_size,1))\n",
        "    \n",
        "    video_ids = df['image']\n",
        "    video_txt = df['txt_enc']\n",
        "    \n",
        "    length = len(df) -1\n",
        "    \n",
        "    while True:\n",
        "        for i in range(batch_size//2):\n",
        "            \n",
        "            i = i*2\n",
        "            \n",
        "            #correct\n",
        "            sample = randint(0,length)\n",
        "            file = video_ids.iloc[sample]\n",
        "            \n",
        "            correct_txt = video_txt.iloc[sample]\n",
        "            \n",
        "            im = load_img(file, target_size=(224, 224))\n",
        "            im = img_to_array(im)\n",
        "            im = np.expand_dims(im, axis=0)\n",
        "            im = preprocess_input(im)\n",
        "            \n",
        "            batch_img[i-2] = im\n",
        "            batch_txt[i-2] = correct_txt\n",
        "            batch_labels[i-2] = 1\n",
        "                       \n",
        "            #incorrect \n",
        "            file = video_ids.iloc[randint(0,length)]\n",
        "                       \n",
        "            im = load_img(file, target_size=(224, 224))\n",
        "            im = img_to_array(im)\n",
        "            im = np.expand_dims(im, axis=0)\n",
        "            im = preprocess_input(im)\n",
        "\n",
        "            batch_img[i-1] = im\n",
        "            batch_txt[i-1] = correct_txt\n",
        "            batch_labels[i-1] = 0\n",
        "                        \n",
        "        yield [batch_txt, batch_img], batch_labels"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Jd2CQ73obHS"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Ye8dgWGgobHT"
      },
      "source": [
        "def euclidean_distance(vects):\n",
        "    x, y = vects\n",
        "    return K.sqrt(K.maximum(K.sum(K.square(x - y), axis=1, keepdims=True), K.epsilon()))\n",
        "\n",
        "def eucl_dist_output_shape(shapes):\n",
        "    shape1, shape2 = shapes\n",
        "    return (shape1[0], 1)\n",
        "\n",
        "def contrastive_loss(y_true, y_pred):\n",
        "    margin = 1\n",
        "    return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))\n",
        "\n",
        "def create_img_encoder(input_dim, resnet):\n",
        "    x = Sequential()\n",
        "    x.add(resnet)\n",
        "    x.add(Dense(500, activation=\"relu\"))\n",
        "    x.add(Dropout(0.5))\n",
        "    x.add(Dense(512, activation=\"relu\"))\n",
        "    return x\n",
        "\n",
        "def create_txt_encoder(input_dim):\n",
        "    x = Sequential()\n",
        "    x.add(Dense(500, input_shape = (512,), activation=\"relu\"))\n",
        "    x.add(Dropout(0.5))\n",
        "    x.add(Dense(512, activation=\"relu\"))\n",
        "    return x\n",
        "\n",
        "def compute_accuracy(predictions, labels):\n",
        "    return labels[predictions.ravel() < 0.5].mean()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3T5g0oNbobHV"
      },
      "source": [
        "## Initialise ResNet152"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-77a0Xr8obHV",
        "outputId": "7e5a9a53-9a96-4212-826e-07eaab5a6a25"
      },
      "source": [
        "resnet = ResNet152(include_top=True, weights='imagenet')\n",
        "\n",
        "for layer in resnet.layers:\n",
        "    layer.trainable = False"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet152_weights_tf_dim_ordering_tf_kernels.h5\n",
            "242900992/242900224 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luQkh4-XobHX"
      },
      "source": [
        "## Creating model and loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x92fCwLmobHX",
        "outputId": "57462219-c1f8-4477-bbfb-885fba5c4d2f"
      },
      "source": [
        "input_txt = Input(shape=(512,))\n",
        "input_img = Input(shape=(224, 224, 3))\n",
        "\n",
        "txt_enc = create_txt_encoder(input_txt)\n",
        "img_enc = create_img_encoder(input_img, resnet)\n",
        "\n",
        "encoded_txt = txt_enc(input_txt)\n",
        "encoded_img = img_enc(input_img)\n",
        "\n",
        "distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([encoded_txt, encoded_img])\n",
        "\n",
        "model = Model([input_txt, input_img], distance)\n",
        "\n",
        "adam = Adam(lr=0.00001)\n",
        "model.compile(loss=contrastive_loss, optimizer=adam)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 512)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequential (Sequential)         (None, 512)          513012      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "sequential_1 (Sequential)       (None, 512)          61176956    input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 1)            0           sequential[0][0]                 \n",
            "                                                                 sequential_1[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 61,689,968\n",
            "Trainable params: 1,270,024\n",
            "Non-trainable params: 60,419,944\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "e2mh48irobHY"
      },
      "source": [
        "# The CSV generated by the word2vec(gensim) model\n",
        "data = pd.read_csv('./word2vec_gensim.csv', header=None)\n",
        "data = list(np.array(data))\n",
        "\n",
        "img_paths = [DATASET_PATH + str(i) + '.jpg' for i in range(12305)]\n",
        "\n",
        "dataset = pd.DataFrame()\n",
        "dataset['image'] = pd.Series(img_paths)\n",
        "dataset['txt_enc'] = pd.Series(data)\n",
        "\n",
        "df_test = dataset[num_samples:]\n",
        "dataset = dataset[:num_samples]\n",
        "\n",
        "df_train = dataset[:int(num_samples*0.8)]\n",
        "df_val = dataset[int(num_samples*0.8):]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aES0ha1fobHa"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "mrlWYM3aobHb"
      },
      "source": [
        "model.fit_generator(generator(30, df_train), steps_per_epoch= int(int(num_samples*0.8)/30), validation_data= generator(30, df_val), validation_steps=int(int(num_samples*0.2)/30), epochs=200, verbose=1)\n",
        "model.save_weights('./weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6aJV10WobHb"
      },
      "source": [
        "## Load saved weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "wgSxvtsrobHb"
      },
      "source": [
        "# Load from where you stored the weights\n",
        "model.load_weights('./weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SLZVox5obHd"
      },
      "source": [
        "## Decide size of test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "mZgSAMXxobHd"
      },
      "source": [
        "subset_size = 300\n",
        "subset = df_test.iloc[:subset_size]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQnoQLbZobHd"
      },
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "4J7OWSyGobHd"
      },
      "source": [
        "# metrics - img -> text\n",
        "\n",
        "mr = []\n",
        "top_1_count = 0\n",
        "top_5_count = 0\n",
        "top_10_count = 0\n",
        "\n",
        "for i in range(subset_size):\n",
        "    file = subset['image'].iloc[i]\n",
        "    im = load_img(file, target_size=(224, 224))\n",
        "    im = img_to_array(im)\n",
        "    im = np.expand_dims(im, axis=0)\n",
        "    im = preprocess_input(im)\n",
        "    \n",
        "    image_array = np.zeros((subset_size, 224, 224, 3))\n",
        "    for k in range(subset_size):\n",
        "        image_array[k] = im\n",
        "        \n",
        "    txt_array = np.zeros((subset_size, 512))\n",
        "    for j in range(subset_size):\n",
        "        txt = subset['txt_enc'].iloc[j]\n",
        "        txt_array[j] = txt\n",
        "    \n",
        "    predictions = [pred[0] for pred in model.predict([txt_array, image_array])]\n",
        "    pred_i = predictions[i]\n",
        "    predictions.sort()\n",
        "    rank = predictions.index(pred_i)\n",
        "    if rank < 10:\n",
        "        top_10_count += 1\n",
        "    if rank < 5:\n",
        "        top_5_count += 1\n",
        "    if rank < 1:\n",
        "        top_1_count += 1\n",
        "    mr.append(rank+1)     \n",
        "\n",
        "print('Median Rank(img->txt):', median(mr)*100/subset_size, '%')\n",
        "print('R@1(img->txt):', top_1_count*100/subset_size, '%')\n",
        "print('R@5(img->txt):', top_5_count*100/subset_size, '%')\n",
        "print('R@10(img->txt):', top_10_count*100/subset_size, '%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "a7YkBwZoobHd",
        "outputId": "e20587e6-d8d5-45af-8e8d-7678adef6643"
      },
      "source": [
        "# metrics - txt -> img\n",
        "\n",
        "mr = []\n",
        "top_1_count = 0\n",
        "top_5_count = 0\n",
        "top_10_count = 0\n",
        "\n",
        "for i in range(subset_size):\n",
        "    txt = subset['txt_enc'].iloc[i] \n",
        "    txt_array = np.zeros((subset_size, 512))\n",
        "    for k in range(subset_size):\n",
        "        txt_array[k] = txt\n",
        "        \n",
        "        \n",
        "    image_array = np.zeros((subset_size, 224, 224, 3))\n",
        "    for j in range(subset_size):\n",
        "        file = subset['image'].iloc[j]\n",
        "        im = load_img(file, target_size=(224, 224))\n",
        "        im = img_to_array(im)\n",
        "        im = np.expand_dims(im, axis=0)\n",
        "        im = preprocess_input(im)\n",
        "        image_array[k] = im\n",
        "    \n",
        "    predictions = [pred[0] for pred in model.predict([txt_array, image_array])]\n",
        "    pred_i = predictions[i]\n",
        "    predictions.sort()\n",
        "    rank = predictions.index(pred_i)\n",
        "    if rank < 10:\n",
        "        top_10_count += 1\n",
        "    if rank < 5:\n",
        "        top_5_count += 1\n",
        "    if rank < 1:\n",
        "        top_1_count += 1\n",
        "    mr.append(rank+1)     \n",
        "\n",
        "print('Median Rank(txt->img):', median(mr)*100/subset_size, '%')\n",
        "print('R@1(txt->img):', top_1_count*100/subset_size, '%')\n",
        "print('R@5(txt->img):', top_5_count*100/subset_size, '%')\n",
        "print('R@10(txt->img):', top_10_count*100/subset_size, '%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Median Rank(txt->img): 0.6666666666666666 %\n",
            "R@1(txt->img): 33.666666666666664 %\n",
            "R@5(txt->img): 95.33333333333333 %\n",
            "R@10(txt->img): 95.33333333333333 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8LTe6sGobHf"
      },
      "source": [
        "## Download Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "osTWoctwobHf"
      },
      "source": [
        "# download weights\n",
        "\n",
        "from IPython.display import FileLink\n",
        "\n",
        "FileLink(r'./weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XObkIosAobHf"
      },
      "source": [
        "## Try predicting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "s7pceQ4dobHg"
      },
      "source": [
        "# trying out predict\n",
        "\n",
        "text = np.zeros((2, 512))\n",
        "image = np.zeros((2, 224, 224, 3))\n",
        "\n",
        "file = dataset['image'].iloc[21]     \n",
        "correct_txt = dataset['txt_enc'].iloc[21]\n",
        "\n",
        "im = load_img(file, target_size=(224, 224))\n",
        "im = img_to_array(im)\n",
        "im = np.expand_dims(im, axis=0)\n",
        "im = preprocess_input(im)\n",
        "\n",
        "image[0] = im\n",
        "\n",
        "text[0] = correct_txt\n",
        "\n",
        "file = dataset['image'].iloc[21]     \n",
        "correct_txt = dataset['txt_enc'].iloc[90]\n",
        "\n",
        "im = load_img(file, target_size=(224, 224))\n",
        "im = img_to_array(im)\n",
        "im = np.expand_dims(im, axis=0)\n",
        "im = preprocess_input(im)\n",
        "\n",
        "image[1] = im\n",
        "\n",
        "text[1] = correct_txt\n",
        "\n",
        "model.predict([text, image])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}